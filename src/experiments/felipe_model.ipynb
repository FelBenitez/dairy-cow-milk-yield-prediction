{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe9743f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# submission functionality\n",
    "# ------------------------\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\")) # so src/ is on the path\n",
    "\n",
    "import importlib\n",
    "import submission_utils\n",
    "importlib.reload(submission_utils) # force reload latest code\n",
    "\n",
    "from submission_utils import save_submission\n",
    "# ------------------------\n",
    "import xgboost as xgb\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "259edd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "test = pd.read_csv(\"../../data/cattle_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da068384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "\n",
    "def preprocess_pipeline(df, encode_flag=True, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "    \n",
    "    # -----------------------\n",
    "    # 1) Drop impossible targets (train only)\n",
    "    # -----------------------\n",
    "    if target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0]\n",
    "        y = milk_features[target_col]\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Basic cleaning\n",
    "    # -----------------------\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    # Fill any remaining numeric NaNs\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    milk_features[numeric_cols] = milk_features[numeric_cols].fillna(milk_features[numeric_cols].median())\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Date features\n",
    "    # -----------------------\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"])\n",
    "        milk_features[\"year\"] = milk_features[\"Date\"].dt.year\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"is_weekend\"] = milk_features[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "        milk_features[\"date_ordinal\"] = milk_features[\"Date\"].map(pd.Timestamp.toordinal)\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "    \n",
    "    \n",
    "\n",
    "    # -----------------------\n",
    "    # 5) Farm clustering\n",
    "    # -----------------------\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        # Use only numeric features for clustering (exclude IDs)\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "\n",
    "        # Aggregate per farm\n",
    "        farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "\n",
    "        # Scale and cluster farms\n",
    "        scaler = StandardScaler()\n",
    "        farm_scaled = scaler.fit_transform(farm_features)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "\n",
    "        # Map back to rows\n",
    "        milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # INSERT: Weight_Efficiency_Z (Breed + Age_Year)\n",
    "    # -----------------------\n",
    "    if \"Weight_kg\" in milk_features.columns and \"Age_Months\" in milk_features.columns:\n",
    "        milk_features[\"Age_Year\"] = milk_features[\"Age_Months\"] // 12\n",
    "        group_cols = [\"Breed\", \"Age_Year\"]\n",
    "\n",
    "        # Calculate cohort stats using transform so it maps back to original rows\n",
    "        cohort_mean = milk_features.groupby(group_cols)[\"Weight_kg\"].transform(\"mean\")\n",
    "        cohort_std = milk_features.groupby(group_cols)[\"Weight_kg\"].transform(\"std\")\n",
    "\n",
    "        # Calculate Z-Score: How heavy is this cow relative to her peers?\n",
    "        milk_features[\"Weight_Efficiency_Z\"] = (\n",
    "            (milk_features[\"Weight_kg\"] - cohort_mean) / (cohort_std + 1e-5)\n",
    "        )\n",
    "\n",
    "        milk_features = milk_features.drop(columns=[\"Age_Year\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # 6) Drop raw IDs\n",
    "    # -----------------------\n",
    "    drop_cols = [\"Cattle_ID\"]\n",
    "    milk_features = milk_features.drop(columns=[c for c in drop_cols if c in milk_features.columns])\n",
    "\n",
    "    # -----------------------\n",
    "    # 7) Optional one-hot encoding (we'll keep it OFF for CatBoost)\n",
    "    # -----------------------\n",
    "    if encode_flag:\n",
    "        cat_cols = milk_features.select_dtypes(include=\"object\").columns.tolist()\n",
    "        milk_features = pd.get_dummies(milk_features, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    # Make sure Farm_Cluster exists even if something went weird\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    final_df = milk_features.copy()\n",
    "    return final_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------\n",
    "# # LightGBM hyperparameter search\n",
    "# # -----------------------------------\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# lgb_base = lgb.LGBMRegressor(\n",
    "#     objective=\"regression\",\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# lgb_param_dist = {\n",
    "#     \"num_leaves\": [31, 63, 127, 255],\n",
    "#     \"max_depth\": [-1, 6, 8, 10],\n",
    "#     \"learning_rate\": [0.01, 0.03, 0.05, 0.08],\n",
    "#     \"n_estimators\": [300, 600, 900, 1200],\n",
    "#     \"subsample\": [0.7, 0.8, 0.9],\n",
    "#     \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "#     \"min_child_samples\": [10, 20, 50, 100]\n",
    "# }\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# lgb_search = RandomizedSearchCV(\n",
    "#     estimator=lgb_base,\n",
    "#     param_distributions=lgb_param_dist,\n",
    "#     n_iter=25,                         # reduce if too slow\n",
    "#     scoring=\"neg_mean_squared_error\",  # we'll sqrt it to get RMSE\n",
    "#     cv=kf,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# print(\"Starting LightGBM hyperparameter search...\")\n",
    "# lgb_search.fit(X, y)\n",
    "\n",
    "# best_lgb_params = lgb_search.best_params_\n",
    "# best_lgb_rmse = np.sqrt(-lgb_search.best_score_)\n",
    "\n",
    "# print(\"\\nBest LightGBM params:\")\n",
    "# print(best_lgb_params)\n",
    "# print(f\"Best LightGBM CV RMSE: {best_lgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce05399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (209926, 43)\n",
      "Test shape: (40000, 43)\n"
     ]
    }
   ],
   "source": [
    "# Build train and test matrices from the new pipeline\n",
    "X, y = preprocess_pipeline(train, encode_flag=False, target_col=target)\n",
    "test_df, _ = preprocess_pipeline(test, encode_flag=False, target_col=None)\n",
    "\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f24468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added feature: Farm_Performance\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# REPLACEMENT: Farm_Performance Target Encoding (The Missing Signal)\n",
    "# ==============================================================================\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# FIX: Reset indices (Keep this from previous attempt, it worked)\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True) \n",
    "\n",
    "def add_target_encoding_features(train_df, test_df, y_train, target_col_name=\"Milk_Yield_L\", n_splits=5):\n",
    "    # CHANGE: Encode Farm_ID instead of Bio features\n",
    "    # This captures \"Managerial Quality\" - the specific effect of this farm on yield\n",
    "    encodings = {\n",
    "        \"Farm_Performance\": [\"Farm_ID\"]\n",
    "    }\n",
    "    \n",
    "    train_encoded = train_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    \n",
    "    # Combine X and y temporarily for the training split\n",
    "    train_temp = train_encoded.copy()\n",
    "    train_temp[target_col_name] = y_train.values\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for new_col, group_cols in encodings.items():\n",
    "        # --- A) TRAINING SET: K-Fold Encoding (Prevents Leakage) ---\n",
    "        train_encoded[new_col] = 0.0\n",
    "        \n",
    "        for tr_ind, val_ind in kf.split(train_temp):\n",
    "            X_tr, X_val = train_temp.iloc[tr_ind], train_temp.iloc[val_ind]\n",
    "            \n",
    "            # Calculate mean yield per Farm on the 'training' part\n",
    "            means = X_tr.groupby(group_cols)[target_col_name].mean()\n",
    "            \n",
    "            # Map to the 'validation' part\n",
    "            # Since it's just one column (Farm_ID), simple map works\n",
    "            train_encoded.loc[val_ind, new_col] = X_val[group_cols[0]].map(means).fillna(X_tr[target_col_name].mean())\n",
    "\n",
    "        # --- B) TEST SET: Global Mean Encoding ---\n",
    "        # Compute global farm stats from full training data\n",
    "        global_means = train_temp.groupby(group_cols)[target_col_name].mean()\n",
    "        global_avg = train_temp[target_col_name].mean()\n",
    "        \n",
    "        # Map to test set\n",
    "        test_encoded[new_col] = test_encoded[group_cols[0]].map(global_means).fillna(global_avg)\n",
    "\n",
    "    # OPTIONAL: Drop the raw Farm_ID now that we have the encoded score\n",
    "    # This keeps the model clean\n",
    "    train_encoded = train_encoded.drop(columns=[\"Farm_ID\"], errors='ignore')\n",
    "    test_encoded = test_encoded.drop(columns=[\"Farm_ID\"], errors='ignore')\n",
    "\n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Execute\n",
    "X, test_df = add_target_encoding_features(X, test_df, y)\n",
    "print(\"Added feature: Farm_Performance\")\n",
    "\n",
    "# ... Continue to CatBoost training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af31e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost categorical features: ['Breed', 'Climate_Zone', 'Management_System', 'Lactation_Stage', 'Feed_Type']\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns for CatBoost (from the processed X)\n",
    "cat_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(\"CatBoost categorical features:\", cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee33aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- CatBoost Fold 1 -----\n",
      "CatBoost Fold 1 RMSE: 4.1073\n",
      "\n",
      "----- CatBoost Fold 2 -----\n",
      "CatBoost Fold 2 RMSE: 4.1007\n",
      "\n",
      "----- CatBoost Fold 3 -----\n",
      "CatBoost Fold 3 RMSE: 4.1208\n",
      "\n",
      "----- CatBoost Fold 4 -----\n",
      "CatBoost Fold 4 RMSE: 4.1066\n",
      "\n",
      "----- CatBoost Fold 5 -----\n",
      "CatBoost Fold 5 RMSE: 4.0996\n",
      "\n",
      "==========================\n",
      "CatBoost Average RMSE: 4.1070\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "\n",
    "# 5-Fold Cross-Validation for CatBoost\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cat_fold_rmse = []\n",
    "cat_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n----- CatBoost Fold {fold+1} -----\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,          # you can try depth=6 like your teammate too\n",
    "        subsample=0.8,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    cat_fold_rmse.append(rmse)\n",
    "\n",
    "    print(f\"CatBoost Fold {fold+1} RMSE: {rmse:.4f}\")\n",
    "    cat_models.append(model)\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(f\"CatBoost Average RMSE: {np.mean(cat_fold_rmse):.4f}\")\n",
    "print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab94853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4625\n",
      "[LightGBM] [Info] Number of data points in the train set: 209926, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score 15.595179\n",
      "Final CatBoost TRAIN RMSE:  3.9662\n",
      "Final LightGBM TRAIN RMSE:  3.8379\n",
      "Ensemble(0.7 CB, 0.3 LGB) TRAIN RMSE: 3.9232\n",
      "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_only__2025-11-15__22-57-41.csv\n"
     ]
    }
   ],
   "source": [
    "# Train final CatBoost model on full data\n",
    "final_cat_model = CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,      # again, can try 6 if you want to match teammate\n",
    "    subsample=0.8,\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "final_cat_model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    cat_features=cat_features,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict on test_df built from preprocess_pipeline\n",
    "cat_test_preds = final_cat_model.predict(test_df)\n",
    "\n",
    "cat_submission = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: cat_test_preds\n",
    "})\n",
    "\n",
    "# save_submission(cat_submission, run_name=\"felipe_catboost_farmcluster_lactation\")\n",
    "print(\"submission_catboost.csv created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c513f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CatBoost Ensemble Fold 1 =====\n",
      "cat_d8_lr005 Fold 1 RMSE: 4.1093\n",
      "cat_d6_lr003 Fold 1 RMSE: 4.1080\n",
      "cat_d10_lr004 Fold 1 RMSE: 4.1117\n",
      "Ensemble Fold 1 RMSE: 4.1075 (weights: w1=0.30, w2=0.60, w3=0.10)\n",
      "\n",
      "===== CatBoost Ensemble Fold 2 =====\n",
      "cat_d8_lr005 Fold 2 RMSE: 4.1027\n",
      "cat_d6_lr003 Fold 2 RMSE: 4.1018\n",
      "cat_d10_lr004 Fold 2 RMSE: 4.1037\n",
      "Ensemble Fold 2 RMSE: 4.1007 (weights: w1=0.20, w2=0.50, w3=0.30)\n",
      "\n",
      "===== CatBoost Ensemble Fold 3 =====\n",
      "cat_d8_lr005 Fold 3 RMSE: 4.1250\n",
      "cat_d6_lr003 Fold 3 RMSE: 4.1212\n",
      "cat_d10_lr004 Fold 3 RMSE: 4.1266\n",
      "Ensemble Fold 3 RMSE: 4.1212 (weights: w1=0.00, w2=0.90, w3=0.10)\n",
      "\n",
      "===== CatBoost Ensemble Fold 4 =====\n",
      "cat_d8_lr005 Fold 4 RMSE: 4.1059\n",
      "cat_d6_lr003 Fold 4 RMSE: 4.1065\n",
      "cat_d10_lr004 Fold 4 RMSE: 4.1101\n",
      "Ensemble Fold 4 RMSE: 4.1051 (weights: w1=0.60, w2=0.40, w3=-0.00)\n",
      "\n",
      "===== CatBoost Ensemble Fold 5 =====\n",
      "cat_d8_lr005 Fold 5 RMSE: 4.1009\n",
      "cat_d6_lr003 Fold 5 RMSE: 4.0992\n",
      "cat_d10_lr004 Fold 5 RMSE: 4.1059\n",
      "Ensemble Fold 5 RMSE: 4.0988 (weights: w1=0.30, w2=0.70, w3=-0.00)\n",
      "\n",
      "==========================\n",
      "cat_d8_lr005 Average RMSE: 4.1088\n",
      "cat_d6_lr003 Average RMSE: 4.1073\n",
      "cat_d10_lr004 Average RMSE: 4.1116\n",
      "Ensemble Average RMSE: 4.1066\n",
      "Average ensemble weights over folds: w1=0.280, w2=0.620, w3=0.100\n",
      "==========================\n",
      "\n",
      "Training final model: cat_d8_lr005\n",
      "Training final model: cat_d6_lr003\n",
      "Training final model: cat_d10_lr004\n",
      "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_catensemble__2025-11-17__02-27-55.csv\n",
      "Saved submission_cat_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# THIS IS ENSEMBLE CODE\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "\n",
    "# 5-Fold Cross-Validation for a CatBoost ensemble\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define 3 diverse CatBoost configs\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"cat_d8_lr005\",\n",
    "        \"params\": dict(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            depth=8,\n",
    "            subsample=0.8,\n",
    "            l2_leaf_reg=3.0,\n",
    "            verbose=False\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cat_d6_lr003\",\n",
    "        \"params\": dict(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            subsample=0.9,\n",
    "            l2_leaf_reg=5.0,\n",
    "            verbose=False\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cat_d10_lr004\",\n",
    "        \"params\": dict(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.04,\n",
    "            depth=10,\n",
    "            subsample=0.7,\n",
    "            l2_leaf_reg=6.0,\n",
    "            verbose=False\n",
    "        )\n",
    "    },\n",
    "]\n",
    "\n",
    "# Track per-model and ensemble RMSEs\n",
    "per_model_fold_rmse = {cfg[\"name\"]: [] for cfg in model_configs}\n",
    "ens_fold_rmse = []\n",
    "ens_fold_weights = []  # store best weights per fold\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n===== CatBoost Ensemble Fold {fold+1} =====\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Store val predictions from each model in this fold\n",
    "    fold_val_preds = []\n",
    "\n",
    "    # 1) Train each CatBoost variant\n",
    "    for i, cfg in enumerate(model_configs):\n",
    "        name = cfg[\"name\"]\n",
    "        params = cfg[\"params\"].copy()\n",
    "\n",
    "        # Slightly different random_seed per fold & model\n",
    "        params[\"random_seed\"] = 42 + fold * 10 + i\n",
    "\n",
    "        model = CatBoostRegressor(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=cat_features,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        per_model_fold_rmse[name].append(rmse)\n",
    "        fold_val_preds.append(preds)\n",
    "\n",
    "        print(f\"{name} Fold {fold+1} RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # 2) Find best non-negative weights (w1,w2,w3) summing to 1\n",
    "    #    on this fold using a small grid search\n",
    "    preds_matrix = np.vstack(fold_val_preds).T  # shape: [n_val, 3]\n",
    "\n",
    "    alphas = np.linspace(0.0, 1.0, 11)  # 0.0, 0.1, ..., 1.0\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_w = None\n",
    "\n",
    "    for a1 in alphas:\n",
    "        for a2 in alphas:\n",
    "            if a1 + a2 > 1.0:\n",
    "                continue\n",
    "            a3 = 1.0 - a1 - a2\n",
    "            w = np.array([a1, a2, a3])\n",
    "            blend = preds_matrix @ w\n",
    "            rmse_blend = np.sqrt(mean_squared_error(y_val, blend))\n",
    "            if rmse_blend < best_rmse:\n",
    "                best_rmse = rmse_blend\n",
    "                best_w = w\n",
    "\n",
    "    ens_fold_rmse.append(best_rmse)\n",
    "    ens_fold_weights.append(best_w)\n",
    "\n",
    "    print(\n",
    "        f\"Ensemble Fold {fold+1} RMSE: {best_rmse:.4f} \"\n",
    "        f\"(weights: w1={best_w[0]:.2f}, w2={best_w[1]:.2f}, w3={best_w[2]:.2f})\"\n",
    "    )\n",
    "\n",
    "# 3) Print average RMSEs\n",
    "print(\"\\n==========================\")\n",
    "for cfg in model_configs:\n",
    "    name = cfg[\"name\"]\n",
    "    avg_rmse = np.mean(per_model_fold_rmse[name])\n",
    "    print(f\"{name} Average RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "ens_avg_rmse = np.mean(ens_fold_rmse)\n",
    "print(f\"Ensemble Average RMSE: {ens_avg_rmse:.4f}\")\n",
    "\n",
    "# Average weights over folds for final test ensemble\n",
    "ens_fold_weights = np.array(ens_fold_weights)  # shape: [n_folds, 3]\n",
    "global_weights = ens_fold_weights.mean(axis=0)\n",
    "print(\n",
    "    f\"Average ensemble weights over folds: \"\n",
    "    f\"w1={global_weights[0]:.3f}, w2={global_weights[1]:.3f}, w3={global_weights[2]:.3f}\"\n",
    ")\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "# 4) Train final 3 CatBoost models on all data and ensemble on test\n",
    "\n",
    "final_test_preds = []\n",
    "for i, cfg in enumerate(model_configs):\n",
    "    name = cfg[\"name\"]\n",
    "    params = cfg[\"params\"].copy()\n",
    "    params[\"random_seed\"] = 999 + i  # fixed seeds for final models\n",
    "\n",
    "    print(f\"Training final model: {name}\")\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds_test = model.predict(test_df)\n",
    "    final_test_preds.append(preds_test)\n",
    "\n",
    "final_test_preds = np.vstack(final_test_preds).T  # [n_test, 3]\n",
    "\n",
    "# Use global_weights from CV\n",
    "final_ensemble_test = final_test_preds @ global_weights  # shape: (n_test,)\n",
    "\n",
    "# Build submission DataFrame: [Cattle_ID, Milk_Yield_L]\n",
    "cat_submission = pd.DataFrame({\n",
    "    id_col: test[id_col].values,     # \"Cattle_ID\"\n",
    "    target: final_ensemble_test      # \"Milk_Yield_L\"\n",
    "})\n",
    "\n",
    "save_submission(cat_submission, run_name=\"felipe_catboost_catensemble\")\n",
    "print(\"Saved submission_cat_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0db3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM categorical features: ['Breed', 'Climate_Zone', 'Management_System', 'Lactation_Stage', 'Feed_Type']\n",
      "\n",
      "----- LightGBM Fold 1 -----\n",
      "LightGBM Fold 1 RMSE: 5.3580\n",
      "  -> best_iteration: 45\n",
      "\n",
      "----- LightGBM Fold 2 -----\n",
      "LightGBM Fold 2 RMSE: 5.3416\n",
      "  -> best_iteration: 7\n",
      "\n",
      "----- LightGBM Fold 3 -----\n",
      "LightGBM Fold 3 RMSE: 5.3441\n",
      "  -> best_iteration: 1\n",
      "\n",
      "----- LightGBM Fold 4 -----\n",
      "LightGBM Fold 4 RMSE: 5.3530\n",
      "  -> best_iteration: 21\n",
      "\n",
      "----- LightGBM Fold 5 -----\n",
      "LightGBM Fold 5 RMSE: 5.3188\n",
      "  -> best_iteration: 32\n",
      "\n",
      "==========================\n",
      "LightGBM Average RMSE: 5.3431\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_rmse = []\n",
    "models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n----- Fold {fold+1} -----\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    fold_rmse.append(rmse)\n",
    "\n",
    "    print(f\"Fold {fold+1} RMSE: {rmse:.4f}\")\n",
    "    models.append(model)\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(f\"Average RMSE: {np.mean(fold_rmse):.4f}\")\n",
    "print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing loaded train/test...\n",
      "\n",
      "----- XGBoost Fold 1 -----\n",
      "Fold 1 RMSE: 4.1888\n",
      "\n",
      "----- XGBoost Fold 2 -----\n",
      "Fold 2 RMSE: 4.1517\n",
      "\n",
      "----- XGBoost Fold 3 -----\n",
      "Fold 3 RMSE: 4.1568\n",
      "\n",
      "----- XGBoost Fold 4 -----\n",
      "Fold 4 RMSE: 4.1814\n",
      "\n",
      "----- XGBoost Fold 5 -----\n",
      "Fold 5 RMSE: 4.1694\n",
      "\n",
      "==========================\n",
      "XGBoost Average RMSE = 4.1696\n",
      "==========================\n",
      "\n",
      "submission_xgb.csv created!\n"
     ]
    }
   ],
   "source": [
    "# --- Reuse already-loaded train/test if they exist ---\n",
    "# If not, load again\n",
    "try:\n",
    "    train\n",
    "    test\n",
    "    print(\"Reusing loaded train/test...\")\n",
    "except NameError:\n",
    "    print(\"Loading data fresh...\")\n",
    "    train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "    test = pd.read_csv(\"../../data/cattle_data_test.csv\")\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].astype(\"category\").cat.codes\n",
    "    return df\n",
    "\n",
    "train_prep = preprocess(train.drop(columns=[target]))\n",
    "test_prep = preprocess(test)\n",
    "\n",
    "X = train_prep\n",
    "y = train[target]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_rmse = []\n",
    "xgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n----- XGBoost Fold {fold+1} -----\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=700,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",      # fast for large data\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    fold_rmse.append(rmse)\n",
    "    \n",
    "    print(f\"Fold {fold+1} RMSE: {rmse:.4f}\")\n",
    "    xgb_models.append(model)\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(f\"XGBoost Average RMSE = {np.mean(fold_rmse):.4f}\")\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "# --- Train final model on all data ---\n",
    "final_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_xgb.fit(X, y)\n",
    "\n",
    "# --- Submission ---\n",
    "test_preds = final_xgb.predict(test_prep)\n",
    "\n",
    "submission_xgb = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: test_preds\n",
    "})\n",
    "\n",
    "# save_submission(submission, run_name=\"felipe_model\")\n",
    "print(\"submission_xgb.csv created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b54e9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET STRUCTURE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. DATE PATTERNS:\n",
      "Date range: 2022-01-01 00:00:00 to 2024-12-30 00:00:00\n",
      "Unique dates: 1095\n",
      "Total rows: 210000\n",
      "\n",
      "Dates per cattle (sample 5):\n",
      "Cattle_ID\n",
      "CATTLE_000001    1\n",
      "CATTLE_000002    1\n",
      "CATTLE_000003    1\n",
      "CATTLE_000004    1\n",
      "CATTLE_000005    1\n",
      "Name: Date, dtype: int64\n",
      "\n",
      "2. FARM PATTERNS:\n",
      "Unique farms: 1000\n",
      "Avg rows per farm: 210.0\n",
      "\n",
      "Top 5 farms by count:\n",
      "Farm_ID\n",
      "FARM_0842    252\n",
      "FARM_0724    251\n",
      "FARM_0405    250\n",
      "FARM_0571    250\n",
      "FARM_0937    249\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. BREED PATTERNS:\n",
      "Breed\n",
      "Holstein        104775\n",
      "Jersey           42183\n",
      "Guernsey         31672\n",
      "Brown Swiss      31155\n",
      "Holstien           112\n",
      " Brown Swiss        57\n",
      "Brown Swiss         46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. CLIMATE × BREED:\n",
      "Breed           Brown Swiss  Brown Swiss  Brown Swiss   Guernsey  Holstein  \\\n",
      "Climate_Zone                                                                 \n",
      "Arid                     12         5235             7      5190     17428   \n",
      "Continental               5         5250             6      5255     17364   \n",
      "Mediterranean            11         5076             8      5302     17532   \n",
      "Subtropical               9         5129             5      5245     17516   \n",
      "Temperate                12         5162            11      5342     17599   \n",
      "Tropical                  8         5303             9      5338     17336   \n",
      "\n",
      "Breed          Holstien  Jersey  \n",
      "Climate_Zone                     \n",
      "Arid                 12    7070  \n",
      "Continental          22    6927  \n",
      "Mediterranean        17    7048  \n",
      "Subtropical          19    7014  \n",
      "Temperate            22    7076  \n",
      "Tropical             20    7048  \n",
      "\n",
      "5. TOP CORRELATIONS WITH TARGET:\n",
      "Milk_Yield_L               1.000000\n",
      "Weight_kg                  0.300464\n",
      "Feed_Quantity_lb           0.223631\n",
      "Feed_Quantity_kg           0.223288\n",
      "Water_Intake_L             0.124911\n",
      "Rumination_Time_hrs        0.089823\n",
      "Previous_Week_Avg_Yield    0.089823\n",
      "IBR_Vaccine                0.072263\n",
      "Anthrax_Vaccine            0.069642\n",
      "Rabies_Vaccine             0.068618\n",
      "Name: Milk_Yield_L, dtype: float64\n",
      "\n",
      "6. NEGATIVE CORRELATIONS:\n",
      "BQ_Vaccine              -0.000466\n",
      "Walking_Distance_km     -0.001538\n",
      "Body_Condition_Score    -0.001647\n",
      "Resting_Hours           -0.001653\n",
      "FMD_Vaccine             -0.002477\n",
      "Ambient_Temperature_C   -0.042036\n",
      "Days_in_Milk            -0.062554\n",
      "Mastitis                -0.122614\n",
      "Parity                  -0.236565\n",
      "Age_Months              -0.309188\n",
      "Name: Milk_Yield_L, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Date analysis\n",
    "print(\"\\n1. DATE PATTERNS:\")\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "print(f\"Date range: {train['Date'].min()} to {train['Date'].max()}\")\n",
    "print(f\"Unique dates: {train['Date'].nunique()}\")\n",
    "print(f\"Total rows: {len(train)}\")\n",
    "print(f\"\\nDates per cattle (sample 5):\")\n",
    "print(train.groupby('Cattle_ID')['Date'].nunique().head())\n",
    "\n",
    "# Farm analysis\n",
    "print(\"\\n2. FARM PATTERNS:\")\n",
    "print(f\"Unique farms: {train['Farm_ID'].nunique()}\")\n",
    "print(f\"Avg rows per farm: {len(train) / train['Farm_ID'].nunique():.1f}\")\n",
    "print(f\"\\nTop 5 farms by count:\")\n",
    "print(train['Farm_ID'].value_counts().head())\n",
    "\n",
    "# Breed analysis\n",
    "print(\"\\n3. BREED PATTERNS:\")\n",
    "print(train['Breed'].value_counts())\n",
    "\n",
    "# Climate analysis\n",
    "print(\"\\n4. CLIMATE × BREED:\")\n",
    "print(pd.crosstab(train['Climate_Zone'], train['Breed']))\n",
    "\n",
    "# Correlation with target\n",
    "print(\"\\n5. TOP CORRELATIONS WITH TARGET:\")\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns\n",
    "corr = train[numeric_cols].corr()['Milk_Yield_L'].sort_values(ascending=False)\n",
    "print(corr.head(10))\n",
    "\n",
    "print(\"\\n6. NEGATIVE CORRELATIONS:\")\n",
    "print(corr.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipebenitez/ML-Project/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-11-17 18:36:04,762] A new study created in memory with name: no-name-1a8eea34-828a-47e9-9eb1-d7daa167ceb6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (209926, 41)\n",
      "Test shape: (40000, 41)\n",
      "CatBoost categorical features: ['Breed', 'Climate_Zone', 'Management_System', 'Lactation_Stage', 'Feed_Type']\n",
      "\n",
      "Starting Optuna hyperparameter search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:   2%|▎         | 1/40 [04:02<2:37:25, 242.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:40:06,955] Trial 0 finished with value: 4.1066350616145835 and parameters: {'depth': 7, 'learning_rate': 0.030033228718532787, 'l2_leaf_reg': 1.7953972301724823, 'subsample': 0.7796965026065017, 'random_strength': 1.0696793682056738, 'bagging_temperature': 0.9530309362394171}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:   5%|▌         | 2/40 [04:12<1:07:03, 105.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:40:17,426] Trial 1 finished with value: 4.1069651873737705 and parameters: {'depth': 6, 'learning_rate': 0.03246286800397102, 'l2_leaf_reg': 6.934585342403436, 'subsample': 0.8892560134025628, 'random_strength': 1.9009017430415895, 'bagging_temperature': 0.0612661117999308}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:   8%|▊         | 3/40 [08:24<1:46:31, 172.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:44:29,731] Trial 3 finished with value: 4.107981500145322 and parameters: {'depth': 5, 'learning_rate': 0.03060417129254867, 'l2_leaf_reg': 3.8027511028361998, 'subsample': 0.7628897889666677, 'random_strength': 4.325941286066591, 'bagging_temperature': 0.7786329332665413}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  10%|█         | 4/40 [08:37<1:05:46, 109.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:44:42,597] Trial 2 finished with value: 4.10740570764789 and parameters: {'depth': 5, 'learning_rate': 0.024493405366674076, 'l2_leaf_reg': 3.6353347752597167, 'subsample': 0.707230473010704, 'random_strength': 2.2892214679264926, 'bagging_temperature': 0.2155123427828266}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  12%|█▎        | 5/40 [13:07<1:37:36, 167.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:49:12,271] Trial 5 finished with value: 4.107690360205317 and parameters: {'depth': 5, 'learning_rate': 0.028564706151943724, 'l2_leaf_reg': 2.3626314183815476, 'subsample': 0.7763443417159476, 'random_strength': 3.032977362587531, 'bagging_temperature': 0.5507477981809029}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  15%|█▌        | 6/40 [13:13<1:03:38, 112.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:49:17,783] Trial 4 finished with value: 4.106676906203669 and parameters: {'depth': 6, 'learning_rate': 0.021298257855780277, 'l2_leaf_reg': 6.460136786522908, 'subsample': 0.7349923254618709, 'random_strength': 1.2501129730356348, 'bagging_temperature': 0.5851409559207785}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  18%|█▊        | 7/40 [16:28<1:16:44, 139.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:52:33,364] Trial 6 finished with value: 4.106900723385433 and parameters: {'depth': 6, 'learning_rate': 0.032643554231493614, 'l2_leaf_reg': 5.1126445494184285, 'subsample': 0.7639782599311191, 'random_strength': 0.7424764599661797, 'bagging_temperature': 0.5274361850636387}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  20%|██        | 8/40 [16:55<55:18, 103.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:53:00,319] Trial 7 finished with value: 4.108050508456788 and parameters: {'depth': 6, 'learning_rate': 0.03715447153262421, 'l2_leaf_reg': 3.1202924983964415, 'subsample': 0.8823717249230913, 'random_strength': 2.737813060872654, 'bagging_temperature': 0.832679423449802}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  22%|██▎       | 9/40 [19:43<1:03:53, 123.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:55:47,850] Trial 8 finished with value: 4.10712894781373 and parameters: {'depth': 7, 'learning_rate': 0.0372063943972149, 'l2_leaf_reg': 3.066826907246639, 'subsample': 0.7433660606551039, 'random_strength': 0.92382483862985, 'bagging_temperature': 0.9435129263785973}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  25%|██▌       | 10/40 [20:40<51:40, 103.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:56:45,703] Trial 9 finished with value: 4.108473595370948 and parameters: {'depth': 6, 'learning_rate': 0.03612595478530639, 'l2_leaf_reg': 2.421872652069468, 'subsample': 0.8598219688266027, 'random_strength': 4.706821695030949, 'bagging_temperature': 0.29411014570143235}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  28%|██▊       | 11/40 [23:29<59:35, 123.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 18:59:34,295] Trial 10 finished with value: 4.107869715812108 and parameters: {'depth': 5, 'learning_rate': 0.03830312520468619, 'l2_leaf_reg': 5.112357991065945, 'subsample': 0.7935863532464469, 'random_strength': 4.266484446357278, 'bagging_temperature': 0.6264913331873335}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  30%|███       | 12/40 [25:43<59:01, 126.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:01:47,976] Trial 11 finished with value: 4.1078967863134634 and parameters: {'depth': 7, 'learning_rate': 0.027287662103531236, 'l2_leaf_reg': 1.00895727744874, 'subsample': 0.8167621612992455, 'random_strength': 3.841572475573071, 'bagging_temperature': 0.7293196793346711}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  32%|███▎      | 13/40 [28:54<1:05:42, 146.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:04:59,014] Trial 12 finished with value: 4.1068503307044795 and parameters: {'depth': 7, 'learning_rate': 0.022000819616776045, 'l2_leaf_reg': 1.3317752666375293, 'subsample': 0.8233925456406492, 'random_strength': 1.5769911781443726, 'bagging_temperature': 0.9836198146410235}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  35%|███▌      | 14/40 [31:21<1:03:28, 146.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:07:26,605] Trial 13 finished with value: 4.106876422073571 and parameters: {'depth': 7, 'learning_rate': 0.022202272557925288, 'l2_leaf_reg': 5.6294485257075175, 'subsample': 0.7253082034156811, 'random_strength': 1.4731605087905866, 'bagging_temperature': 0.9979819308612154}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  38%|███▊      | 15/40 [33:42<1:00:17, 144.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:09:47,167] Trial 14 finished with value: 4.106954616492338 and parameters: {'depth': 7, 'learning_rate': 0.025607587308905125, 'l2_leaf_reg': 6.51590579058398, 'subsample': 0.7184142194838564, 'random_strength': 1.32515565201582, 'bagging_temperature': 0.3912123875074608}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.10664:  40%|████      | 16/40 [35:28<53:17, 133.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:11:33,713] Trial 15 finished with value: 4.106740958173306 and parameters: {'depth': 6, 'learning_rate': 0.02626299033290152, 'l2_leaf_reg': 6.786081886657344, 'subsample': 0.7002672200682457, 'random_strength': 0.5709183840263852, 'bagging_temperature': 0.3092743770314209}. Best is trial 0 with value: 4.1066350616145835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  42%|████▎     | 17/40 [38:45<58:22, 152.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:14:50,393] Trial 16 finished with value: 4.106398510192625 and parameters: {'depth': 6, 'learning_rate': 0.02006010827962057, 'l2_leaf_reg': 4.434343507978291, 'subsample': 0.7483861301411402, 'random_strength': 0.8777706129477674, 'bagging_temperature': 0.6453948564266518}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  45%|████▌     | 18/40 [38:58<40:30, 110.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:15:03,526] Trial 17 finished with value: 4.107661171901893 and parameters: {'depth': 6, 'learning_rate': 0.03334803917868082, 'l2_leaf_reg': 4.507009586304742, 'subsample': 0.7485421714075146, 'random_strength': 2.1481771797298777, 'bagging_temperature': 0.6734256967656533}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  48%|████▊     | 19/40 [42:46<50:58, 145.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:18:51,009] Trial 18 finished with value: 4.10756628257958 and parameters: {'depth': 7, 'learning_rate': 0.03371968895642867, 'l2_leaf_reg': 4.519233529009053, 'subsample': 0.7938956325903803, 'random_strength': 2.230417344316034, 'bagging_temperature': 0.6893165924057587}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  50%|█████     | 20/40 [43:36<39:00, 117.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:19:41,369] Trial 19 finished with value: 4.107437874958039 and parameters: {'depth': 7, 'learning_rate': 0.02981163612740578, 'l2_leaf_reg': 1.877287177795457, 'subsample': 0.8060920787883783, 'random_strength': 2.9633146331715228, 'bagging_temperature': 0.8628879909835876}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  52%|█████▎    | 21/40 [47:20<47:10, 148.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:23:24,793] Trial 20 finished with value: 4.107400725180578 and parameters: {'depth': 7, 'learning_rate': 0.030018802385249874, 'l2_leaf_reg': 1.6317341049787633, 'subsample': 0.8410941602515147, 'random_strength': 3.3178159118097934, 'bagging_temperature': 0.845108030783518}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  55%|█████▌    | 22/40 [48:33<37:54, 126.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:24:38,404] Trial 21 finished with value: 4.106536433303795 and parameters: {'depth': 6, 'learning_rate': 0.02340955831094361, 'l2_leaf_reg': 2.815489644128715, 'subsample': 0.8487654758428467, 'random_strength': 0.5052836552908216, 'bagging_temperature': 0.42024609910984945}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  57%|█████▊    | 23/40 [52:44<46:24, 163.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:28:49,502] Trial 22 finished with value: 4.106669315031203 and parameters: {'depth': 6, 'learning_rate': 0.020533797958888145, 'l2_leaf_reg': 5.840037868006504, 'subsample': 0.7393182725960709, 'random_strength': 1.1678961110629231, 'bagging_temperature': 0.4221993761406658}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  60%|██████    | 24/40 [53:18<33:17, 124.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:29:23,434] Trial 23 finished with value: 4.106739718599835 and parameters: {'depth': 6, 'learning_rate': 0.023423180609812397, 'l2_leaf_reg': 2.663993896088838, 'subsample': 0.8407637958717706, 'random_strength': 0.5116175060830896, 'bagging_temperature': 0.41124032305142444}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  62%|██████▎   | 25/40 [57:20<39:59, 159.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:33:25,291] Trial 24 finished with value: 4.1065106290625675 and parameters: {'depth': 6, 'learning_rate': 0.023740225569556593, 'l2_leaf_reg': 2.8288747962637615, 'subsample': 0.846071175407146, 'random_strength': 0.699342275403859, 'bagging_temperature': 0.45617487034191834}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  65%|██████▌   | 26/40 [58:44<32:01, 137.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:34:49,557] Trial 25 finished with value: 4.106680765873017 and parameters: {'depth': 6, 'learning_rate': 0.020173976783207856, 'l2_leaf_reg': 3.2853277559042393, 'subsample': 0.7777175316042767, 'random_strength': 1.0013652592519824, 'bagging_temperature': 0.1699275571019232}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  68%|██████▊   | 27/40 [1:02:51<36:51, 170.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:38:56,484] Trial 26 finished with value: 4.107059927548338 and parameters: {'depth': 6, 'learning_rate': 0.02003645553938417, 'l2_leaf_reg': 3.2807894436896374, 'subsample': 0.8631749102780778, 'random_strength': 1.7908882000908406, 'bagging_temperature': 0.46679314866346666}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  70%|███████   | 28/40 [1:03:19<25:29, 127.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:39:24,424] Trial 27 finished with value: 4.107344713976985 and parameters: {'depth': 5, 'learning_rate': 0.024021642693085724, 'l2_leaf_reg': 4.227111817093591, 'subsample': 0.8727227115802122, 'random_strength': 1.7502742930428385, 'bagging_temperature': 0.519740342805473}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 4.1064:  72%|███████▎  | 29/40 [1:07:54<31:29, 171.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:43:59,484] Trial 28 finished with value: 4.1071017667363 and parameters: {'depth': 5, 'learning_rate': 0.02324321093252745, 'l2_leaf_reg': 4.204741087809179, 'subsample': 0.8672360438472356, 'random_strength': 0.7853534773712, 'bagging_temperature': 0.3098809048154888}. Best is trial 16 with value: 4.106398510192625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  75%|███████▌  | 30/40 [1:08:23<21:29, 128.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:44:28,462] Trial 29 finished with value: 4.106391174299612 and parameters: {'depth': 6, 'learning_rate': 0.022872270426979868, 'l2_leaf_reg': 4.014881862534349, 'subsample': 0.847236734294946, 'random_strength': 0.731107887920747, 'bagging_temperature': 0.4635035306948895}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  78%|███████▊  | 31/40 [1:12:11<23:48, 158.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:48:16,577] Trial 30 finished with value: 4.106742185275502 and parameters: {'depth': 6, 'learning_rate': 0.025698965440708266, 'l2_leaf_reg': 2.119365465321693, 'subsample': 0.842736469388962, 'random_strength': 0.5156466484870262, 'bagging_temperature': 0.6070860571369767}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  80%|████████  | 32/40 [1:13:23<17:40, 132.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:49:28,065] Trial 31 finished with value: 4.10691715767228 and parameters: {'depth': 6, 'learning_rate': 0.025326241348709796, 'l2_leaf_reg': 3.6236468599762377, 'subsample': 0.8998197254947831, 'random_strength': 1.0591608371136803, 'bagging_temperature': 0.6096925441164875}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  82%|████████▎ | 33/40 [1:17:29<19:26, 166.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:53:34,377] Trial 32 finished with value: 4.106845225980416 and parameters: {'depth': 6, 'learning_rate': 0.02253353547453913, 'l2_leaf_reg': 2.7390716617616846, 'subsample': 0.8534606754827547, 'random_strength': 1.0411471456831247, 'bagging_temperature': 0.4664352766106681}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  85%|████████▌ | 34/40 [1:18:29<13:26, 134.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:54:33,832] Trial 33 finished with value: 4.1065102453425855 and parameters: {'depth': 6, 'learning_rate': 0.022473152362790745, 'l2_leaf_reg': 2.7648812978803323, 'subsample': 0.825516159876847, 'random_strength': 0.8207204129662253, 'bagging_temperature': 0.46535814766909717}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  88%|████████▊ | 35/40 [1:22:58<14:34, 174.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 19:59:03,309] Trial 34 finished with value: 4.106499338330347 and parameters: {'depth': 6, 'learning_rate': 0.02144219023403332, 'l2_leaf_reg': 4.672094674220013, 'subsample': 0.8232211434553708, 'random_strength': 0.7663706319680869, 'bagging_temperature': 0.3548656280325466}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  90%|█████████ | 36/40 [1:24:12<09:38, 144.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 20:00:17,401] Trial 35 finished with value: 4.10674606513646 and parameters: {'depth': 6, 'learning_rate': 0.021437969491837437, 'l2_leaf_reg': 4.8274965453274294, 'subsample': 0.8272245302831016, 'random_strength': 1.4455312399626012, 'bagging_temperature': 0.3528952583168568}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  92%|█████████▎| 37/40 [1:28:52<09:16, 185.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 20:04:57,708] Trial 36 finished with value: 4.106682045624402 and parameters: {'depth': 6, 'learning_rate': 0.021366052730996142, 'l2_leaf_reg': 4.864301781371315, 'subsample': 0.8254602853352773, 'random_strength': 1.4110009768197087, 'bagging_temperature': 0.10366421105715207}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  95%|█████████▌| 38/40 [1:30:19<05:11, 155.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 20:06:24,329] Trial 37 finished with value: 4.106685214153209 and parameters: {'depth': 6, 'learning_rate': 0.021224595425353376, 'l2_leaf_reg': 3.7288952126765063, 'subsample': 0.8079377679075879, 'random_strength': 2.0751816513193564, 'bagging_temperature': 0.20718472942570026}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639:  98%|█████████▊| 39/40 [1:33:43<02:50, 170.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 20:09:48,357] Trial 38 finished with value: 4.107314015070822 and parameters: {'depth': 5, 'learning_rate': 0.024778472112600383, 'l2_leaf_reg': 3.805345305329153, 'subsample': 0.8100157870165497, 'random_strength': 1.9927500915285554, 'bagging_temperature': 0.2433839797830411}. Best is trial 29 with value: 4.106391174299612.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 4.10639: 100%|██████████| 40/40 [1:34:29<00:00, 141.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-17 20:10:33,822] Trial 39 finished with value: 4.107113798299714 and parameters: {'depth': 5, 'learning_rate': 0.02733868841994063, 'l2_leaf_reg': 5.343837851160658, 'subsample': 0.7589913249369241, 'random_strength': 0.8732091640464774, 'bagging_temperature': 0.5614393962636348}. Best is trial 29 with value: 4.106391174299612.\n",
      "\n",
      "Optuna search complete.\n",
      "Best CV RMSE from Optuna: 4.106391174299612\n",
      "Best params: {'depth': 6, 'learning_rate': 0.022872270426979868, 'l2_leaf_reg': 4.014881862534349, 'subsample': 0.847236734294946, 'random_strength': 0.731107887920747, 'bagging_temperature': 0.4635035306948895}\n",
      "\n",
      "Training final 5-fold ensemble with best_params...\n",
      "\n",
      "----- Final CV Fold 1 -----\n",
      "Fold 1 RMSE: 4.1070, best_iter=1157\n",
      "\n",
      "----- Final CV Fold 2 -----\n",
      "Fold 2 RMSE: 4.1002, best_iter=1127\n",
      "\n",
      "----- Final CV Fold 3 -----\n",
      "Fold 3 RMSE: 4.1201, best_iter=1093\n",
      "\n",
      "----- Final CV Fold 4 -----\n",
      "Fold 4 RMSE: 4.1063, best_iter=1094\n",
      "\n",
      "----- Final CV Fold 5 -----\n",
      "Fold 5 RMSE: 4.0984, best_iter=999\n",
      "\n",
      "===============================\n",
      "Final 5-fold ensemble CV RMSE: 4.1064\n",
      "Fold RMSEs: [np.float64(4.106956550228758), np.float64(4.100151472353626), np.float64(4.120099489769896), np.float64(4.106327628323443), np.float64(4.0984207308223395)]\n",
      "Best iters: [1157, 1127, 1093, 1094, 999]\n",
      "===============================\n",
      "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_optuna_cv_only__2025-11-17__20-14-39.csv\n",
      "Saved CV-only submission: felipe_catboost_optuna_cv_only\n",
      "\n",
      "Average best_iteration across folds: 1094\n",
      "Using n_estimators for full-data models: 1094\n",
      "\n",
      "Training full-data model with seed=101...\n",
      "\n",
      "Training full-data model with seed=202...\n",
      "\n",
      "Training full-data model with seed=303...\n",
      "\n",
      "Training full-data model with seed=404...\n",
      "\n",
      "Training full-data model with seed=505...\n",
      "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_optuna_full_seed_only__2025-11-17__20-19-08.csv\n",
      "Saved full-data seed ensemble submission: felipe_catboost_optuna_full_seed_only\n",
      "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_optuna_blend_70_30__2025-11-17__20-19-08.csv\n",
      "Saved blend 70/30 submission: felipe_catboost_optuna_blend_70_30\n",
      "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_optuna_blend_50_50__2025-11-17__20-19-08.csv\n",
      "Saved blend 50/50 submission: felipe_catboost_optuna_blend_50_50\n",
      "\n",
      "\n",
      "=== Running OOF comparison for strategies (this is offline, NOT Kaggle) ===\n",
      "\n",
      "[OOF Eval] Fold 1\n",
      "\n",
      "[OOF Eval] Fold 2\n",
      "\n",
      "[OOF Eval] Fold 3\n",
      "\n",
      "[OOF Eval] Fold 4\n",
      "\n",
      "[OOF Eval] Fold 5\n",
      "\n",
      "=== OOF RMSE summary (lower is better) ===\n",
      "CV-only ensemble OOF RMSE:        4.106398\n",
      "Full-seed-style ensemble OOF RMSE: 4.106304\n",
      "Blend 70% CV / 30% full OOF RMSE: 4.106199\n",
      "Blend 50% CV / 50% full OOF RMSE: 4.106148\n",
      "Note: CV-only OOF RMSE should closely match 'Final 5-fold ensemble CV RMSE' above.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb      # still imported in case you need it later\n",
    "import xgboost as xgb       # same\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import submission_utils\n",
    "importlib.reload(submission_utils)\n",
    "from submission_utils import save_submission\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import optuna  # make sure you have `pip install optuna` first\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 0) Load data\n",
    "# ======================================================\n",
    "\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "test = pd.read_csv(\"../../data/cattle_data_test.csv\")\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1) Your existing preprocessing pipeline (UNCHANGED)\n",
    "# ======================================================\n",
    "\n",
    "def preprocess_pipeline(df, encode_flag=True, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "    \n",
    "    # -----------------------\n",
    "    # 1) Drop impossible targets (train only)\n",
    "    # -----------------------\n",
    "    if target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0]\n",
    "        y = milk_features[target_col]\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Basic cleaning\n",
    "    # -----------------------\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    \n",
    "    # Fill any remaining numeric NaNs\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    milk_features[numeric_cols] = milk_features[numeric_cols].fillna(milk_features[numeric_cols].median())\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Date features\n",
    "    # -----------------------\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"])\n",
    "        milk_features[\"year\"] = milk_features[\"Date\"].dt.year\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"is_weekend\"] = milk_features[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "        milk_features[\"date_ordinal\"] = milk_features[\"Date\"].map(pd.Timestamp.toordinal)\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "    \n",
    "    # -----------------------\n",
    "    # 5) Farm clustering (your current version)\n",
    "    # -----------------------\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        # Use only numeric features for clustering (exclude IDs)\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "\n",
    "        # Aggregate per farm\n",
    "        farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "\n",
    "        # Scale and cluster farms\n",
    "        scaler = StandardScaler()\n",
    "        farm_scaled = scaler.fit_transform(farm_features)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "\n",
    "        # Map back to rows\n",
    "        milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # 6) Drop raw IDs\n",
    "    # -----------------------\n",
    "    drop_cols = [\"Cattle_ID\", \"Farm_ID\"]\n",
    "    milk_features = milk_features.drop(columns=[c for c in drop_cols if c in milk_features.columns])\n",
    "\n",
    "    # -----------------------\n",
    "    # 7) Optional one-hot encoding (we'll keep it OFF for CatBoost)\n",
    "    # -----------------------\n",
    "    if encode_flag:\n",
    "        cat_cols = milk_features.select_dtypes(include=\"object\").columns.tolist()\n",
    "        milk_features = pd.get_dummies(milk_features, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    # Make sure Farm_Cluster exists even if something went weird\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    final_df = milk_features.copy()\n",
    "    return final_df, y\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2) Build X, y, test_df (frozen pipeline)\n",
    "# ======================================================\n",
    "\n",
    "X, y = preprocess_pipeline(train, encode_flag=False, target_col=target)\n",
    "test_df, _ = preprocess_pipeline(test, encode_flag=False, target_col=None)\n",
    "\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# Categorical columns for CatBoost (from the processed X)\n",
    "cat_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(\"CatBoost categorical features:\", cat_features)\n",
    "\n",
    "# KFold for everything\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3) Optuna hyperparameter search (5-fold CV)\n",
    "# ======================================================\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 5, 7),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.04),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 7.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.9),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.5, 5.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        # you can add more (e.g., border_count) if desired\n",
    "    }\n",
    "\n",
    "    fold_rmses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=3000,            # big cap, rely on early stopping\n",
    "            early_stopping_rounds=100,\n",
    "            random_seed=42,\n",
    "            thread_count=4,\n",
    "            verbose=False,                # keep Optuna runs quiet\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=cat_features,   # using column names\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    mean_rmse = float(np.mean(fold_rmses))\n",
    "    return mean_rmse\n",
    "\n",
    "\n",
    "# You can bump this if you want; 40–60 is a nice compromise\n",
    "N_TRIALS = 40  \n",
    "\n",
    "print(\"\\nStarting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=N_TRIALS, n_jobs=2, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_cv_rmse = study.best_value\n",
    "\n",
    "print(\"\\nOptuna search complete.\")\n",
    "print(\"Best CV RMSE from Optuna:\", best_cv_rmse)\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4) Final 5-fold CV ensemble with best_params\n",
    "# ======================================================\n",
    "\n",
    "cv_models = []\n",
    "fold_rmses = []\n",
    "fold_best_iters = []\n",
    "\n",
    "print(\"\\nTraining final 5-fold ensemble with best_params...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n----- Final CV Fold {fold} -----\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # use SAME seed as Optuna for consistency\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        **best_params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    best_iter = model.get_best_iteration()\n",
    "\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}, best_iter={best_iter}\")\n",
    "\n",
    "    fold_rmses.append(rmse)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    cv_models.append(model)\n",
    "\n",
    "final_cv_rmse = float(np.mean(fold_rmses))\n",
    "print(\"\\n===============================\")\n",
    "print(f\"Final 5-fold ensemble CV RMSE: {final_cv_rmse:.4f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "\n",
    "# sanity check vs Optuna's own CV\n",
    "if final_cv_rmse > best_cv_rmse + 0.001:\n",
    "    print(\"⚠️ WARNING: Final CV RMSE is worse than Optuna's best!\")\n",
    "    print(f\"Optuna best: {best_cv_rmse:.4f}, Final CV: {final_cv_rmse:.4f}\")\n",
    "    print(\"This might indicate some instability / differences in folds.\")\n",
    "print(\"===============================\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5) CV-ensemble predictions on test_df (SAFE baseline)\n",
    "# ======================================================\n",
    "\n",
    "cv_test_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for model in cv_models:\n",
    "    cv_test_preds += model.predict(test_df)\n",
    "\n",
    "cv_test_preds /= len(cv_models)\n",
    "\n",
    "# This is your safest, fully CV-validated prediction vector\n",
    "sub_cv = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: cv_test_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_cv, run_name=\"felipe_catboost_optuna_cv_only\")\n",
    "print(\"Saved CV-only submission: felipe_catboost_optuna_cv_only\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6) Optional: full-data multi-seed ensemble\n",
    "# ======================================================\n",
    "\n",
    "avg_best_iter = int(np.mean(fold_best_iters))\n",
    "print(\"\\nAverage best_iteration across folds:\", avg_best_iter)\n",
    "\n",
    "n_estimators_full = avg_best_iter\n",
    "print(\"Using n_estimators for full-data models:\", n_estimators_full)\n",
    "\n",
    "seed_list = [101, 202, 303, 404, 505]\n",
    "full_seed_models = []\n",
    "full_seed_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for seed in seed_list:\n",
    "    print(f\"\\nTraining full-data model with seed={seed}...\")\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=n_estimators_full,\n",
    "        random_seed=seed,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        **best_params\n",
    "        # no early_stopping_rounds and no eval_set here\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    full_seed_models.append(model)\n",
    "    full_seed_preds += model.predict(test_df)\n",
    "\n",
    "full_seed_preds /= len(seed_list)\n",
    "\n",
    "sub_full = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: full_seed_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_full, run_name=\"felipe_catboost_optuna_full_seed_only\")\n",
    "print(\"Saved full-data seed ensemble submission: felipe_catboost_optuna_full_seed_only\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 7) Blended submissions (CV + full-data)\n",
    "# ======================================================\n",
    "\n",
    "# 70% CV ensemble, 30% full-data ensemble\n",
    "alpha_70 = 0.7\n",
    "blend_70_30 = alpha_70 * cv_test_preds + (1.0 - alpha_70) * full_seed_preds\n",
    "\n",
    "sub_blend_70_30 = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: blend_70_30\n",
    "})\n",
    "save_submission(sub_blend_70_30, run_name=\"felipe_catboost_optuna_blend_70_30\")\n",
    "print(\"Saved blend 70/30 submission: felipe_catboost_optuna_blend_70_30\")\n",
    "\n",
    "# 50% / 50% blend\n",
    "alpha_50 = 0.5\n",
    "blend_50_50 = alpha_50 * cv_test_preds + (1.0 - alpha_50) * full_seed_preds\n",
    "\n",
    "sub_blend_50_50 = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: blend_50_50\n",
    "})\n",
    "save_submission(sub_blend_50_50, run_name=\"felipe_catboost_optuna_blend_50_50\")\n",
    "print(\"Saved blend 50/50 submission: felipe_catboost_optuna_blend_50_50\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 8) EXTRA: OOF RMSE comparison of strategies\n",
    "#     (CV-only vs \"full-seed style\" vs blends)\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\\n=== Running OOF comparison for strategies (this is offline, NOT Kaggle) ===\")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# OOF containers (same length as y)\n",
    "cv_only_oof        = np.zeros(len(X), dtype=float)\n",
    "full_style_oof     = np.zeros(len(X), dtype=float)\n",
    "blend_70_30_oof    = np.zeros(len(X), dtype=float)\n",
    "blend_50_50_oof    = np.zeros(len(X), dtype=float)\n",
    "\n",
    "# Re-use same KFold splits and same cv_models (order aligned with enumerate start=1)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n[OOF Eval] Fold {fold}\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # 1) CV-only predictions (from already trained cv_models)\n",
    "    cv_model = cv_models[fold - 1]\n",
    "    cv_preds_val = cv_model.predict(X_val)\n",
    "\n",
    "    # 2) \"Full-seed style\" ensemble trained ONLY on X_train (no leakage)\n",
    "    fold_full_preds = np.zeros(len(val_idx), dtype=float)\n",
    "    for seed in seed_list:\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=n_estimators_full,\n",
    "            random_seed=seed,\n",
    "            verbose=False,\n",
    "            thread_count=4,\n",
    "            **best_params\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cat_features=cat_features,\n",
    "            verbose=False\n",
    "        )\n",
    "        fold_full_preds += model.predict(X_val)\n",
    "\n",
    "    fold_full_preds /= len(seed_list)\n",
    "\n",
    "    # Store OOF preds\n",
    "    cv_only_oof[val_idx]     = cv_preds_val\n",
    "    full_style_oof[val_idx]  = fold_full_preds\n",
    "    blend_70_30_oof[val_idx] = alpha_70 * cv_preds_val + (1.0 - alpha_70) * fold_full_preds\n",
    "    blend_50_50_oof[val_idx] = alpha_50 * cv_preds_val + (1.0 - alpha_50) * fold_full_preds\n",
    "\n",
    "# Compute OOF RMSEs for each strategy\n",
    "rmse_cv_only     = rmse(y, cv_only_oof)\n",
    "rmse_full_style  = rmse(y, full_style_oof)\n",
    "rmse_blend_70_30 = rmse(y, blend_70_30_oof)\n",
    "rmse_blend_50_50 = rmse(y, blend_50_50_oof)\n",
    "\n",
    "print(\"\\n=== OOF RMSE summary (lower is better) ===\")\n",
    "print(f\"CV-only ensemble OOF RMSE:        {rmse_cv_only:.6f}\")\n",
    "print(f\"Full-seed-style ensemble OOF RMSE: {rmse_full_style:.6f}\")\n",
    "print(f\"Blend 70% CV / 30% full OOF RMSE: {rmse_blend_70_30:.6f}\")\n",
    "print(f\"Blend 50% CV / 50% full OOF RMSE: {rmse_blend_50_50:.6f}\")\n",
    "print(\"Note: CV-only OOF RMSE should closely match 'Final 5-fold ensemble CV RMSE' above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb      # still imported in case you need it later\n",
    "import xgboost as xgb       # same\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import submission_utils\n",
    "importlib.reload(submission_utils)\n",
    "from submission_utils import save_submission\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import optuna  # make sure you have `pip install optuna` first\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 0) Load data\n",
    "# ======================================================\n",
    "\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "test = pd.read_csv(\"../../data/cattle_data_test.csv\")\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1) Your existing preprocessing pipeline (UNCHANGED)\n",
    "# ======================================================\n",
    "\n",
    "def preprocess_pipeline(df, encode_flag=True, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "    \n",
    "    # -----------------------\n",
    "    # 1) Drop impossible targets (train only)\n",
    "    # -----------------------\n",
    "    if target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0]\n",
    "        y = milk_features[target_col]\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Basic cleaning\n",
    "    # -----------------------\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    # Fill any remaining numeric NaNs\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    milk_features[numeric_cols] = milk_features[numeric_cols].fillna(milk_features[numeric_cols].median())\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Date features\n",
    "    # -----------------------\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"])\n",
    "        milk_features[\"year\"] = milk_features[\"Date\"].dt.year\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"is_weekend\"] = milk_features[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "        milk_features[\"date_ordinal\"] = milk_features[\"Date\"].map(pd.Timestamp.toordinal)\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "    \n",
    "    \n",
    "\n",
    "    # -----------------------\n",
    "    # 5) Farm clustering\n",
    "    # -----------------------\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        # Use only numeric features for clustering (exclude IDs)\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "\n",
    "        # Aggregate per farm\n",
    "        farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "\n",
    "        # Scale and cluster farms\n",
    "        scaler = StandardScaler()\n",
    "        farm_scaled = scaler.fit_transform(farm_features)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "\n",
    "        # Map back to rows\n",
    "        milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # INSERT: Weight_Efficiency_Z (Breed + Age_Year)\n",
    "    # -----------------------\n",
    "    if \"Weight_kg\" in milk_features.columns and \"Age_Months\" in milk_features.columns:\n",
    "        milk_features[\"Age_Year\"] = milk_features[\"Age_Months\"] // 12\n",
    "        group_cols = [\"Breed\", \"Age_Year\"]\n",
    "\n",
    "        # Calculate cohort stats using transform so it maps back to original rows\n",
    "        cohort_mean = milk_features.groupby(group_cols)[\"Weight_kg\"].transform(\"mean\")\n",
    "        cohort_std = milk_features.groupby(group_cols)[\"Weight_kg\"].transform(\"std\")\n",
    "\n",
    "        # Calculate Z-Score: How heavy is this cow relative to her peers?\n",
    "        milk_features[\"Weight_Efficiency_Z\"] = (\n",
    "            (milk_features[\"Weight_kg\"] - cohort_mean) / (cohort_std + 1e-5)\n",
    "        )\n",
    "\n",
    "        milk_features = milk_features.drop(columns=[\"Age_Year\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # 6) Drop raw IDs\n",
    "    # -----------------------\n",
    "    drop_cols = [\"Cattle_ID\"]\n",
    "    milk_features = milk_features.drop(columns=[c for c in drop_cols if c in milk_features.columns])\n",
    "\n",
    "    # -----------------------\n",
    "    # 7) Optional one-hot encoding (we'll keep it OFF for CatBoost)\n",
    "    # -----------------------\n",
    "    if encode_flag:\n",
    "        cat_cols = milk_features.select_dtypes(include=\"object\").columns.tolist()\n",
    "        milk_features = pd.get_dummies(milk_features, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    # Make sure Farm_Cluster exists even if something went weird\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    final_df = milk_features.copy()\n",
    "    return final_df, y\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2) Build X, y, test_df (frozen pipeline)\n",
    "# ======================================================\n",
    "\n",
    "X, y = preprocess_pipeline(train, encode_flag=False, target_col=target)\n",
    "test_df, _ = preprocess_pipeline(test, encode_flag=False, target_col=None)\n",
    "\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# REPLACEMENT: Farm_Performance Target Encoding (The Missing Signal)\n",
    "# ==============================================================================\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# FIX: Reset indices (Keep this from previous attempt, it worked)\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True) \n",
    "\n",
    "def add_target_encoding_features(train_df, test_df, y_train, target_col_name=\"Milk_Yield_L\", n_splits=5):\n",
    "    # CHANGE: Encode Farm_ID instead of Bio features\n",
    "    # This captures \"Managerial Quality\" - the specific effect of this farm on yield\n",
    "    encodings = {\n",
    "        \"Farm_Performance\": [\"Farm_ID\"]\n",
    "    }\n",
    "    \n",
    "    train_encoded = train_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    \n",
    "    # Combine X and y temporarily for the training split\n",
    "    train_temp = train_encoded.copy()\n",
    "    train_temp[target_col_name] = y_train.values\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for new_col, group_cols in encodings.items():\n",
    "        # --- A) TRAINING SET: K-Fold Encoding (Prevents Leakage) ---\n",
    "        train_encoded[new_col] = 0.0\n",
    "        \n",
    "        for tr_ind, val_ind in kf.split(train_temp):\n",
    "            X_tr, X_val = train_temp.iloc[tr_ind], train_temp.iloc[val_ind]\n",
    "            \n",
    "            # Calculate mean yield per Farm on the 'training' part\n",
    "            means = X_tr.groupby(group_cols)[target_col_name].mean()\n",
    "            \n",
    "            # Map to the 'validation' part\n",
    "            # Since it's just one column (Farm_ID), simple map works\n",
    "            train_encoded.loc[val_ind, new_col] = X_val[group_cols[0]].map(means).fillna(X_tr[target_col_name].mean())\n",
    "\n",
    "        # --- B) TEST SET: Global Mean Encoding ---\n",
    "        # Compute global farm stats from full training data\n",
    "        global_means = train_temp.groupby(group_cols)[target_col_name].mean()\n",
    "        global_avg = train_temp[target_col_name].mean()\n",
    "        \n",
    "        # Map to test set\n",
    "        test_encoded[new_col] = test_encoded[group_cols[0]].map(global_means).fillna(global_avg)\n",
    "\n",
    "    # OPTIONAL: Drop the raw Farm_ID now that we have the encoded score\n",
    "    # This keeps the model clean\n",
    "    train_encoded = train_encoded.drop(columns=[\"Farm_ID\"], errors='ignore')\n",
    "    test_encoded = test_encoded.drop(columns=[\"Farm_ID\"], errors='ignore')\n",
    "\n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Execute\n",
    "X, test_df = add_target_encoding_features(X, test_df, y)\n",
    "print(\"Added feature: Farm_Performance\")\n",
    "\n",
    "# ... Continue to CatBoost training ...\n",
    "\n",
    "\n",
    "\n",
    "# Categorical columns for CatBoost (from the processed X)\n",
    "cat_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(\"CatBoost categorical features:\", cat_features)\n",
    "\n",
    "# KFold for everything\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3) Optuna hyperparameter search (5-fold CV)\n",
    "# ======================================================\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 5, 8),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.04),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 7.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.9),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.5, 5.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        # you can add more (e.g., border_count) if desired\n",
    "    }\n",
    "\n",
    "    fold_rmses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=3000,            # big cap, rely on early stopping\n",
    "            early_stopping_rounds=100,\n",
    "            random_seed=42,\n",
    "            thread_count=5,\n",
    "            verbose=False,                # keep Optuna runs quiet\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=cat_features,   # using column names\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    mean_rmse = float(np.mean(fold_rmses))\n",
    "    return mean_rmse\n",
    "\n",
    "\n",
    "# You can bump this if you want; 40–60 is a nice compromise\n",
    "N_TRIALS = 40  \n",
    "\n",
    "print(\"\\nStarting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=N_TRIALS, n_jobs=4, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_cv_rmse = study.best_value\n",
    "\n",
    "print(\"\\nOptuna search complete.\")\n",
    "print(\"Best CV RMSE from Optuna:\", best_cv_rmse)\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4) Final 5-fold CV ensemble with best_params\n",
    "# ======================================================\n",
    "\n",
    "cv_models = []\n",
    "fold_rmses = []\n",
    "fold_best_iters = []\n",
    "\n",
    "print(\"\\nTraining final 5-fold ensemble with best_params...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n----- Final CV Fold {fold} -----\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # use SAME seed as Optuna for consistency\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=5,\n",
    "        verbose=False,\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",   # or \"0:1\" if he wants multi-GPU (rarely needed)\n",
    "        **best_params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    best_iter = model.get_best_iteration()\n",
    "\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}, best_iter={best_iter}\")\n",
    "\n",
    "    fold_rmses.append(rmse)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    cv_models.append(model)\n",
    "\n",
    "final_cv_rmse = float(np.mean(fold_rmses))\n",
    "print(\"\\n===============================\")\n",
    "print(f\"Final 5-fold ensemble CV RMSE: {final_cv_rmse:.4f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "\n",
    "# sanity check vs Optuna's own CV\n",
    "if final_cv_rmse > best_cv_rmse + 0.001:\n",
    "    print(\"⚠️ WARNING: Final CV RMSE is worse than Optuna's best!\")\n",
    "    print(f\"Optuna best: {best_cv_rmse:.4f}, Final CV: {final_cv_rmse:.4f}\")\n",
    "    print(\"This might indicate some instability / differences in folds.\")\n",
    "print(\"===============================\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5) CV-ensemble predictions on test_df (SAFE baseline)\n",
    "# ======================================================\n",
    "\n",
    "cv_test_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for model in cv_models:\n",
    "    cv_test_preds += model.predict(test_df)\n",
    "\n",
    "cv_test_preds /= len(cv_models)\n",
    "\n",
    "# This is your safest, fully CV-validated prediction vector\n",
    "sub_cv = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: cv_test_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_cv, run_name=\"felipe_catboost_optuna_cv_only\")\n",
    "print(\"Saved CV-only submission: felipe_catboost_optuna_cv_only\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6) Optional: full-data multi-seed ensemble\n",
    "# ======================================================\n",
    "\n",
    "avg_best_iter = int(np.mean(fold_best_iters))\n",
    "print(\"\\nAverage best_iteration across folds:\", avg_best_iter)\n",
    "\n",
    "n_estimators_full = avg_best_iter\n",
    "print(\"Using n_estimators for full-data models:\", n_estimators_full)\n",
    "\n",
    "seed_list = [101, 202, 303, 404, 505]\n",
    "full_seed_models = []\n",
    "full_seed_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for seed in seed_list:\n",
    "    print(f\"\\nTraining full-data model with seed={seed}...\")\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=n_estimators_full,\n",
    "        random_seed=seed,\n",
    "        thread_count=5,\n",
    "        verbose=False,\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",   # or \"0:1\" if he wants multi-GPU (rarely needed)\n",
    "        **best_params\n",
    "        # no early_stopping_rounds and no eval_set here\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    full_seed_models.append(model)\n",
    "    full_seed_preds += model.predict(test_df)\n",
    "\n",
    "full_seed_preds /= len(seed_list)\n",
    "\n",
    "sub_full = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: full_seed_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_full, run_name=\"felipe_catboost_optuna_full_seed_only\")\n",
    "print(\"Saved full-data seed ensemble submission: felipe_catboost_optuna_full_seed_only\")\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# ======================================================\n",
    "# 7) Blended submissions (CV + full-data)\n",
    "# ======================================================\n",
    "\n",
    "cv_only_oof    = np.zeros(len(X), dtype=float)\n",
    "full_style_oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n[OOF Eval] Fold {fold}\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # CV-only preds\n",
    "    cv_model = cv_models[fold - 1]\n",
    "    cv_preds_val = cv_model.predict(X_val)\n",
    "\n",
    "    # Full-seed-style preds (train on fold train only)\n",
    "    fold_full_preds = np.zeros(len(val_idx), dtype=float)\n",
    "    for seed in seed_list:\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=n_estimators_full,\n",
    "            random_seed=seed,\n",
    "            verbose=False,\n",
    "            thread_count=5,\n",
    "            task_type=\"GPU\",\n",
    "            devices=\"0\",   # or \"0:1\" if he wants multi-GPU (rarely needed)\n",
    "            **best_params\n",
    "        )\n",
    "        model.fit(X_train, y_train, cat_features=cat_features, verbose=False)\n",
    "        fold_full_preds += model.predict(X_val)\n",
    "\n",
    "    fold_full_preds /= len(seed_list)\n",
    "\n",
    "    cv_only_oof[val_idx]    = cv_preds_val\n",
    "    full_style_oof[val_idx] = fold_full_preds\n",
    "\n",
    "rmse_cv_only    = rmse(y, cv_only_oof)\n",
    "rmse_full_style = rmse(y, full_style_oof)\n",
    "\n",
    "print(\"\\n=== OOF RMSE (base strategies) ===\")\n",
    "print(f\"CV-only OOF RMSE:        {rmse_cv_only:.6f}\")\n",
    "print(f\"Full-seed-style OOF RMSE:{rmse_full_style:.6f}\")\n",
    "\n",
    "# Search best alpha\n",
    "alphas = np.linspace(0, 1, 101)\n",
    "best_alpha, best_rmse = None, 1e9\n",
    "\n",
    "for a in alphas:\n",
    "    blend_oof = a * cv_only_oof + (1 - a) * full_style_oof\n",
    "    r = rmse(y, blend_oof)\n",
    "    if r < best_rmse:\n",
    "        best_rmse, best_alpha = r, a\n",
    "\n",
    "print(f\"\\nBest alpha from OOF search: {best_alpha:.2f}\")\n",
    "print(f\"Best blended OOF RMSE:      {best_rmse:.6f}\")\n",
    "\n",
    "# Final blended test preds using best_alpha\n",
    "final_blend_test = best_alpha * cv_test_preds + (1 - best_alpha) * full_seed_preds\n",
    "\n",
    "sub_blend_best = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: final_blend_test\n",
    "})\n",
    "save_submission(sub_blend_best, run_name=f\"felipe_catboost_optuna_blend_best_{best_alpha:.2f}\")\n",
    "print(\"Saved best-alpha blended submission.\")\n",
    "\n",
    "print(\"\\n=== OOF RMSE comparison ===\")\n",
    "print(f\"CV-only:             {rmse_cv_only:.6f}\")\n",
    "print(f\"Full-seed-style:     {rmse_full_style:.6f}\")\n",
    "print(f\"Best blend (alpha={best_alpha:.2f}): {best_rmse:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
