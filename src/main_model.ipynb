{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce5db82",
   "metadata": {},
   "source": [
    "# Predicting Daily Milk Yield - Fall 2025 ML Course Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Felipe Benitez, feb478\n",
    "# Edwin Torres, EID\n",
    "# Gora Bepary, EID\n",
    "# Sankarsh Narayanan, EID\n",
    "\n",
    "'''\n",
    "Make cells to explain what dataset is\n",
    "Objective\n",
    "Tools used\n",
    "Summarizing final results, such like \"Best local CV RMSE... Best Kaggle RMSE... Kaggle LB Score..., etc.\" Things to orient reader.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430ff86",
   "metadata": {},
   "source": [
    "# Data Loading & Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Goal: show we know what we're working with before touching models.\n",
    "So show train/test. Other things we could show as example is train and test shape, train head, info, describe, # of rows/columns, target column, presence of categorical columns, missing values, just to name a few. Show whatever is important and useful. This will contribute to Data Exploration + Quality & Clarity.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac009a5e",
   "metadata": {},
   "source": [
    "# Data Cleaning Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3c347",
   "metadata": {},
   "source": [
    "## 3.1 Summary of what worked\n",
    "\n",
    "- **Removed invalid target values**  \n",
    "  - Filtered out rows where `Milk_Yield_L < 0` for both CatBoost and XGBoost pipelines.\n",
    "\n",
    "- **Standardized categorical text**  \n",
    "  - Trimmed whitespace from `Breed` and corrected inconsistent spelling (`Holstien → Holstein`).\n",
    "\n",
    "- **Imputed missing values**\n",
    "  - `Housing_Score`: replaced missing values using the column median.\n",
    "  - `Feed_Quantity_kg`: imputed per‐`Feed_Type` median to preserve context.\n",
    "  - All remaining numeric fields: filled missing entries using column medians.\n",
    "  - Median is safe to extremes in general.\n",
    "\n",
    "- **Temporal feature extraction from `Date`**  \n",
    "  - Parsed as datetime and derived features: `month`, `day`, `dayofweek`, `weekofyear`, `quarter`, and `is_weekend`; then removed `Date`.\n",
    "\n",
    "- **Categorical feature handling**\n",
    "  - CatBoost: preserved original categorical columns and passed them as native categorical features.\n",
    "  - XGBoost: applied one-hot encoding after other feature engineering steps.\n",
    "\n",
    "- **Farm identifier treatment**\n",
    "  - Removed `Farm_ID` in CatBoost pipeline.\n",
    "  - In XGBoost pipeline, applied fold-safe target encoding to convert `Farm_ID` into a numeric performance metric.\n",
    "\n",
    "- **Outlier columns and IDs**\n",
    "  - Dropped columns that uniquely identify samples (`Cattle_ID`) in both pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3af10b",
   "metadata": {},
   "source": [
    "## 3.2 Other things we tried but did not work. \n",
    "- **Handeling Negative Values**\n",
    "    - Taking an absolute value of those missing values made performance worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32770a7c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913258dc",
   "metadata": {},
   "source": [
    "# Target Distribution 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6959d",
   "metadata": {},
   "source": [
    "# Relationships with Key Features (or things we tried idk) 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd260ac",
   "metadata": {},
   "source": [
    "# Farm-Level Differences 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since we did farm clustering, show why and how, and also how it ended up being wrong in some way. Just talk about why we did this for example\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4840f",
   "metadata": {},
   "source": [
    "# Feature Engineering 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Here we tell the story of our features\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da875272",
   "metadata": {},
   "source": [
    "### 5.1 Overview\n",
    "\n",
    "We did not just throw the raw CSV into CatBoost or XGBoost. We iteratively engineered features, tested them with cross validation, and only kept ideas that were neutral or helpful for RMSE. Most experiments were guided by dairy domain logic and short, focused code changes, followed by logging the new average CV RMSE. If a change made the model worse or clearly added noise, we removed it from the final pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee4b58",
   "metadata": {},
   "source": [
    "### 5.2 Core CatBoost feature engineering\n",
    "\n",
    "For CatBoost we started from a clean baseline and then tried small, targeted feature blocks.\n",
    "\n",
    "Kept features and steps:\n",
    "\n",
    "- New biologically meaningful ratios:\n",
    "  - `Feed_per_kg_bw` (feed quantity divided by body weight).\n",
    "  - Temperature humidity index (THI) as a standard heat stress indicator.\n",
    "  - Grazing efficiency `Walk_per_graze` (distance walked over grazing hours).\n",
    "- Telling CatBoost exactly which columns are categorical so it can use its native categorical handling.\n",
    "- Dropping 74 rows with **negative** `Milk_Yield_L` labels because they are physically impossible and hurt training.\n",
    "- Lactation curve features such as `is_peak_lactation`, `is_early_lactation`, `is_late_lactation`, `dim_squared`, `dim_cubed`, `dim_log`, and `dim_parity`. These were mostly neutral but did not make things worse, so we kept them for biological interpretability.\n",
    "\n",
    "These changes moved CatBoost from about 4.114 RMSE down to about 4.108 on average, with dropping negative labels being the single biggest win.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e858824",
   "metadata": {},
   "source": [
    "### 5.3 CatBoost ideas that we tested but did not keep\n",
    "\n",
    "We also tried several larger “interaction blocks” that ended up hurting performance:\n",
    "\n",
    "- Additional efficiency ratios like `water_per_weight`, `age_parity_ratio`, `age_parity_product`, and combined activity metrics such as `total_activity` and `rest_activity_ratio`.\n",
    "- Extra interaction terms between temperature and humidity such as `temp_humidity` and heavy weight–feed products.\n",
    "- Bundling various vaccine columns and using sums or more complex combinations.\n",
    "\n",
    "These sets usually moved CatBoost’s average RMSE in the wrong direction, so we removed them from the final pipeline and kept only the simpler, more robust pieces. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0326d",
   "metadata": {},
   "source": [
    "### 5.4 Rumination and farm-level statistics\n",
    "\n",
    "We explored several ways to handle the strange `Rumination_Time_hrs` values and farm context:\n",
    "\n",
    "- Naively making all rumination values positive performed much worse.\n",
    "- A more careful approach that split positive and negative rumination into separate modes and created flags seemed conceptually better but did not beat the simpler baselines.\n",
    "- Setting negative rumination values to missing (NaN) and letting the model handle them was roughly neutral and is closer to how we would treat systemic sensor problems.\n",
    "- We added farm-level mean and standard deviation features per `Farm_ID` for important columns like `Weight_kg`, `Feed_Quantity_kg`, `Water_Intake_L`, `Age_Months`, `Days_in_Milk`, and `Ambient_Temperature_C`. These helped describe the overall environment of each farm without leaking label information.\n",
    "\n",
    "In the final model we kept a simpler rumination treatment and a lighter version of farm stats to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc2d5c",
   "metadata": {},
   "source": [
    "### 5.5 Peer-relative features and clustering\n",
    "\n",
    "We tried two main ideas for farm context beyond simple stats:\n",
    "\n",
    "1. Peer-relative features  \n",
    "   For each farm we computed how a cow compares to its farm mates for key predictors like `Weight_kg`, `Age_Months`, `Days_in_Milk`, `Previous_Week_Avg_Yield`, `Water_Intake_L`, `Body_Condition_Score`, and `Feed_Quantity_kg`. For each column we created:\n",
    "   - A difference feature, like `Weight_kg_vs_farm_diff`.\n",
    "   - A ratio feature, like `Weight_kg_vs_farm_ratio`.\n",
    "\n",
    "   These were designed to capture “is this cow above or below the typical animal on this farm” rather than only absolute levels.\n",
    "\n",
    "2. Farm clustering  \n",
    "   We also experimented with clustering farms using KMeans on farm-level aggregates and using the cluster label as `Farm_Cluster`. However, doing this separately on train and test created mismatched cluster labels and effectively injected anti-signal in evaluation. Once we understood this, we removed `Farm_Cluster` and avoided that form of clustering in the final pipeline.\n",
    "\n",
    "Overall, peer-relative features were conceptually strong but did not clearly beat our simpler combination of farm stats and core predictors, so we relied more heavily on the latter in the final CatBoost version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630eb21d",
   "metadata": {},
   "source": [
    "### 5.6 XGBoost feature engineering\n",
    "\n",
    "We also built an XGBoost pipeline to complement CatBoost and used it as a second family of models. We tried a wide range of engineered features:\n",
    "\n",
    "- Age and parity transforms: `Age_Years`, `Age_Years2`, `Parity2`, and `Age_x_Parity`.\n",
    "- Nonlinear day in milk transforms such as `DIM_log`.\n",
    "- Efficiency ratios: `Feed_per_kgBW`, `Water_per_kgBW`, and `PrevYield_per_Feed`.\n",
    "- Heat stress index THI and health summaries like `Vax_Sum`.\n",
    "- Farm deltas and cohort features that compare a cow’s performance to its farm or to breed plus lactation stage averages.\n",
    "- A large “biological feature block” including lactation curve flags, behavioral health indicators, parity categories, stress indicators, water and body condition flags, and age parity deviation.\n",
    "\n",
    "Most of these had very small effects on average XGBoost RMSE. The best improvements came from relatively simple parity categories, certain farm normalized predictors, and a `Vax_Sum` plus farm delta combination that gave a small but consistent gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78f4d1",
   "metadata": {},
   "source": [
    "### 5.7 Encodings and dimensionality reduction\n",
    "\n",
    "Instead of classic PCA style dimensionality reduction we focused on encodings that compress categorical structure in a supervised way.\n",
    "\n",
    "We tried:\n",
    "\n",
    "- Farm normalized predictors where we subtract or standardize by farm means and standard deviations for variables like `Previous_Week_Avg_Yield`, `Feed_Quantity_kg`, `Water_Intake_L`, and `Weight_kg`. This effectively reduces useless absolute scale variation and focuses on deviations within each farm.\n",
    "- KFold target encoding for several high signal categorical variables, with out of fold means on the training set and full train means on the test set, to avoid leakage.\n",
    "- Frequency encoding for some remaining categoricals like `Breed`.\n",
    "\n",
    "Target encoding in particular did not help XGBoost in this setup and sometimes hurt RMSE, so we removed it from the final feature set. Farm normalized predictors and simple parity categories were the most reliable pieces we kept.\n",
    "\n",
    "We considered PCA style dimensionality reduction on standardized numeric features, but since our strongest models are tree based and already handle moderate dimensionality well, PCA did not provide a clear benefit and would have reduced interpretability. We chose targeted feature selection, encoding, and dropping noisy columns instead of global projection methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e09f85",
   "metadata": {},
   "source": [
    "### 5.8 Final feature set used\n",
    "\n",
    "In the final models we kept a compact but expressive set of engineered features:\n",
    "\n",
    "- Cleaned labels with negative milk yields removed.\n",
    "- Date features like year, month, day of week, week of year, quarter, `date_ordinal`, and sometimes a weekend flag.\n",
    "- Key dairy science features: feed per body weight, THI, grazing efficiency, and lactation curve terms.\n",
    "- Light but useful farm context features based on per farm means and standard deviations.\n",
    "- Select efficiency and interaction terms that consistently did not hurt cross validation.\n",
    "- Parity categories and a small number of health and management summaries.\n",
    "\n",
    "We intentionally dropped many of the more complex interaction blocks and heavy encodings because they either did not help or made performance worse. This left us with a feature space that is easier to explain and tuned to what the cross validation actually supported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f519a",
   "metadata": {},
   "source": [
    "## 6. Modeling Approach\n",
    "\n",
    "Our modeling work focused on gradient boosted tree methods. We compared CatBoost, LightGBM, and XGBoost using the same preprocessing pipeline and cross validation splits.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "- Established baseline CatBoost and LightGBM models, which showed CatBoost was consistently stronger on this dataset.\n",
    "- Removed LightGBM from the final ensemble once we saw that blending it with CatBoost hurt leaderboard performance.\n",
    "- Tuned CatBoost with Optuna over depth, learning rate, regularization, subsample, random strength, and bagging temperature, reaching a best CV RMSE around 4.1064 with depth 6 and a five fold ensemble.\n",
    "- Built multiple CatBoost models using the best hyperparameters and different random seeds, then blended:\n",
    "  - CV only ensemble.\n",
    "  - Full data seed ensemble.\n",
    "  - Weighted blends between CV and full style predictions.\n",
    "- Trained and tuned an XGBoost model mainly as a diverse second opinion. It never beat CatBoost alone but helped us better understand which engineered features were robust across model classes.\n",
    "\n",
    "Overall, the best performing pipeline was an optimized CatBoost ensemble with carefully chosen features and tuned hyperparameters, rather than very heavy feature blocks or wide model diversity at all costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639ba8f",
   "metadata": {},
   "source": [
    "# Baseline Model (Modeling Approach) 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143db51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we show the baseline model we used w/ default ish paramaters. Show it, train it, run it, no optuna, show fold RMSEs, Average CV RMSEs, and explain what it's doing for us and anything else we can add to make this section full\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab589c7",
   "metadata": {},
   "source": [
    "## 6. Modeling Approach & Experiments\n",
    "In this section we describe our full modeling process starting from our base model, from how we tuned and compared, the different experiments, and how we eventually arrived and stuck with our final CatBoost ensemble with blending. The goal was not just to get a good leaderboard score but also explore model decisions and what needed to take place for things to improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be3d5f",
   "metadata": {},
   "source": [
    "### 6.1 Hyperparameter Tuning with Optuna\n",
    "\n",
    "After we selected CatBoost as our main model and saw that additional feature engineering was giving only small gains compared to the numerous experiments we tried with features, we focused on **hyperparameter tuning** to squeeze out as much performance as possible from our single best strongest learner.\n",
    "\n",
    "We actually ran **two separate Optuna studies** for CatBoost:\n",
    "\n",
    "- **Run 1 – 40 trials (our best hyperparameters)**\n",
    "- **Run 2 – 80 trials with an expanded search space**\n",
    "\n",
    "Both runs used the same 5-fold CV split and the same preprocessing pipeline, so their RMSEs are directly comparable.\n",
    "\n",
    "\n",
    "#### 6.1.1 First Optuna Run (40 trials)\n",
    "\n",
    "In the first study, we tuned the “core” CatBoost hyperparameters:\n",
    "\n",
    "- `depth` (5–7)\n",
    "- `learning_rate` (0.02–0.04)\n",
    "- `l2_leaf_reg` (L2 regularization)\n",
    "- `subsample` (row subsampling)\n",
    "- `random_strength` (randomness in split selection)\n",
    "- `bagging_temperature` (controls how aggressive the sampling is)\n",
    "- `n_estimators` was capped at 3000, with early stopping on each fold\n",
    "\n",
    "For each trial, the objective function trained on 4 folds and validated on the 5th, and we minimized the **mean 5-fold RMSE**.\n",
    "\n",
    "- **Best CV RMSE (Run 1):** ≈ **4.10639**\n",
    "- **Best hyperparameters (Run 1):**  \n",
    "  - `depth = 6`  \n",
    "  - `learning_rate ≈ 0.0229`  \n",
    "  - `l2_leaf_reg ≈ 4.01`  \n",
    "  - `subsample ≈ 0.847`  \n",
    "  - `random_strength ≈ 0.73`  \n",
    "  - `bagging_temperature ≈ 0.46`\n",
    "\n",
    "When we retrained a 5-fold CV ensemble with these parameters, we got:\n",
    "\n",
    "- **Final 5-fold CV RMSE:** ≈ **4.1064**  \n",
    "- **Best iterations per fold:** around **1,000–1,150 trees**, with an average of ~**1,094** boosting rounds\n",
    "\n",
    "This first Optuna run gave us the **strongest configuration** we found and became the base for our ensembling and blending experiments.\n",
    "\n",
    "\n",
    "#### 6.1.2 Second Optuna Run (80 trials, expanded space)\n",
    "\n",
    "Later, we ran a **second Optuna study with 80 trials**, this time expanding the search space to include more tree-shape and regularization parameters:\n",
    "\n",
    "- New hyperparameters included:\n",
    "  - `border_count` (number of candidate split points)\n",
    "  - `min_data_in_leaf` (minimum samples per leaf)\n",
    "  - `bootstrap_type = \"Bayesian\"` with tuned `bagging_temperature`\n",
    "- We continued to tune:\n",
    "  - `learning_rate`\n",
    "  - `l2_leaf_reg`\n",
    "  - `random_strength`\n",
    "\n",
    "Again we minimized the mean 5-fold CV RMSE.\n",
    "\n",
    "- **Best CV RMSE (Run 2):** ≈ **4.10642**\n",
    "\n",
    "So the second run got basically the **same performance**, but **very slightly worse** than Run 1 (difference on the order of 0.00003 in RMSE, which is completely negligible and likely within CV noise).\n",
    "\n",
    "The best hyperparameters from Run 2 looked like:\n",
    "\n",
    "- `depth = 6` (fixed)\n",
    "- `border_count = 128`\n",
    "- `learning_rate ≈ 0.0150`\n",
    "- `l2_leaf_reg ≈ 1.94`\n",
    "- `random_strength ≈ 0.30`\n",
    "- `bagging_temperature ≈ 2.16`\n",
    "- `min_data_in_leaf = 44`\n",
    "- `bootstrap_type = \"Bayesian\"`\n",
    "- `grow_policy = \"SymmetricTree\"`\n",
    "\n",
    "When we retrained with these parameters:\n",
    "\n",
    "- **Final 5-fold CV RMSE:** again ≈ **4.1064**\n",
    "- **Best iterations per fold:** much **larger**, around **1,700–2,100 trees**, with an average of ~**1,883** boosting rounds\n",
    "\n",
    "\n",
    "#### 6.1.3 Interpreting the differences between the two runs\n",
    "\n",
    "Even though Run 2 searched a bigger space and ran for more trials, it did **not** improve RMSE beyond Run 1. The difference in parameters tells us why:\n",
    "\n",
    "- **Learning rate**\n",
    "  - Run 1: `learning_rate ≈ 0.0229`  \n",
    "  - Run 2: `learning_rate ≈ 0.0150` (smaller)  \n",
    "  → A smaller learning rate usually needs **more trees** (which we see from the best iterations) and can make training slower without guaranteeing a better optimum. In our case, the lower learning rate just led to **more boosting rounds** with essentially the same RMSE.\n",
    "\n",
    "- **L2 regularization (`l2_leaf_reg`)**\n",
    "  - Run 1: `≈ 4.01` (stronger regularization)  \n",
    "  - Run 2: `≈ 1.94` (weaker regularization)  \n",
    "  → Run 2 allowed individual leaves to fit slightly more aggressively, but we also increased bagging and min leaf size. These trade-offs roughly canceled out, leading again to almost identical performance.\n",
    "\n",
    "- **Bagging behavior**\n",
    "  - Run 1: `bagging_temperature ≈ 0.46` (milder stochasticity)  \n",
    "  - Run 2: `bagging_temperature ≈ 2.16` + `bootstrap_type = \"Bayesian\"`  \n",
    "  → Run 2 used **much more aggressive Bayesian-style bagging**, injecting more randomness into which data points each tree sees. This can help reduce overfitting, but because our dataset is large and our Run 1 model was already well-regularized, the extra randomness did not produce a clear RMSE gain.\n",
    "\n",
    "- **Tree shape and leaf constraints**\n",
    "  - Run 1: used CatBoost’s default `border_count` and leaf constraints  \n",
    "  - Run 2: explicitly tuned  \n",
    "    - `border_count = 128` (fewer split candidates than 254, slightly simpler trees)  \n",
    "    - `min_data_in_leaf = 44` (prevents tiny leaves, smooths predictions)  \n",
    "  → These changes **regularize** the tree structure: they avoid overly fine splits and tiny leaves. That can improve generalization if the model is overfitting, but in our case the Run 1 configuration was already near the bias-variance sweet spot, so the extra constraints did not translate into a meaningful RMSE improvement.\n",
    "\n",
    "Overall, the second Optuna run **validated** that we were already sitting in a very flat optimum: many slightly different hyperparameter combinations (with different regularization/bagging trade-offs) all land around RMSE ≈ 4.1064.\n",
    "\n",
    "Because **Run 1** achieved the **lowest CV RMSE** and used a slightly simpler set of hyperparameters, we treated it as our **primary “best” configuration** and used it as the base for our CatBoost ensembles and blending experiments. The second run mainly served as a robustness check and showed that even after 80 more trials and a richer search space, we could not significantly beat our original tuned model.\n",
    "\n",
    "#### 6.1.4 XGBoost Hyperparameter Tuning with Optuna (500 trials)\n",
    "\n",
    "To build a strong **second model** for ensembling, we also ran a large Optuna study for **XGBoost** with **500 trials**, using the same 5-fold CV and essentially the same preprocessing pipeline as CatBoost:\n",
    "\n",
    "- cleaned and engineered features (date features, farm clustering, vaccine sum, parity indicators),\n",
    "- applied **fold-safe target encoding** on `Farm_ID` and created farm-delta features (`Prev_vs_Farm`, `Prev_over_Farm`),\n",
    "- one-hot encoded categorical variables **within each fold** and aligned train/validation columns to avoid leakage.\n",
    "\n",
    "Each Optuna trial trained a 5-fold CV XGBoost regressor (with early stopping) and minimized the **mean 5-fold RMSE**.\n",
    "\n",
    "The search space covered both **tree structure** and **regularization**:\n",
    "\n",
    "- Tree growth and structure:\n",
    "  - `grow_policy ∈ {depthwise, lossguide}`\n",
    "  - `max_depth` (0–12, depending on `grow_policy`)\n",
    "  - `max_leaves` (16–256)\n",
    "  - `max_bin ∈ {128, 256, 512}`\n",
    "- Learning and sampling:\n",
    "  - `learning_rate ∈ [0.005, 0.1]` (log-scaled)\n",
    "  - `subsample ∈ [0.5, 0.95]`\n",
    "  - `colsample_bytree`, `colsample_bylevel`, `colsample_bynode ∈ [0.5, 0.95]`\n",
    "- Regularization:\n",
    "  - `gamma ∈ [1e-4, 10]` (log-scaled)\n",
    "  - `reg_alpha ∈ [1e-4, 50]` (log-scaled)\n",
    "  - `reg_lambda ∈ [1e-3, 50]` (log-scaled)\n",
    "\n",
    "The **best trial** out of 500 achieved a **mean CV RMSE ≈ 4.1151** with a relatively shallow but strongly regularized configuration:\n",
    "\n",
    "- `grow_policy = \"depthwise\"`\n",
    "- `max_depth = 4`\n",
    "- `max_bin = 128`\n",
    "- `learning_rate ≈ 0.00681`\n",
    "- `max_leaves ≈ 171`\n",
    "- `min_child_weight ≈ 2.88`\n",
    "- `subsample ≈ 0.529`\n",
    "- `colsample_bytree ≈ 0.528`\n",
    "- `colsample_bylevel ≈ 0.920`\n",
    "- `colsample_bynode ≈ 0.941`\n",
    "- `gamma ≈ 0.632`\n",
    "- `reg_alpha ≈ 49.96`\n",
    "- `reg_lambda ≈ 35.12`\n",
    "\n",
    "Using these tuned hyperparameters, we then retrained:\n",
    "\n",
    "- A **5-fold CV ensemble**, which achieved  \n",
    "  - **Final 5-fold CV RMSE:** ≈ **4.1151**  \n",
    "  - Fold RMSEs in the range **4.106–4.127**, with best iteration counts around **2,800–3,300** trees per fold.\n",
    "- A **full-data multi-seed ensemble**, where we:\n",
    "  - re-fit on all training data for ~the average best-iteration from CV,\n",
    "  - used multiple random seeds (e.g., 42, 100, 200, 300, 400),\n",
    "  - averaged their predictions to produce our final XGBoost test predictions.\n",
    "\n",
    "Even after this large 500-trial search, tuned XGBoost remained slightly weaker than our best CatBoost configuration (≈ **4.115** vs. ≈ **4.106** RMSE). However, because its error pattern was different, this XGBoost model was still **valuable as a complementary learner**, and we used its OOF and test predictions in our **stacking / blending experiments** described in Section 6.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8761f",
   "metadata": {},
   "source": [
    "### 6.2 Ensembling, Stacking, and Snapshot Strategy\n",
    "\n",
    "Once we had a strong single CatBoost model, we explored techniques to reduce variance and squeeze out additional performance:\n",
    "\n",
    "#### 6.2.1 Multi-Seed Ensembling\n",
    "\n",
    "Even with fixed hyperparameters, CatBoost’s training process is stochastic (e.g., random permutations of categorical features, bootstrap sampling). We trained the same CatBoost configuration with **different random seeds**, and averaged their predictions:\n",
    "\n",
    "- Trained the tuned CatBoost model on the full training data with several seeds (e.g., 5 seeds).  \n",
    "- Averaged the predictions from all seed models on the test set.\n",
    "\n",
    "This simple **multi-seed ensemble** slightly improved CV RMSE and also stabilized leaderboard performance, consistent with variance-reduction theory.\n",
    "\n",
    "\n",
    "#### 6.2.2 Stacking and Blending\n",
    "\n",
    "We also experimented with **stacking / blending** strategies:\n",
    "\n",
    "- **CatBoost + XGBoost blend**  \n",
    "  - Trained both a tuned CatBoost model and a tuned XGBoost model (500-trial Optuna study described in 6.1.4).  \n",
    "  - Created out-of-fold (OOF) predictions for each model using 5-fold CV.  \n",
    "  - Searched over a blending weight `alpha` in  \n",
    "    `y_blend = alpha * y_catboost + (1 - alpha) * y_xgboost`.  \n",
    "  - Selected the `alpha` that minimized OOF RMSE, then used that same weight to blend test predictions.\n",
    "\n",
    "- **Ridge stacking on top of OOF predictions**  \n",
    "  - Built a 2-feature meta-dataset where each row contained `(CatBoost_OOF, XGBoost_OOF)`.  \n",
    "  - Trained a Ridge regression model to learn the optimal linear combination.  \n",
    "  - Used the fitted Ridge model to combine test predictions.  \n",
    "\n",
    "These stacking experiments generally gave small gains on CV, but in some cases were less stable on the leaderboard than a pure CatBoost ensemble. This is likely because the meta-learner can overfit to the noise in the OOF predictions, especially when the base models are already highly correlated.\n",
    "\n",
    "#### 6.2.3 Snapshot-Style Ensembling (Parameter Variants)\n",
    "\n",
    "We then tried to extend this idea to a **snapshot ensemble** during our last moments before the submission deadline:\n",
    "\n",
    "- In addition to changing the random seed, we trained several “nearby” versions of the model by slightly varying:\n",
    "  - `depth` (e.g., 5, 6, 7)\n",
    "  - `learning_rate` (e.g., ±10% around the tuned value)\n",
    "- For each parameter variant, we trained models with multiple seeds and averaged all of them.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- **Shallower trees** (e.g., depth 5) capture smoother, more global patterns.\n",
    "- **Deeper trees** (e.g., depth 7) can model more complex feature interactions but risk overfitting.\n",
    "- Slight learning-rate shifts change the effective regularization.\n",
    "\n",
    "By averaging these diverse models, we hoped to obtain a more robust predictor that was less sensitive to any specific hyperparameter setting or random seed. The model ended up taking up 4+ hours to train after we started it at 10pm before the 11:55pm deadline, so we ended up stopping it after we saw it likely wasn't finishing anytime soon, but believe it would have done best since it was squeezing out our already best model even more.\n",
    "\n",
    "In the final model, we relied primarily on **CatBoost-only CV + Multi-Seed ensembling**, with blending used carefully and only when it showed consistent improvement across folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96473ecf",
   "metadata": {},
   "source": [
    "### 6.3 Cross-Validation Experiments and Robustness\n",
    "\n",
    "A major focus of our modeling approach was making sure our CV estimates were **reliable** and not overly optimistic.\n",
    "\n",
    "We experimented with multiple validation strategies:\n",
    "\n",
    "- **Standard KFold with shuffling**  \n",
    "  - Our default setup: 5-fold KFold with `shuffle=True` and a fixed `random_state`.  \n",
    "  - Simple and effective for quickly comparing model variants.\n",
    "\n",
    "- **GroupKFold by `Farm_ID` (sanity check)**  \n",
    "  - To test whether the model was accidentally memorizing per-farm patterns in a way that would not transfer to unseen farms, we ran experiments where entire farms were held out in validation.  \n",
    "  - Result: RMSE under GroupKFold was very similar to our standard KFold RMSE, suggesting that the model generalizes reasonably well across farms.\n",
    "\n",
    "- **Time-related considerations**  \n",
    "  - We created date-based features (e.g., month, week of year, day of week) and considered that the hidden test set might correspond to later calendar periods.  \n",
    "  - We monitored whether features like `year` or `date_ordinal` appeared to cause over-optimistic CV. In parallel, we compared CV scores with and without these trend-like features as a robustness check.\n",
    "\n",
    "Overall, these CV experiments increased our confidence that the improvements we saw during development were not purely due to leakage or overfitting to a specific fold configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243ba99",
   "metadata": {},
   "source": [
    "### 6.4 Final Model and Reflection\n",
    "\n",
    "Our final submission is based on a **CatBoost-only ensemble**, with:\n",
    "\n",
    "- Carefully tuned hyperparameters obtained via Optuna.\n",
    "- Multiple random seeds to create a diverse ensemble.\n",
    "- An alpha-blending step to optimally combine:\n",
    "  - The CV-trained CatBoost model, and  \n",
    "  - The multi-seed ensemble.\n",
    "\n",
    "On our internal 5-fold CV, this final configuration achieved a best RMSE of approximately **4.106145**, and translated into 4th place out of 52 total teams on the course Kaggle leaderboard.\n",
    "\n",
    "**What helped the most:**\n",
    "\n",
    "- Moving from simple baselines to tuned CatBoost gave the largest improvement.\n",
    "- Thoughtful feature engineering (especially around date, feed, and farm-level behavior) provided strong, interpretable signals.\n",
    "- Multi-seed and snapshot ensembling added a smaller but reliable boost and made predictions more stable.\n",
    "\n",
    "**What helped less or was risky:**\n",
    "\n",
    "- Very aggressive stacking and meta-models sometimes improved CV by a tiny margin but did not always translate into better leaderboard performance.\n",
    "- Certain features and encodings (e.g., farm clustering variations, very trend-heavy date features) required careful validation and could lead to overly optimistic CV if not tested properly.\n",
    "\n",
    "Overall, our modeling process was iterative and experimental: we started from simple models, gradually introduced more sophisticated techniques, and continuously used cross-validation and leaderboard feedback to decide which ideas were genuinely helpful and which were mainly adding complexity. This approach allowed us to build a final model that is both **accurate** and **robust**, while also learning a lot about practical model development in the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
