{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce5db82",
   "metadata": {},
   "source": [
    "# Predicting Daily Milk Yield - Fall 2025 ML Course Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Felipe Benitez, feb478\n",
    "# Edwin Torres, ert863\n",
    "# Gora Bepary, gcb883\n",
    "# Sankarsh Narayanan, EID\n",
    "\n",
    "'''\n",
    "Make cells to explain what dataset is\n",
    "Objective\n",
    "Tools used\n",
    "Summarizing final results, such like \"Best local CV RMSE... Best Kaggle RMSE... Kaggle LB Score..., etc.\" Things to orient reader.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430ff86",
   "metadata": {},
   "source": [
    "# Data Loading & Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Goal: show we know what we're working with before touching models.\n",
    "So show train/test. Other things we could show as example is train and test shape, train head, info, describe, # of rows/columns, target column, presence of categorical columns, missing values, just to name a few. Show whatever is important and useful. This will contribute to Data Exploration + Quality & Clarity.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac009a5e",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3c347",
   "metadata": {},
   "source": [
    "## 2.1 Summary of what worked\n",
    "\n",
    "- **Removed invalid target values**  \n",
    "  - Filtered out rows where `Milk_Yield_L < 0` for both CatBoost and XGBoost pipelines.\n",
    "\n",
    "- **Standardized categorical text**  \n",
    "  - Trimmed whitespace from `Breed` and corrected inconsistent spelling (`Holstien → Holstein`).\n",
    "\n",
    "- **Imputed missing values**\n",
    "  - `Housing_Score`: replaced missing values using the column median.\n",
    "  - `Feed_Quantity_kg`: imputed per‐`Feed_Type` median to preserve context.\n",
    "  - All remaining numeric fields: filled missing entries using column medians.\n",
    "  - Median is safe to extremes in general.\n",
    "\n",
    "- **Temporal feature extraction from `Date`**  \n",
    "  - Parsed as datetime and derived features: `month`, `day`, `dayofweek`, `weekofyear`, `quarter`, and `is_weekend`; then removed `Date`.\n",
    "\n",
    "- **Categorical feature handling**\n",
    "  - CatBoost: preserved original categorical columns and passed them as native categorical features.\n",
    "  - XGBoost: applied one-hot encoding after other feature engineering steps.\n",
    "\n",
    "- **Farm identifier treatment**\n",
    "  - Removed `Farm_ID` in CatBoost pipeline.\n",
    "  - In XGBoost pipeline, applied fold-safe target encoding to convert `Farm_ID` into a numeric performance metric.\n",
    "\n",
    "- **Outlier columns and IDs**\n",
    "  - Dropped columns that uniquely identify samples (`Cattle_ID`) in both pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3af10b",
   "metadata": {},
   "source": [
    "## 2.2 Summary of what did not work. \n",
    "- **Handeling Negative Values**\n",
    "    - Taking an absolute value of those missing values made performance worse.\n",
    "    - Since 55% of entries were negative, we treated this as a systemic issue rather than random noise. Positive values (mean 5.74, max 31.2) and negative values (mean −4.22, min −8.8) appeared to represent different behaviors, so taking the absolute value mixed distinct patterns. We attempted to separate them into different features, but this split did not improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32770a7c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913258dc",
   "metadata": {},
   "source": [
    "# Target Distribution 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6959d",
   "metadata": {},
   "source": [
    "# Relationships with Key Features (or things we tried idk) 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd260ac",
   "metadata": {},
   "source": [
    "# Farm-Level Differences 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since we did farm clustering, show why and how, and also how it ended up being wrong in some way. Just talk about why we did this for example\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4840f",
   "metadata": {},
   "source": [
    "# Feature Engineering 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da875272",
   "metadata": {},
   "source": [
    "### 5.1 Overview\n",
    "\n",
    "We didn’t just feed the raw CSVs directly into our models. We iteratively engineered features, tested them with cross validation, and only kept transformations that were neutral or helpful for RMSE. Most experiments were guided by dairy domain reasoning, short and focused code changes, and immediate validation through average CV RMSE. If a feature block added noise or made the models worse, we removed it from the final pipeline.\n",
    "\n",
    "A major early issue involved the **date column**. The raw file stored the date as an **object** rather than a true `datetime`. Because our preprocessing step converted all object columns into categoricals, the date was being treated as thousands of unrelated categories instead of a temporal variable. This caused instability and strange model behavior. Converting the date into a proper `datetime` type and extracting structured components — such as **year**, **month**, **day**, and **day of week** — immediately fixed this instability and made the models behave far more consistently. This correction became an important foundation for every later feature block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee4b58",
   "metadata": {},
   "source": [
    "### 5.2 Core CatBoost Feature Engineering\n",
    "\n",
    "For CatBoost, we built features incrementally, keeping only the pieces that consistently helped or at least did not hurt CV RMSE.\n",
    "\n",
    "**Kept features and steps included:**\n",
    "- Biologically meaningful ratios:\n",
    "  - `Feed_per_kg_bw`\n",
    "  - Temperature-Humidity Index (THI) for heat stress  \n",
    "  - `Walk_per_graze` (distance walked per grazing hour)\n",
    "- Explicitly labeling categorical columns so CatBoost uses its native encoding properly.\n",
    "- Dropping 74 rows with **negative** `Milk_Yield_L` labels, which are physically impossible and were hurting training.\n",
    "- Lactation curve features such as:\n",
    "  - `is_peak_lactation`, `is_early_lactation`, `is_late_lactation`\n",
    "  - `dim_squared`, `dim_cubed`, `dim_log`\n",
    "  - `dim_parity`\n",
    "\n",
    "These changes moved CatBoost from about **4.114 RMSE → ~4.108**, with dropping negative labels being the biggest single improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e858824",
   "metadata": {},
   "source": [
    "### 5.3 CatBoost Ideas We Tested But Did Not Keep\n",
    "\n",
    "We experimented with several larger interaction blocks that ended up hurting RMSE:\n",
    "\n",
    "- Additional efficiency ratios (`water_per_weight`, `age_parity_ratio`, `age_parity_product`, etc.)\n",
    "- Activity combinations (`total_activity`, `rest_activity_ratio`)\n",
    "- More complicated temperature-humidity interactions\n",
    "- Bundled vaccine indicators or sums\n",
    "\n",
    "Most of these made RMSE worse or added noise, so we removed them and kept only the simpler, more stable features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0326d",
   "metadata": {},
   "source": [
    "### 5.4 Rumination and Farm-Level Statistics\n",
    "\n",
    "We explored multiple strategies to handle the unusual rumination values and farm-level context:\n",
    "\n",
    "- Making all rumination values positive made the model worse.\n",
    "- Splitting rumination into “positive mode” and “negative mode” plus flags was more logical but still not better in practice.\n",
    "- Treating negative rumination values as **missing (NaN)** was neutral and aligned with the idea of faulty sensors.\n",
    "- Farm-level stats — farm mean and standard deviation for predictors like:\n",
    "  - `Weight_kg`, `Feed_Quantity_kg`, `Water_Intake_L`, `Age_Months`\n",
    "  - `Days_in_Milk`, `Ambient_Temperature_C`\n",
    "\n",
    "These features describe farm environment without leaking targets.\n",
    "\n",
    "In the final pipeline, we used a simpler rumination approach and a **lighter** set of farm statistics to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc2d5c",
   "metadata": {},
   "source": [
    "### 5.5 Peer-Relative Features and Clustering\n",
    "\n",
    "We explored two types of contextual features:\n",
    "\n",
    "#### 1. Peer-relative features\n",
    "For each farm, we compared each cow to its farm peers via:\n",
    "- **Difference** features (e.g., `Weight_kg_vs_farm_diff`)\n",
    "- **Ratio** features (e.g., `Weight_kg_vs_farm_ratio`)\n",
    "\n",
    "These were meant to capture whether a cow is above or below typical farm-level baselines.\n",
    "\n",
    "#### 2. Farm clustering\n",
    "We tried clustering farms with KMeans and assigning each farm a cluster label.\n",
    "However, since train and test were clustered separately, the IDs did not align, which created anti-signal.  \n",
    "Once we realized this, we removed `Farm_Cluster`.\n",
    "\n",
    "Peer-relative features were conceptually strong but did not outperform simpler normalized farm stats, so we relied mainly on the latter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630eb21d",
   "metadata": {},
   "source": [
    "### 5.6 XGBoost Feature Engineering\n",
    "\n",
    "We also maintained an XGBoost pipeline to complement CatBoost and use as a second model family.  \n",
    "XGBoost was very sensitive to early feature blocks, especially before we correctly processed the **date column**, which had been treated as a high-cardinality categorical. Once we converted the date to `datetime` and stabilized the feature set, **we returned to XGBoost**, and it became much more consistent.\n",
    "\n",
    "We engineered and tested:\n",
    "\n",
    "- Age and parity transforms (`Age_Years`, `Age_Years2`, `Parity2`, `Age_x_Parity`)\n",
    "- Nonlinear `Days_in_Milk` transforms (`DIM_log`)\n",
    "- Efficiency ratios (`Feed_per_kgBW`, `Water_per_kgBW`, `PrevYield_per_Feed`)\n",
    "- THI and simple vaccine summaries (`Vax_Sum`)\n",
    "- Farm-delta and cohort-relative features\n",
    "- A large biological block of lactation and health indicators\n",
    "\n",
    "The most consistently helpful additions were:\n",
    "- Simple **parity categories**\n",
    "- **Farm-normalized predictors**\n",
    "- A **Vax_Sum + farm-delta** combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78f4d1",
   "metadata": {},
   "source": [
    "### 5.7 Encodings and Dimensionality Reduction\n",
    "\n",
    "Instead of PCA, we used targeted supervised encodings and structured normalization:\n",
    "\n",
    "**Tried:**\n",
    "- Farm-normalized predictors (subtracting or standardizing by farm averages)\n",
    "- K-Fold target encoding with out-of-fold means\n",
    "- Frequency encoding for large categoricals such as `Breed`\n",
    "\n",
    "Target encoding usually hurt XGBoost and didn’t help CatBoost, so we removed it.  \n",
    "Farm-normalized predictors and simple parity categories were the most reliable encodings.\n",
    "\n",
    "We considered PCA, but tree models handle moderate dimensionality well and PCA reduces interpretability.  \n",
    "Targeted feature selection and selective dropping worked better for our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e09f85",
   "metadata": {},
   "source": [
    "### 5.8 Summary\n",
    "\n",
    "Across all iterations, the most important feature engineering wins were:\n",
    "\n",
    "- **Fixing the date column** by converting it from object → datetime  \n",
    "- **Dropping physically impossible negative labels**\n",
    "- **Meaningful biological ratios and farm-normalized predictors**\n",
    "- **Lightweight farm statistics**\n",
    "- **Avoiding oversized interaction blocks**\n",
    "- **Revisiting XGBoost only after stabilizing the feature space**\n",
    "\n",
    "These steps led to a stable, interpretable, and high-performing feature set that consistently improved CatBoost and allowed XGBoost to be evaluated fairly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f519a",
   "metadata": {},
   "source": [
    "### 5.9 Model Feedback During Feature Engineering\n",
    "\n",
    "Our modeling work during feature engineering focused on using CatBoost, LightGBM, and XGBoost as feedback tools rather than finalized models. We applied the same preprocessing pipeline and cross validation splits to each model to understand how different engineered features affected stability, signal strength, and overall RMSE.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- CatBoost consistently responded the best to our engineered features, making it a reliable indicator of whether a new feature block was helpful or harmful.\n",
    "- LightGBM performed reasonably but tended to lose performance once the feature space became more complex.\n",
    "- XGBoost struggled early on—especially before fixing the date column—but improved significantly after the feature pipeline was stabilized and we revisited it near the end.\n",
    "\n",
    "We also monitored feature interactions through multiple CatBoost runs, tracking how RMSE changed as new biological ratios, farm-normalized predictors, and date-derived components were added. This allowed us to keep only the transformations that consistently improved model behavior.\n",
    "\n",
    "Overall, this model feedback loop was essential during feature engineering. It helped confirm which features were robust across different frameworks and guided the final feature set before moving on to full modeling and tuned ensembles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639ba8f",
   "metadata": {},
   "source": [
    "# Baseline Model (Modeling Approach) 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143db51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we show the baseline model we used w/ default ish paramaters. Show it, train it, run it, no optuna, show fold RMSEs, Average CV RMSEs, and explain what it's doing for us and anything else we can add to make this section full\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab589c7",
   "metadata": {},
   "source": [
    "## 6. Modeling Approach & Experiments\n",
    "In this section we describe our full modeling process starting from our base model, from how we tuned and compared, the different experiments, and how we eventually arrived and stuck with our final CatBoost ensemble with blending. The goal was not just to get a good leaderboard score but also explore model decisions and what needed to take place for things to improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be3d5f",
   "metadata": {},
   "source": [
    "### 6.1 Hyperparameter Tuning with Optuna\n",
    "\n",
    "After we selected CatBoost as our main model and saw that additional feature engineering was giving only small gains compared to the numerous experiments we tried with features, we focused on **hyperparameter tuning** to squeeze out as much performance as possible from our single best strongest learner.\n",
    "\n",
    "We actually ran **two separate Optuna studies** for CatBoost:\n",
    "\n",
    "- **Run 1 – 40 trials (our best hyperparameters)**\n",
    "- **Run 2 – 80 trials with an expanded search space**\n",
    "\n",
    "Both runs used the same 5-fold CV split and the same preprocessing pipeline, so their RMSEs are directly comparable.\n",
    "\n",
    "\n",
    "#### 6.1.1 First Optuna Run (40 trials)\n",
    "\n",
    "In the first study, we tuned the “core” CatBoost hyperparameters:\n",
    "\n",
    "- `depth` (5–7)\n",
    "- `learning_rate` (0.02–0.04)\n",
    "- `l2_leaf_reg` (L2 regularization)\n",
    "- `subsample` (row subsampling)\n",
    "- `random_strength` (randomness in split selection)\n",
    "- `bagging_temperature` (controls how aggressive the sampling is)\n",
    "- `n_estimators` was capped at 3000, with early stopping on each fold\n",
    "\n",
    "For each trial, the objective function trained on 4 folds and validated on the 5th, and we minimized the **mean 5-fold RMSE**.\n",
    "\n",
    "- **Best CV RMSE (Run 1):** ≈ **4.10639**\n",
    "- **Best hyperparameters (Run 1):**  \n",
    "  - `depth = 6`  \n",
    "  - `learning_rate ≈ 0.0229`  \n",
    "  - `l2_leaf_reg ≈ 4.01`  \n",
    "  - `subsample ≈ 0.847`  \n",
    "  - `random_strength ≈ 0.73`  \n",
    "  - `bagging_temperature ≈ 0.46`\n",
    "\n",
    "When we retrained a 5-fold CV ensemble with these parameters, we got:\n",
    "\n",
    "- **Final 5-fold CV RMSE:** ≈ **4.1064**  \n",
    "- **Best iterations per fold:** around **1,000–1,150 trees**, with an average of ~**1,094** boosting rounds\n",
    "\n",
    "This first Optuna run gave us the **strongest configuration** we found and became the base for our ensembling and blending experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702dfc88",
   "metadata": {},
   "source": [
    "### Code for first Optuna run. Not the full code we used in the cell to prevent bloat. Full run and output can be found in felipe_model.ipynb on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c35a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing just Optuna search and final model training with best params, not the initial baseline model as it remains unchanged from previous version\n",
    "\n",
    "# Optuna hyperparameter search (5-fold CV)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 5, 7),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.04),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 7.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.9),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.5, 5.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "    }\n",
    "\n",
    "    fold_rmses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=3000, # big cap, rely on early stopping\n",
    "            early_stopping_rounds=100,\n",
    "            random_seed=42,\n",
    "            thread_count=4,\n",
    "            verbose=False, # keep Optuna runs quiet\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=cat_features, # using column names\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    mean_rmse = float(np.mean(fold_rmses))\n",
    "    return mean_rmse\n",
    "\n",
    "# Optuna study\n",
    "N_TRIALS = 40  \n",
    "\n",
    "print(\"\\nStarting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=N_TRIALS, n_jobs=2, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_cv_rmse = study.best_value\n",
    "\n",
    "print(\"\\nOptuna search complete.\")\n",
    "print(\"Best CV RMSE from Optuna:\", best_cv_rmse)\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Final 5-fold CV ensemble with best_params\n",
    "cv_models = []\n",
    "fold_rmses = []\n",
    "fold_best_iters = []\n",
    "\n",
    "print(\"\\nTraining final 5-fold ensemble with best_params...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n----- Final CV Fold {fold} -----\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # use SAME seed as Optuna for consistency\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        **best_params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    best_iter = model.get_best_iteration()\n",
    "\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}, best_iter={best_iter}\")\n",
    "\n",
    "    fold_rmses.append(rmse)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    cv_models.append(model)\n",
    "\n",
    "final_cv_rmse = float(np.mean(fold_rmses))\n",
    "print(\"\\n===============================\")\n",
    "print(f\"Final 5-fold ensemble CV RMSE: {final_cv_rmse:.4f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "\n",
    "# sanity check vs Optuna's own CV\n",
    "if final_cv_rmse > best_cv_rmse + 0.001:\n",
    "    print(\"WARNING: Final CV RMSE is worse than Optuna's best!\")\n",
    "    print(f\"Optuna best: {best_cv_rmse:.4f}, Final CV: {final_cv_rmse:.4f}\")\n",
    "    print(\"This might indicate some instability / differences in folds.\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# CV-ensemble predictions on test_df\n",
    "cv_test_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for model in cv_models:\n",
    "    cv_test_preds += model.predict(test_df)\n",
    "\n",
    "cv_test_preds /= len(cv_models)\n",
    "sub_cv = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: cv_test_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_cv, run_name=\"felipe_catboost_optuna_cv_only\")\n",
    "print(\"Saved CV-only submission: felipe_catboost_optuna_cv_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6cc7e6",
   "metadata": {},
   "source": [
    "### Output (truncated to not show all Optuna trials). Full output can be found in felipe_model.ipynb\n",
    "\n",
    "Optuna search complete.\n",
    "\n",
    "Best CV RMSE from Optuna: 4.106391174299612\n",
    "\n",
    "Best params: {'depth': 6, 'learning_rate': 0.022872270426979868, 'l2_leaf_reg': 4.014881862534349, 'subsample': 0.847236734294946, 'random_strength': 0.731107887920747, 'bagging_temperature': 0.4635035306948895}\n",
    "\n",
    "\n",
    "Training final 5-fold ensemble with best_params...\n",
    "\n",
    "----- Final CV Fold 1 -----\n",
    "Fold 1 RMSE: 4.1070, best_iter=1157\n",
    "\n",
    "----- Final CV Fold 2 -----\n",
    "Fold 2 RMSE: 4.1002, best_iter=1127\n",
    "\n",
    "----- Final CV Fold 3 -----\n",
    "Fold 3 RMSE: 4.1201, best_iter=1093\n",
    "\n",
    "----- Final CV Fold 4 -----\n",
    "Fold 4 RMSE: 4.1063, best_iter=1094\n",
    "\n",
    "----- Final CV Fold 5 -----\n",
    "Fold 5 RMSE: 4.0984, best_iter=999\n",
    "\n",
    "Final 5-fold ensemble CV RMSE: 4.1064\n",
    "\n",
    "Fold RMSEs: [np.float64(4.106956550228758), np.float64(4.100151472353626), np.float64(4.120099489769896), np.float64(4.106327628323443), np.float64(4.0984207308223395)]\n",
    "\n",
    "Best iters: [1157, 1127, 1093, 1094, 999]\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_optuna_cv_only__2025-11-17__20-14-39.csv\n",
    "\n",
    "Saved CV-only submission: felipe_catboost_optuna_cv_only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe82392",
   "metadata": {},
   "source": [
    "#### 6.1.2 Second Optuna Run (80 trials, expanded space)\n",
    "\n",
    "Later, we ran a **second Optuna study with 80 trials**, this time expanding the search space to include more tree-shape and regularization parameters:\n",
    "\n",
    "- New hyperparameters included:\n",
    "  - `border_count` (number of candidate split points)\n",
    "  - `min_data_in_leaf` (minimum samples per leaf)\n",
    "  - `bootstrap_type = \"Bayesian\"` with tuned `bagging_temperature`\n",
    "- We continued to tune:\n",
    "  - `learning_rate`\n",
    "  - `l2_leaf_reg`\n",
    "  - `random_strength`\n",
    "\n",
    "Again we minimized the mean 5-fold CV RMSE.\n",
    "\n",
    "- **Best CV RMSE (Run 2):** ≈ **4.10642**\n",
    "\n",
    "So the second run got basically the **same performance**, but **very slightly worse** than Run 1 (difference on the order of 0.00003 in RMSE, which is completely negligible and likely within CV noise).\n",
    "\n",
    "The best hyperparameters from Run 2 looked like:\n",
    "\n",
    "- `depth = 6` (fixed)\n",
    "- `border_count = 128`\n",
    "- `learning_rate ≈ 0.0150`\n",
    "- `l2_leaf_reg ≈ 1.94`\n",
    "- `random_strength ≈ 0.30`\n",
    "- `bagging_temperature ≈ 2.16`\n",
    "- `min_data_in_leaf = 44`\n",
    "- `bootstrap_type = \"Bayesian\"`\n",
    "- `grow_policy = \"SymmetricTree\"`\n",
    "\n",
    "When we retrained with these parameters:\n",
    "\n",
    "- **Final 5-fold CV RMSE:** again ≈ **4.1064**\n",
    "- **Best iterations per fold:** much **larger**, around **1,700–2,100 trees**, with an average of ~**1,883** boosting rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c715842",
   "metadata": {},
   "source": [
    "### Showing just optuna search and final model training with best params, not the initial baseline model as it remains unchanged from previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76de30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "    \"depth\": 6,\n",
    "    \"bootstrap_type\": \"Bayesian\",\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "}\n",
    "\n",
    "# Optuna hyperparameter search (5-fold CV)\n",
    "def objective(trial):\n",
    "    tuned_params = {\n",
    "        # QUANTIZATION\n",
    "        # 128 is a sweet spot for noisy data (vs default 254)\n",
    "        \"border_count\": trial.suggest_categorical(\"border_count\", [128, 254]),\n",
    "        # REGULARIZATION\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.015, 0.06),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.1, 8.0),\n",
    "        # BAYESIAN BAGGING (Replaces subsample)\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 3.0),\n",
    "        # 5. COLUMN SAMPLING\n",
    "        \"rsm\": trial.suggest_float(\"rsm\", 0.7, 1.0),  # colsample_bylevel\n",
    "        # OVERFITTING CONTROL\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 80),\n",
    "    }\n",
    "\n",
    "    params = {**fixed_params, **tuned_params}\n",
    "\n",
    "    fold_rmses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=3000, # big cap, rely on early stopping\n",
    "            early_stopping_rounds=100,\n",
    "            random_seed=42,\n",
    "            use_best_model=True,\n",
    "            thread_count=4,\n",
    "            verbose=False, # keep Optuna runs quiet\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=cat_features, # using column names\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    mean_rmse = float(np.mean(fold_rmses))\n",
    "    return mean_rmse\n",
    "\n",
    "N_TRIALS = 80  \n",
    "\n",
    "print(\"\\nStarting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=N_TRIALS, n_jobs=2, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_cv_rmse = study.best_value\n",
    "\n",
    "final_params = {**fixed_params, **best_params}\n",
    "\n",
    "print(\"\\nOptuna search complete.\")\n",
    "print(\"Best CV RMSE from Optuna:\", best_cv_rmse)\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Final params used for training:\", final_params)\n",
    "\n",
    "import json\n",
    "with open(\"catboost_best_params.json\", \"w\") as f:\n",
    "    json.dump(final_params, f, indent=2)\n",
    "print(\"Saved best params to catboost_best_params.json\")\n",
    "\n",
    "\n",
    "# Final 5-fold CV ensemble with best_params\n",
    "cv_models = []\n",
    "fold_rmses = []\n",
    "fold_best_iters = []\n",
    "\n",
    "print(\"\\nTraining final 5-fold ensemble with best_params...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n----- Final CV Fold {fold} -----\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # use SAME seed as Optuna for consistency\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        use_best_model=True,\n",
    "        random_seed=42,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        **final_params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    best_iter = model.get_best_iteration()\n",
    "\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}, best_iter={best_iter}\")\n",
    "\n",
    "    fold_rmses.append(rmse)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    cv_models.append(model)\n",
    "\n",
    "final_cv_rmse = float(np.mean(fold_rmses))\n",
    "print(\"\\n===============================\")\n",
    "print(f\"Final 5-fold ensemble CV RMSE: {final_cv_rmse:.4f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "\n",
    "# sanity check vs Optuna's own CV\n",
    "if final_cv_rmse > best_cv_rmse + 0.001:\n",
    "    print(\"WARNING: Final CV RMSE is worse than Optuna's best!\")\n",
    "    print(f\"Optuna best: {best_cv_rmse:.4f}, Final CV: {final_cv_rmse:.4f}\")\n",
    "    print(\"This might indicate some instability / differences in folds.\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# CV-ensemble predictions on test_df (SAFE baseline)\n",
    "\n",
    "cv_test_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for model in cv_models:\n",
    "    cv_test_preds += model.predict(test_df)\n",
    "\n",
    "cv_test_preds /= len(cv_models)\n",
    "sub_cv = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: cv_test_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_cv, run_name=\"felipe_catboost_optuna_cv_only\")\n",
    "print(\"Saved CV-only submission: felipe_catboost_optuna_cv_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb6444",
   "metadata": {},
   "source": [
    "### Output (truncated to not show all Optuna trials). Full output can be found in felipe_model.ipynb\n",
    "\n",
    "Optuna search complete.\n",
    "\n",
    "Best CV RMSE from Optuna: 4.106420555491676\n",
    "\n",
    "Best params: {'border_count': 128, 'learning_rate': 0.015001219012030928, 'l2_leaf_reg': 1.9410139722523063, 'random_strength': 0.2958486733531681, 'bagging_temperature': 2.161101900330739, 'min_data_in_leaf': 44}\n",
    "Final params used for training: {'depth': 6, 'bootstrap_type': 'Bayesian', 'grow_policy': 'SymmetricTree', 'border_count': 128, 'learning_rate': 0.015001219012030928, 'l2_leaf_reg': 1.9410139722523063, 'random_strength': 0.2958486733531681, 'bagging_temperature': 2.161101900330739, 'min_data_in_leaf': 44}\n",
    "\n",
    "Saved best params to catboost_best_params.json\n",
    "\n",
    "Training final 5-fold ensemble with best_params...\n",
    "\n",
    "----- Final CV Fold 1 -----\n",
    "Fold 1 RMSE: 4.1075, best_iter=1913\n",
    "\n",
    "----- Final CV Fold 2 -----\n",
    "Fold 2 RMSE: 4.0993, best_iter=2164\n",
    "\n",
    "----- Final CV Fold 3 -----\n",
    "Fold 3 RMSE: 4.1205, best_iter=1676\n",
    "\n",
    "----- Final CV Fold 4 -----\n",
    "Fold 4 RMSE: 4.1063, best_iter=1735\n",
    "\n",
    "----- Final CV Fold 5 -----\n",
    "Fold 5 RMSE: 4.0985, best_iter=1931\n",
    "\n",
    "\n",
    "Final 5-fold ensemble CV RMSE: 4.1064\n",
    "\n",
    "Fold RMSEs: [np.float64(4.107491747188065), np.float64(4.09931832448274), np.float64(4.120481129392497), np.float64(4.10632954798902), np.float64(4.098482019336163)]\n",
    "\n",
    "Best iters: [1913, 2164, 1676, 1735, 1931]\n",
    "\n",
    "\n",
    "Saved submission -> c:\\Users\\Edwin\\cs363M\\project\\ML-Project\\src\\submissions\\felipe_catboost_optuna_cv_only__2025-11-23__03-39-41.csv\n",
    "\n",
    "Saved CV-only submission: felipe_catboost_optuna_cv_only\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f7bc9",
   "metadata": {},
   "source": [
    "#### 6.1.3 Interpreting the differences between the two runs\n",
    "\n",
    "Even though Run 2 searched a bigger space and ran for more trials, it did **not** improve RMSE beyond Run 1. The difference in parameters tells us why:\n",
    "\n",
    "- **Learning rate**\n",
    "  - Run 1: `learning_rate ≈ 0.0229`  \n",
    "  - Run 2: `learning_rate ≈ 0.0150` (smaller)  \n",
    "  → A smaller learning rate usually needs **more trees** (which we see from the best iterations) and can make training slower without guaranteeing a better optimum. In our case, the lower learning rate just led to **more boosting rounds** with essentially the same RMSE.\n",
    "\n",
    "- **L2 regularization (`l2_leaf_reg`)**\n",
    "  - Run 1: `≈ 4.01` (stronger regularization)  \n",
    "  - Run 2: `≈ 1.94` (weaker regularization)  \n",
    "  → Run 2 allowed individual leaves to fit slightly more aggressively, but we also increased bagging and min leaf size. These trade-offs roughly canceled out, leading again to almost identical performance.\n",
    "\n",
    "- **Bagging behavior**\n",
    "  - Run 1: `bagging_temperature ≈ 0.46` (milder stochasticity)  \n",
    "  - Run 2: `bagging_temperature ≈ 2.16` + `bootstrap_type = \"Bayesian\"`  \n",
    "  → Run 2 used **much more aggressive Bayesian-style bagging**, injecting more randomness into which data points each tree sees. This can help reduce overfitting, but because our dataset is large and our Run 1 model was already well-regularized, the extra randomness did not produce a clear RMSE gain.\n",
    "\n",
    "- **Tree shape and leaf constraints**\n",
    "  - Run 1: used CatBoost’s default `border_count` and leaf constraints  \n",
    "  - Run 2: explicitly tuned  \n",
    "    - `border_count = 128` (fewer split candidates than 254, slightly simpler trees)  \n",
    "    - `min_data_in_leaf = 44` (prevents tiny leaves, smooths predictions)  \n",
    "  → These changes **regularize** the tree structure: they avoid overly fine splits and tiny leaves. That can improve generalization if the model is overfitting, but in our case the Run 1 configuration was already near the bias-variance sweet spot, so the extra constraints did not translate into a meaningful RMSE improvement.\n",
    "\n",
    "Overall, the second Optuna run **validated** that we were already sitting in a very flat optimum: many slightly different hyperparameter combinations (with different regularization/bagging trade-offs) all land around RMSE ≈ 4.1064.\n",
    "\n",
    "Because **Run 1** achieved the **lowest CV RMSE** and used a slightly simpler set of hyperparameters, we treated it as our **primary “best” configuration** and used it as the base for our CatBoost ensembles and blending experiments. The second run mainly served as a robustness check and showed that even after 80 more trials and a richer search space, we could not significantly beat our original tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5492ebb",
   "metadata": {},
   "source": [
    "#### 6.1.4 XGBoost Hyperparameter Tuning with Optuna (500 trials)\n",
    "\n",
    "To build a strong **second model** for ensembling, we also ran a large Optuna study for **XGBoost** with **500 trials**, using the same 5-fold CV and essentially the same preprocessing pipeline as CatBoost:\n",
    "\n",
    "- cleaned and engineered features (date features, farm clustering, vaccine sum, parity indicators),\n",
    "- applied **fold-safe target encoding** on `Farm_ID` and created farm-delta features (`Prev_vs_Farm`, `Prev_over_Farm`),\n",
    "- one-hot encoded categorical variables **within each fold** and aligned train/validation columns to avoid leakage.\n",
    "\n",
    "Each Optuna trial trained a 5-fold CV XGBoost regressor (with early stopping) and minimized the **mean 5-fold RMSE**.\n",
    "\n",
    "The search space covered both **tree structure** and **regularization**:\n",
    "\n",
    "- Tree growth and structure:\n",
    "  - `grow_policy ∈ {depthwise, lossguide}`\n",
    "  - `max_depth` (0–12, depending on `grow_policy`)\n",
    "  - `max_leaves` (16–256)\n",
    "  - `max_bin ∈ {128, 256, 512}`\n",
    "- Learning and sampling:\n",
    "  - `learning_rate ∈ [0.005, 0.1]` (log-scaled)\n",
    "  - `subsample ∈ [0.5, 0.95]`\n",
    "  - `colsample_bytree`, `colsample_bylevel`, `colsample_bynode ∈ [0.5, 0.95]`\n",
    "- Regularization:\n",
    "  - `gamma ∈ [1e-4, 10]` (log-scaled)\n",
    "  - `reg_alpha ∈ [1e-4, 50]` (log-scaled)\n",
    "  - `reg_lambda ∈ [1e-3, 50]` (log-scaled)\n",
    "\n",
    "The **best trial** out of 500 achieved a **mean CV RMSE ≈ 4.1151** with a relatively shallow but strongly regularized configuration:\n",
    "\n",
    "- `grow_policy = \"depthwise\"`\n",
    "- `max_depth = 4`\n",
    "- `max_bin = 128`\n",
    "- `learning_rate ≈ 0.00681`\n",
    "- `max_leaves ≈ 171`\n",
    "- `min_child_weight ≈ 2.88`\n",
    "- `subsample ≈ 0.529`\n",
    "- `colsample_bytree ≈ 0.528`\n",
    "- `colsample_bylevel ≈ 0.920`\n",
    "- `colsample_bynode ≈ 0.941`\n",
    "- `gamma ≈ 0.632`\n",
    "- `reg_alpha ≈ 49.96`\n",
    "- `reg_lambda ≈ 35.12`\n",
    "\n",
    "Using these tuned hyperparameters, we then retrained:\n",
    "\n",
    "- A **5-fold CV ensemble**, which achieved  \n",
    "  - **Final 5-fold CV RMSE:** ≈ **4.1151**  \n",
    "  - Fold RMSEs in the range **4.106–4.127**, with best iteration counts around **2,800–3,300** trees per fold.\n",
    "- A **full-data multi-seed ensemble**, where we:\n",
    "  - re-fit on all training data for ~the average best-iteration from CV,\n",
    "  - used multiple random seeds (e.g., 42, 100, 200, 300, 400),\n",
    "  - averaged their predictions to produce our final XGBoost test predictions.\n",
    "\n",
    "Even after this large 500-trial search, tuned XGBoost remained slightly weaker than our best CatBoost configuration (≈ **4.115** vs. ≈ **4.106** RMSE). However, because its error pattern was different, this XGBoost model was still **valuable as a complementary learner**, and we used its OOF and test predictions in our **stacking / blending experiments** described in Section 6.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c22953",
   "metadata": {},
   "source": [
    "### Showing only the optuna search and final model training with best params, not the initial baseline model as it remains unchanged from previous version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Tuning for XGBoost (500 Trials + Advanced Space)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    grow_policy = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if grow_policy == \"lossguide\":\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 0, 10)\n",
    "    else:\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 12)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"seed\": 42,\n",
    "        \"nthread\": 4,\n",
    "        \"grow_policy\": grow_policy,\n",
    "\n",
    "        \"max_bin\": trial.suggest_categorical(\"max_bin\", [128, 256, 512]),\n",
    "\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "\n",
    "        \"max_depth\": max_depth,\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 16, 256),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 20.0, log=True),\n",
    "\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.95),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.95),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 0.95),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 0.95),\n",
    "\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-4, 10.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-4, 50.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 50.0, log=True),\n",
    "    }\n",
    "\n",
    "    fold_rmses = []\n",
    "\n",
    "    for tr_idx, val_idx in kf.split(X_base):\n",
    "        X_tr_base = X_base.iloc[tr_idx]\n",
    "        X_val_base = X_base.iloc[val_idx]\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        # fold-safe TE\n",
    "        X_tr, X_val, _ = fold_target_encode(X_tr_base, y_tr, X_val_base, test_base)\n",
    "        X_tr  = add_farm_deltas(X_tr)\n",
    "        X_val = add_farm_deltas(X_val)\n",
    "\n",
    "        # OHE + align within fold\n",
    "        X_tr  = pd.get_dummies(X_tr, drop_first=False)\n",
    "        X_val = pd.get_dummies(X_val, drop_first=False)\n",
    "        X_tr, X_val = X_tr.align(X_val, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "        dtr  = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtr,\n",
    "            num_boost_round=3000,\n",
    "            evals=[(dval, \"val\")],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        best_iter = model.best_iteration if hasattr(model, \"best_iteration\") else 2999\n",
    "        preds = model.predict(dval, iteration_range=(0, best_iter + 1))\n",
    "        fold_rmses.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    return float(np.mean(fold_rmses))\n",
    "\n",
    "\n",
    "N_TRIALS = 500\n",
    "print(f\"\\nStarting Diamond-Tier XGBoost Optuna ({N_TRIALS} trials)...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS, n_jobs=2, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42,\n",
    "    \"nthread\": 4\n",
    "})\n",
    "final_params = dict(best_params)\n",
    "\n",
    "print(\"\\nBest Params:\", best_params)\n",
    "with open(\"xgboost_diamond_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "print(\"Saved best params to xgboost_diamond_params.json\")\n",
    "\n",
    "# Build GLOBAL feature template (prevents mismatch)\n",
    "X_full_enc, _, test_full_enc = fold_target_encode(X_base, y, X_base.iloc[:1], test_base)\n",
    "X_full_enc  = add_farm_deltas(X_full_enc)\n",
    "test_full_enc = add_farm_deltas(test_full_enc)\n",
    "\n",
    "X_full_enc = pd.get_dummies(X_full_enc, drop_first=False)\n",
    "test_full_enc = pd.get_dummies(test_full_enc, drop_first=False)\n",
    "X_full_enc, test_full_enc = X_full_enc.align(test_full_enc, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "GLOBAL_COLS = X_full_enc.columns.tolist()\n",
    "\n",
    "# Final 5-fold CV ensemble with tuned params\n",
    "cv_models = []\n",
    "fold_rmses = []\n",
    "fold_best_iters = []\n",
    "cv_oof = np.zeros(len(X_base), dtype=float)\n",
    "\n",
    "num_boost_round = 6000\n",
    "early_stop = 120\n",
    "\n",
    "print(\"\\nTraining final XGB 5-fold ensemble...\")\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_base), 1):\n",
    "    print(f\"\\n----- Final Fold {fold} -----\")\n",
    "    X_tr_base = X_base.iloc[tr_idx]\n",
    "    X_val_base = X_base.iloc[val_idx]\n",
    "    y_tr = y.iloc[tr_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    X_tr, X_val, _ = fold_target_encode(X_tr_base, y_tr, X_val_base, test_base)\n",
    "    X_tr  = add_farm_deltas(X_tr)\n",
    "    X_val = add_farm_deltas(X_val)\n",
    "\n",
    "    X_tr  = pd.get_dummies(X_tr, drop_first=False).reindex(columns=GLOBAL_COLS, fill_value=0)\n",
    "    X_val = pd.get_dummies(X_val, drop_first=False).reindex(columns=GLOBAL_COLS, fill_value=0)\n",
    "\n",
    "    dtr  = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=final_params,\n",
    "        dtrain=dtr,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=early_stop,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    best_iter = model.best_iteration if hasattr(model, \"best_iteration\") else num_boost_round - 1\n",
    "    preds_val = model.predict(dval, iteration_range=(0, best_iter + 1))\n",
    "\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_val, preds_val)))\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.6f}, best_iter={best_iter}\")\n",
    "\n",
    "    fold_rmses.append(rmse)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    cv_models.append(model)\n",
    "    cv_oof[val_idx] = preds_val\n",
    "\n",
    "final_cv_rmse = float(np.sqrt(mean_squared_error(y, cv_oof)))\n",
    "print(\"\\n===========================\")\n",
    "print(f\"Final CV RMSE: {final_cv_rmse:.6f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "\n",
    "# CV-ensemble predictions on test\n",
    "dtest = xgb.DMatrix(test_full_enc)\n",
    "\n",
    "cv_test_preds = np.zeros(len(test_full_enc), dtype=float)\n",
    "for m in cv_models:\n",
    "    best_iter = m.best_iteration if hasattr(m, \"best_iteration\") else int(np.mean(fold_best_iters))\n",
    "    cv_test_preds += m.predict(dtest, iteration_range=(0, best_iter + 1))\n",
    "cv_test_preds /= len(cv_models)\n",
    "\n",
    "sub_cv = pd.DataFrame({id_col: test[id_col], target: cv_test_preds})\n",
    "save_submission(sub_cv, run_name=\"felipe_xgb_optuna_cv_only\")\n",
    "print(\"Saved: felipe_xgb_optuna_cv_only\")\n",
    "\n",
    "# Rest of code from original cell is truncated as this shows the relevant parts of Optuna search and final model training. We ran Optuna + Multi-Seed Ensemble + Alpha Blending together multiple times for the different Optuna searches we attempted, but we'll show those parts later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c3561",
   "metadata": {},
   "source": [
    "### Output (truncated to not show all Optuna trials). Full output can be found in felipe_model.ipynb\n",
    "\n",
    "Optuna search complete (XGBoost, 500 trials).\n",
    "\n",
    "Best CV RMSE from Optuna: 4.115123725059022\n",
    "\n",
    "Best params: {\n",
    "  'grow_policy': 'depthwise',\n",
    "  'max_depth': 4,\n",
    "  'max_bin': 128,\n",
    "  'learning_rate': 0.006807542126783938,\n",
    "  'max_leaves': 171,\n",
    "  'min_child_weight': 2.8803643269910313,\n",
    "  'subsample': 0.5293099455741657,\n",
    "  'colsample_bytree': 0.5275894416747775,\n",
    "  'colsample_bylevel': 0.9198507961630028,\n",
    "  'colsample_bynode': 0.9413057910082173,\n",
    "  'gamma': 0.6316823862950522,\n",
    "  'reg_alpha': 49.95634643015541,\n",
    "  'reg_lambda': 35.11913756388966,\n",
    "  'objective': 'reg:squarederror',\n",
    "  'eval_metric': 'rmse',\n",
    "  'tree_method': 'hist',\n",
    "  'seed': 42,\n",
    "  'nthread': 4\n",
    "}\n",
    "\n",
    "Saved best params to xgboost_diamond_params.json\n",
    "\n",
    "Training final XGB 5-fold ensemble with best_params...\n",
    "\n",
    "----- Final CV Fold 1 -----\n",
    "Fold 1 RMSE: 4.1169, best_iter=2787\n",
    "\n",
    "----- Final CV Fold 2 -----\n",
    "Fold 2 RMSE: 4.1084, best_iter=3056\n",
    "\n",
    "----- Final CV Fold 3 -----\n",
    "Fold 3 RMSE: 4.1271, best_iter=3277\n",
    "\n",
    "----- Final CV Fold 4 -----\n",
    "Fold 4 RMSE: 4.1167, best_iter=2884\n",
    "\n",
    "----- Final CV Fold 5 -----\n",
    "Fold 5 RMSE: 4.1064, best_iter=2886\n",
    "\n",
    "Final 5-fold ensemble CV RMSE: 4.115104\n",
    "\n",
    "Fold RMSEs: [4.116921161642315, 4.1084274121406805, 4.127090955599498, 4.116687659383571, 4.106359858790124]\n",
    "\n",
    "Best iters: [2787, 3056, 3277, 2884, 2886]\n",
    "\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_xgb_optuna_cv_only__2025-11-23__05-36-21.csv\n",
    "\n",
    "Saved CV-only submission: felipe_xgb_optuna_cv_only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8761f",
   "metadata": {},
   "source": [
    "### 6.2 Ensembling, Stacking, and Snapshot Strategy\n",
    "\n",
    "Once we had a strong single CatBoost model, we explored techniques to reduce variance and squeeze out additional performance:\n",
    "\n",
    "#### 6.2.1 Multi-Seed Ensembling\n",
    "\n",
    "Even with fixed hyperparameters, CatBoost’s training process is stochastic (e.g., random permutations of categorical features, bootstrap sampling). We trained the same CatBoost configuration with **different random seeds**, and averaged their predictions:\n",
    "\n",
    "- Trained the tuned CatBoost model on the full training data with several seeds (e.g., 5 seeds).  \n",
    "- Averaged the predictions from all seed models on the test set.\n",
    "\n",
    "This simple **multi-seed ensemble** slightly improved CV RMSE and also stabilized leaderboard performance, consistent with variance-reduction theory.\n",
    "\n",
    "\n",
    "#### 6.2.2 Stacking and Blending\n",
    "\n",
    "We also experimented with **stacking / blending** strategies:\n",
    "\n",
    "- **CatBoost + XGBoost blend**  \n",
    "  - Trained both a tuned CatBoost model and a tuned XGBoost model (500-trial Optuna study described in 6.1.4).  \n",
    "  - Created out-of-fold (OOF) predictions for each model using 5-fold CV.  \n",
    "  - Searched over a blending weight `alpha` in  \n",
    "    `y_blend = alpha * y_catboost + (1 - alpha) * y_xgboost`.  \n",
    "  - Selected the `alpha` that minimized OOF RMSE, then used that same weight to blend test predictions.\n",
    "\n",
    "- **Ridge stacking on top of OOF predictions**  \n",
    "  - Built a 2-feature meta-dataset where each row contained `(CatBoost_OOF, XGBoost_OOF)`.  \n",
    "  - Trained a Ridge regression model to learn the optimal linear combination.  \n",
    "  - Used the fitted Ridge model to combine test predictions.  \n",
    "\n",
    "These stacking experiments generally gave small gains on CV, but in some cases were less stable on the leaderboard than a pure CatBoost ensemble. This is likely because the meta-learner can overfit to the noise in the OOF predictions, especially when the base models are already highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2de4b",
   "metadata": {},
   "source": [
    "### One example of the Multi-Seed Full-Data Ensemble + Blending below for CatBoost only (continuation of the previous Optuna cells, as we ran Optuna + Multi-Seed Ensemble + Alpha Blending together various times for the different Optuna searches and model variations we attempted)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-data multi-seed ensemble\n",
    "avg_best_iter = int(np.mean(fold_best_iters))\n",
    "print(\"\\nAverage best_iteration across folds:\", avg_best_iter)\n",
    "\n",
    "n_estimators_full = avg_best_iter\n",
    "print(\"Using n_estimators for full-data models:\", n_estimators_full)\n",
    "\n",
    "seed_list = [101, 202, 303, 404, 505]\n",
    "full_seed_models = []\n",
    "full_seed_preds = np.zeros(len(test_df), dtype=float)\n",
    "\n",
    "for seed in seed_list:\n",
    "    print(f\"\\nTraining full-data model with seed={seed}...\")\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=n_estimators_full,\n",
    "        random_seed=seed,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        **final_params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    full_seed_models.append(model)\n",
    "    full_seed_preds += model.predict(test_df)\n",
    "\n",
    "full_seed_preds /= len(seed_list)\n",
    "\n",
    "sub_full = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: full_seed_preds\n",
    "})\n",
    "\n",
    "save_submission(sub_full, run_name=\"felipe_catboost_optuna_full_seed_only\")\n",
    "print(\"Saved full-data seed ensemble submission: felipe_catboost_optuna_full_seed_only\")\n",
    "\n",
    "\n",
    "# Blended submissions (CV + full-data)\n",
    "\n",
    "# 70% CV ensemble, 30% full-data ensemble\n",
    "alpha_70 = 0.7\n",
    "blend_70_30 = alpha_70 * cv_test_preds + (1.0 - alpha_70) * full_seed_preds\n",
    "\n",
    "sub_blend_70_30 = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: blend_70_30\n",
    "})\n",
    "save_submission(sub_blend_70_30, run_name=\"felipe_catboost_optuna_blend_70_30\")\n",
    "print(\"Saved blend 70/30 submission: felipe_catboost_optuna_blend_70_30\")\n",
    "\n",
    "# 50% / 50% blend\n",
    "alpha_50 = 0.5\n",
    "blend_50_50 = alpha_50 * cv_test_preds + (1.0 - alpha_50) * full_seed_preds\n",
    "\n",
    "sub_blend_50_50 = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: blend_50_50\n",
    "})\n",
    "save_submission(sub_blend_50_50, run_name=\"felipe_catboost_optuna_blend_50_50\")\n",
    "print(\"Saved blend 50/50 submission: felipe_catboost_optuna_blend_50_50\")\n",
    "\n",
    "# 8) EXTRA: OOF RMSE comparison of strategies\n",
    "#     (CV-only vs \"full-seed style\" vs blends)\n",
    "print(\"\\n\\n=== Running OOF comparison for strategies (this is offline, NOT Kaggle) ===\")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# OOF containers (same length as y)\n",
    "cv_only_oof        = np.zeros(len(X), dtype=float)\n",
    "full_style_oof     = np.zeros(len(X), dtype=float)\n",
    "blend_70_30_oof    = np.zeros(len(X), dtype=float)\n",
    "blend_50_50_oof    = np.zeros(len(X), dtype=float)\n",
    "\n",
    "# Re-use same KFold splits and same cv_models (order aligned with enumerate start=1)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), start=1):\n",
    "    print(f\"\\n[OOF Eval] Fold {fold}\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # 1) CV-only predictions (from already trained cv_models)\n",
    "    cv_model = cv_models[fold - 1]\n",
    "    cv_preds_val = cv_model.predict(X_val)\n",
    "\n",
    "    # 2) \"Full-seed style\" ensemble trained ONLY on X_train (no leakage)\n",
    "    fold_full_preds = np.zeros(len(val_idx), dtype=float)\n",
    "    for seed in seed_list:\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=n_estimators_full,\n",
    "            random_seed=seed,\n",
    "            verbose=False,\n",
    "            thread_count=4,\n",
    "            **final_params\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cat_features=cat_features,\n",
    "            verbose=False\n",
    "        )\n",
    "        fold_full_preds += model.predict(X_val)\n",
    "\n",
    "    fold_full_preds /= len(seed_list)\n",
    "\n",
    "    # Store OOF preds\n",
    "    cv_only_oof[val_idx]     = cv_preds_val\n",
    "    full_style_oof[val_idx]  = fold_full_preds\n",
    "    blend_70_30_oof[val_idx] = alpha_70 * cv_preds_val + (1.0 - alpha_70) * fold_full_preds\n",
    "    blend_50_50_oof[val_idx] = alpha_50 * cv_preds_val + (1.0 - alpha_50) * fold_full_preds\n",
    "\n",
    "# Compute OOF RMSEs for each strategy\n",
    "rmse_cv_only     = rmse(y, cv_only_oof)\n",
    "rmse_full_style  = rmse(y, full_style_oof)\n",
    "rmse_blend_70_30 = rmse(y, blend_70_30_oof)\n",
    "rmse_blend_50_50 = rmse(y, blend_50_50_oof)\n",
    "\n",
    "print(\"\\n=== OOF RMSE summary (lower is better) ===\")\n",
    "print(f\"CV-only ensemble OOF RMSE:        {rmse_cv_only:.6f}\")\n",
    "print(f\"Full-seed-style ensemble OOF RMSE: {rmse_full_style:.6f}\")\n",
    "print(f\"Blend 70% CV / 30% full OOF RMSE: {rmse_blend_70_30:.6f}\")\n",
    "print(f\"Blend 50% CV / 50% full OOF RMSE: {rmse_blend_50_50:.6f}\")\n",
    "print(\"Note: CV-only OOF RMSE should closely match 'Final 5-fold ensemble CV RMSE' above.\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 9) Find best alpha blend by OOF (NOT hardcoded)\n",
    "# ======================================================\n",
    "\n",
    "alphas = np.linspace(0, 1, 201)  # 0.005 steps\n",
    "best_alpha = None\n",
    "best_alpha_rmse = float(\"inf\")\n",
    "\n",
    "for a in alphas:\n",
    "    blend_oof = a * cv_only_oof + (1 - a) * full_style_oof\n",
    "    r = rmse(y, blend_oof)\n",
    "    if r < best_alpha_rmse:\n",
    "        best_alpha_rmse = r\n",
    "        best_alpha = a\n",
    "\n",
    "print(\"\\n=== Best OOF blend search ===\")\n",
    "print(f\"Best alpha (CV weight) = {best_alpha:.3f}\")\n",
    "print(f\"Best blended OOF RMSE  = {best_alpha_rmse:.6f}\")\n",
    "\n",
    "# Use this alpha to blend TEST predictions too\n",
    "best_blend_test = best_alpha * cv_test_preds + (1 - best_alpha) * full_seed_preds\n",
    "\n",
    "sub_best_blend = pd.DataFrame({\n",
    "    id_col: test[id_col],\n",
    "    target: best_blend_test\n",
    "})\n",
    "\n",
    "save_submission(sub_best_blend, run_name=f\"felipe_catboost_optuna_best_blend_{best_alpha:.3f}\")\n",
    "print(\"Saved best-alpha blend submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be806816",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Then this was our attempt to stack our best CatBoost and XGBoost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, importlib, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# submission utils\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import submission_utils\n",
    "importlib.reload(submission_utils)\n",
    "from submission_utils import save_submission\n",
    "\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "test  = pd.read_csv(\"../../data/cattle_data_test.csv\")\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "\n",
    "# CATBOOST preprocessing\n",
    "def preprocess_pipeline_cb(df, encode_flag=True, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "\n",
    "    if target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0]\n",
    "        y = milk_features[target_col]\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    milk_features[numeric_cols] = milk_features[numeric_cols].fillna(milk_features[numeric_cols].median())\n",
    "\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"])\n",
    "        milk_features[\"year\"] = milk_features[\"Date\"].dt.year\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"is_weekend\"] = milk_features[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "        milk_features[\"date_ordinal\"] = milk_features[\"Date\"].map(pd.Timestamp.toordinal)\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "\n",
    "        farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "        scaler = StandardScaler()\n",
    "        farm_scaled = scaler.fit_transform(farm_features)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "        milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "\n",
    "    milk_features = milk_features.drop(columns=[\"Cattle_ID\", \"Farm_ID\"], errors=\"ignore\")\n",
    "\n",
    "    if encode_flag:\n",
    "        cat_cols = milk_features.select_dtypes(include=\"object\").columns.tolist()\n",
    "        milk_features = pd.get_dummies(milk_features, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    return milk_features.copy(), y\n",
    "\n",
    "\n",
    "# XGBOOST preprocessing\n",
    "def preprocess_pipeline_xgb(df, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "\n",
    "    if target_col is not None and target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0].copy()\n",
    "        y = milk_features[target_col].copy()\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].astype(str).str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    if len(numeric_cols) > 0:\n",
    "        milk_features[numeric_cols] = milk_features[numeric_cols].fillna(\n",
    "            milk_features[numeric_cols].median()\n",
    "        )\n",
    "\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"], errors=\"coerce\")\n",
    "        milk_features[\"year\"] = milk_features[\"Date\"].dt.year\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"date_ordinal\"] = milk_features[\"Date\"].map(pd.Timestamp.toordinal)\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "        if len(farm_numeric_cols) > 0:\n",
    "            farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "            scaler = StandardScaler()\n",
    "            farm_scaled = scaler.fit_transform(farm_features)\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "            farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "            milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "        else:\n",
    "            milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    vax_cols = [c for c in milk_features.columns if c.endswith(\"_Vaccine\")]\n",
    "    if len(vax_cols) > 0:\n",
    "        milk_features[\"Vax_Sum\"] = milk_features[vax_cols].sum(axis=1)\n",
    "\n",
    "    if \"Parity\" in milk_features.columns:\n",
    "        milk_features[\"First_Calf\"] = (milk_features[\"Parity\"] == 1).astype(int)\n",
    "        milk_features[\"Prime_Cow\"] = ((milk_features[\"Parity\"] >= 2) & (milk_features[\"Parity\"] <= 4)).astype(int)\n",
    "        milk_features[\"Old_Cow\"]   = (milk_features[\"Parity\"] > 4).astype(int)\n",
    "\n",
    "    milk_features = milk_features.drop(columns=[\"Cattle_ID\"], errors=\"ignore\")\n",
    "\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    return milk_features.copy(), y\n",
    "\n",
    "\n",
    "def fold_target_encode(train_df, y_train, val_df, test_df,\n",
    "                       col=\"Farm_ID\", new_name=\"Farm_Performance\"):\n",
    "    train_df = train_df.copy()\n",
    "    val_df   = val_df.copy()\n",
    "    test_df  = test_df.copy()\n",
    "\n",
    "    global_avg = float(y_train.mean())\n",
    "    means = y_train.groupby(train_df[col]).mean()\n",
    "\n",
    "    train_df[new_name] = train_df[col].map(means).fillna(global_avg)\n",
    "    val_df[new_name]   = val_df[col].map(means).fillna(global_avg)\n",
    "    test_df[new_name]  = test_df[col].map(means).fillna(global_avg)\n",
    "\n",
    "    train_df = train_df.drop(columns=[col], errors=\"ignore\")\n",
    "    val_df   = val_df.drop(columns=[col], errors=\"ignore\")\n",
    "    test_df  = test_df.drop(columns=[col], errors=\"ignore\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def add_farm_deltas(df_):\n",
    "    df_ = df_.copy()\n",
    "    if \"Previous_Week_Avg_Yield\" in df_.columns and \"Farm_Performance\" in df_.columns:\n",
    "        df_[\"Prev_vs_Farm\"]   = df_[\"Previous_Week_Avg_Yield\"] - df_[\"Farm_Performance\"]\n",
    "        df_[\"Prev_over_Farm\"] = df_[\"Previous_Week_Avg_Yield\"] / (df_[\"Farm_Performance\"] + 1e-6)\n",
    "    return df_\n",
    "\n",
    "\n",
    "# Build base datasets\n",
    "# CatBoost base\n",
    "X_cb, y_cb = preprocess_pipeline_cb(train, encode_flag=False, target_col=target)\n",
    "test_cb, _ = preprocess_pipeline_cb(test, encode_flag=False, target_col=None)\n",
    "cat_features = X_cb.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# XGB base\n",
    "X_base, y = preprocess_pipeline_xgb(train, target_col=target)\n",
    "test_base, _ = preprocess_pipeline_xgb(test, target_col=None)\n",
    "\n",
    "X_base = X_base.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "test_base = test_base.reset_index(drop=True)\n",
    "\n",
    "# Same KFold splits for alignment\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Best params for each\n",
    "catboost_best = {\n",
    "    \"depth\": 6,\n",
    "    \"learning_rate\": 0.022872270426979868,\n",
    "    \"l2_leaf_reg\": 4.014881862534349,\n",
    "    \"subsample\": 0.847236734294946,\n",
    "    \"random_strength\": 0.731107887920747,\n",
    "    \"bagging_temperature\": 0.4635035306948895\n",
    "}\n",
    "\n",
    "xgb_best = {\n",
    "    \"grow_policy\": \"depthwise\",\n",
    "    \"max_depth\": 4,\n",
    "    \"max_bin\": 128,\n",
    "    \"learning_rate\": 0.006807542126783938,\n",
    "    \"max_leaves\": 171,\n",
    "    \"min_child_weight\": 2.8803643269910313,\n",
    "    \"subsample\": 0.5293099455741657,\n",
    "    \"colsample_bytree\": 0.5275894416747775,\n",
    "    \"colsample_bylevel\": 0.9198507961630028,\n",
    "    \"colsample_bynode\": 0.9413057910082173,\n",
    "    \"gamma\": 0.6316823862950522,\n",
    "    \"reg_alpha\": 49.95634643015541,\n",
    "    \"reg_lambda\": 35.11913756388966,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42,\n",
    "    \"nthread\": 4\n",
    "}\n",
    "\n",
    "# feature template for XGB (prevents mismatch)\n",
    "X_full_enc, _, test_full_enc = fold_target_encode(X_base, y, X_base.iloc[:1], test_base)\n",
    "X_full_enc = add_farm_deltas(X_full_enc)\n",
    "test_full_enc = add_farm_deltas(test_full_enc)\n",
    "\n",
    "X_full_enc = pd.get_dummies(X_full_enc, drop_first=False)\n",
    "test_full_enc = pd.get_dummies(test_full_enc, drop_first=False)\n",
    "X_full_enc, test_full_enc = X_full_enc.align(test_full_enc, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "GLOBAL_COLS = X_full_enc.columns.tolist()\n",
    "dtest_global = xgb.DMatrix(test_full_enc)\n",
    "\n",
    "\n",
    "# Rebuild CatBoost OOF + Test preds\n",
    "print(\"\\nRebuilding CatBoost OOF + test preds...\")\n",
    "cb_oof = np.zeros(len(X_cb))\n",
    "cb_models = []\n",
    "cb_best_iters = []\n",
    "cb_fold_rmses = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr, y_val = y_cb.iloc[tr_idx], y_cb.iloc[val_idx]\n",
    "\n",
    "    cb = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        use_best_model=True,\n",
    "        **catboost_best\n",
    "    )\n",
    "    cb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_iter = cb.get_best_iteration()\n",
    "    preds = cb.predict(X_val)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    print(f\"CB Fold {fold} RMSE: {rmse:.6f}, best_iter={best_iter}\")\n",
    "\n",
    "    cb_oof[val_idx] = preds\n",
    "    cb_models.append(cb)\n",
    "    cb_best_iters.append(best_iter)\n",
    "    cb_fold_rmses.append(rmse)\n",
    "\n",
    "cb_cv_rmse = float(np.sqrt(mean_squared_error(y_cb, cb_oof)))\n",
    "print(f\"CatBoost OOF RMSE: {cb_cv_rmse:.6f}\")\n",
    "\n",
    "cb_test_preds = np.mean([m.predict(test_cb) for m in cb_models], axis=0)\n",
    "\n",
    "# Rebuild XGBoost OOF + Test preds\n",
    "print(\"\\nRebuilding XGBoost OOF + test preds...\")\n",
    "xgb_oof = np.zeros(len(X_base))\n",
    "xgb_models = []\n",
    "xgb_best_iters = []\n",
    "xgb_fold_rmses = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_base), 1):\n",
    "    X_tr_base = X_base.iloc[tr_idx]\n",
    "    X_val_base = X_base.iloc[val_idx]\n",
    "    y_tr = y.iloc[tr_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    # fold-safe TE\n",
    "    X_tr, X_val, _ = fold_target_encode(X_tr_base, y_tr, X_val_base, test_base)\n",
    "    X_tr = add_farm_deltas(X_tr)\n",
    "    X_val = add_farm_deltas(X_val)\n",
    "\n",
    "    # OHE + align to GLOBAL_COLS\n",
    "    X_tr = pd.get_dummies(X_tr, drop_first=False).reindex(columns=GLOBAL_COLS, fill_value=0)\n",
    "    X_val = pd.get_dummies(X_val, drop_first=False).reindex(columns=GLOBAL_COLS, fill_value=0)\n",
    "\n",
    "    dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=xgb_best,\n",
    "        dtrain=dtr,\n",
    "        num_boost_round=6000,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=120,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    best_iter = model.best_iteration if hasattr(model, \"best_iteration\") else 5999\n",
    "    preds = model.predict(dval, iteration_range=(0, best_iter + 1))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    print(f\"XGB Fold {fold} RMSE: {rmse:.6f}, best_iter={best_iter}\")\n",
    "\n",
    "    xgb_oof[val_idx] = preds\n",
    "    xgb_models.append(model)\n",
    "    xgb_best_iters.append(best_iter)\n",
    "    xgb_fold_rmses.append(rmse)\n",
    "\n",
    "xgb_cv_rmse = float(np.sqrt(mean_squared_error(y, xgb_oof)))\n",
    "print(f\"XGBoost OOF RMSE: {xgb_cv_rmse:.6f}\")\n",
    "\n",
    "# test preds = average of fold models using their own best_iter\n",
    "xgb_test_preds = np.zeros(len(test_full_enc))\n",
    "for m, bi in zip(xgb_models, xgb_best_iters):\n",
    "    xgb_test_preds += m.predict(dtest_global, iteration_range=(0, bi + 1))\n",
    "xgb_test_preds /= len(xgb_models)\n",
    "\n",
    "# Linear alpha stacking (OOF search)\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "print(\"\\nAlpha search between CatBoost and XGBoost...\")\n",
    "alphas = np.linspace(0, 1, 401)  # 0.0025 steps\n",
    "best_a, best_r = None, float(\"inf\")\n",
    "\n",
    "for a in alphas:\n",
    "    blend_oof = a * cb_oof + (1 - a) * xgb_oof\n",
    "    r = rmse(y_cb, blend_oof)\n",
    "    if r < best_r:\n",
    "        best_r, best_a = r, a\n",
    "\n",
    "print(f\"Best alpha (CB weight) = {best_a:.4f}\")\n",
    "print(f\"Best 2-model blended OOF RMSE = {best_r:.6f}\")\n",
    "\n",
    "stack_test_alpha = best_a * cb_test_preds + (1 - best_a) * xgb_test_preds\n",
    "sub_alpha = pd.DataFrame({id_col: test[id_col], target: stack_test_alpha})\n",
    "# save_submission(sub_alpha, run_name=f\"felipe_stack_cb_xgb_alpha_{best_a:.4f}\")\n",
    "print(\"Saved alpha-stacked submission.\")\n",
    "\n",
    "\n",
    "# Ridge meta-stacking (still just 2-model linear stack)\n",
    "print(\"\\nRidge meta-stacking on OOF preds...\")\n",
    "Z = np.vstack([cb_oof, xgb_oof]).T  # shape (n, 2)\n",
    "ridge_alphas = np.logspace(-4, 3, 50)\n",
    "\n",
    "best_ra, best_rr = None, float(\"inf\")\n",
    "best_ridge = None\n",
    "\n",
    "# simple CV over ridge strength on OOF\n",
    "for ra in ridge_alphas:\n",
    "    # fit ridge on full OOF, eval via same OOF (OK since OOF already leak-safe)\n",
    "    model = Ridge(alpha=ra, fit_intercept=True)\n",
    "    model.fit(Z, y_cb)\n",
    "    preds = model.predict(Z)\n",
    "    r = rmse(y_cb, preds)\n",
    "\n",
    "    if r < best_rr:\n",
    "        best_rr, best_ra, best_ridge = r, ra, model\n",
    "\n",
    "print(f\"Best Ridge alpha = {best_ra:.6f}\")\n",
    "print(f\"Best Ridge-stacked OOF RMSE = {best_rr:.6f}\")\n",
    "print(\"Ridge weights (CB, XGB) =\", best_ridge.coef_, \"intercept =\", best_ridge.intercept_)\n",
    "\n",
    "Z_test = np.vstack([cb_test_preds, xgb_test_preds]).T\n",
    "stack_test_ridge = best_ridge.predict(Z_test)\n",
    "\n",
    "sub_ridge = pd.DataFrame({id_col: test[id_col], target: stack_test_ridge})\n",
    "save_submission(sub_ridge, run_name=f\"felipe_stack_cb_xgb_ridge_{best_ra:.6f}\")\n",
    "print(\"Saved ridge-stacked submission.\")\n",
    "\n",
    "\n",
    "print(\"\\nDONE. Compare alpha-stack vs ridge-stack RMSE above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8aaf7",
   "metadata": {},
   "source": [
    "### Full output of trying to stack best CatBoost + XGBoost\n",
    "\n",
    "Rebuilding CatBoost OOF + test preds...\n",
    "\n",
    "CB Fold 1 RMSE: 4.106957, best_iter=1157\n",
    "\n",
    "CB Fold 2 RMSE: 4.100151, best_iter=1127\n",
    "\n",
    "CB Fold 3 RMSE: 4.120099, best_iter=1093\n",
    "\n",
    "CB Fold 4 RMSE: 4.106328, best_iter=1094\n",
    "\n",
    "CB Fold 5 RMSE: 4.098421, best_iter=999\n",
    "\n",
    "CatBoost OOF RMSE: 4.106398\n",
    "\n",
    "\n",
    "Rebuilding XGBoost OOF + test preds...\n",
    "\n",
    "XGB Fold 1 RMSE: 4.116921, best_iter=2787\n",
    "\n",
    "XGB Fold 2 RMSE: 4.108427, best_iter=3056\n",
    "\n",
    "XGB Fold 3 RMSE: 4.127091, best_iter=3277\n",
    "\n",
    "XGB Fold 4 RMSE: 4.116688, best_iter=2884\n",
    "\n",
    "XGB Fold 5 RMSE: 4.106360, best_iter=2886\n",
    "\n",
    "XGBoost OOF RMSE: 4.115104\n",
    "\n",
    "\n",
    "Alpha search between CatBoost and XGBoost...\n",
    "\n",
    "Best alpha (CB weight) = 0.8675\n",
    "\n",
    "Best 2-model blended OOF RMSE = 4.106189\n",
    "\n",
    "Saved alpha-stacked submission.\n",
    "\n",
    "Ridge meta-stacking on OOF preds...\n",
    "\n",
    "Best Ridge alpha = 0.000100\n",
    "\n",
    "Best Ridge-stacked OOF RMSE = 4.105955\n",
    "\n",
    "Ridge weights (CB, XGB) = [0.87095368 0.14202604] intercept = -0.20003385945580554\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_stack_cb_xgb_ridge_0.000100__2025-11-23__06-20-49.csv\n",
    "\n",
    "Saved ridge-stacked submission.\n",
    "\n",
    "DONE. Compare alpha-stack vs ridge-stack RMSE above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b5ca0",
   "metadata": {},
   "source": [
    "#### 6.2.3 Snapshot-Style Ensembling (Parameter Variants)\n",
    "\n",
    "We then tried to extend this idea to a **snapshot ensemble** during our last moments before the submission deadline:\n",
    "\n",
    "- In addition to changing the random seed, we trained several “nearby” versions of the model by slightly varying:\n",
    "  - `depth` (e.g., 5, 6, 7)\n",
    "  - `learning_rate` (e.g., ±10% around the tuned value)\n",
    "- For each parameter variant, we trained models with multiple seeds and averaged all of them.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- **Shallower trees** (e.g., depth 5) capture smoother, more global patterns.\n",
    "- **Deeper trees** (e.g., depth 7) can model more complex feature interactions but risk overfitting.\n",
    "- Slight learning-rate shifts change the effective regularization.\n",
    "\n",
    "By averaging these diverse models, we hoped to obtain a more robust predictor that was less sensitive to any specific hyperparameter setting or random seed. The model ended up taking up 4+ hours to train after we started it at 10pm before the 11:55pm deadline, so we ended up stopping it after we saw it likely wasn't finishing anytime soon, but believe it would have done best since it was squeezing out our already best model even more.\n",
    "\n",
    "In the final model, we relied primarily on **CatBoost-only CV + Multi-Seed ensembling**, with blending used carefully and only when it showed consistent improvement across folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49417558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params for CatBoost\n",
    "catboost_best = {\n",
    "    \"depth\": 6,\n",
    "    \"learning_rate\": 0.022872270426979868,\n",
    "    \"l2_leaf_reg\": 4.014881862534349,\n",
    "    \"subsample\": 0.847236734294946,\n",
    "    \"random_strength\": 0.731107887920747,\n",
    "    \"bagging_temperature\": 0.4635035306948895\n",
    "}\n",
    "\n",
    "SEED_LIST = [101, 202, 303, 404, 505]\n",
    "\n",
    "# SNAPSHOT VARIANTS\n",
    "PARAM_VARIANTS = [\n",
    "    dict(catboost_best),\n",
    "    dict(catboost_best, depth=5),\n",
    "    dict(catboost_best, depth=7),\n",
    "    dict(catboost_best, learning_rate=catboost_best[\"learning_rate\"] * 0.9),\n",
    "    dict(catboost_best, learning_rate=catboost_best[\"learning_rate\"] * 1.1),\n",
    "]\n",
    "\n",
    "# CV-only OOF + test preds (YOUR exact training)\n",
    "print(\"\\nRebuilding CatBoost CV-only OOF + test preds...\")\n",
    "cv_only_oof = np.zeros(len(X_cb))\n",
    "cv_models = []\n",
    "fold_best_iters = []\n",
    "fold_rmses = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr, y_val = y_cb.iloc[tr_idx], y_cb.iloc[val_idx]\n",
    "\n",
    "    cb = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=THREADS,\n",
    "        verbose=False,\n",
    "        use_best_model=True,\n",
    "        **catboost_best\n",
    "    )\n",
    "    cb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_iter = cb.get_best_iteration()\n",
    "    preds_val = cb.predict(X_val)\n",
    "    fold_rmse = rmse(y_val, preds_val)\n",
    "\n",
    "    print(f\"CB Fold {fold} RMSE: {fold_rmse:.6f}, best_iter={best_iter}\")\n",
    "\n",
    "    cv_only_oof[val_idx] = preds_val\n",
    "    cv_models.append(cb)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    fold_rmses.append(fold_rmse)\n",
    "\n",
    "cv_only_rmse = rmse(y_cb, cv_only_oof)\n",
    "avg_best_iter = int(np.mean(fold_best_iters))\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(f\"CV-only ensemble OOF RMSE: {cv_only_rmse:.6f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "print(\"Average best_iteration across folds:\", avg_best_iter)\n",
    "print(\"===============================\\n\")\n",
    "\n",
    "# CV-only test preds\n",
    "cv_test_preds = np.mean([m.predict(test_cb) for m in cv_models], axis=0)\n",
    "\n",
    "\n",
    "# Full-style OOF + full-seed test preds\n",
    "print(\"\\nRebuilding Full-style OOF + full-seed test preds...\")\n",
    "full_style_oof = np.zeros(len(X_cb))\n",
    "full_seed_test_preds = np.zeros(len(test_cb))\n",
    "\n",
    "# Full-style OOF: per fold, train multi-seed on fold-train only\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr = y_cb.iloc[tr_idx]\n",
    "\n",
    "    fold_full_preds = np.zeros(len(val_idx), dtype=float)\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        m = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=avg_best_iter,\n",
    "            random_seed=seed,\n",
    "            thread_count=THREADS,\n",
    "            verbose=False,\n",
    "            **catboost_best\n",
    "        )\n",
    "        m.fit(X_tr, y_tr, cat_features=cat_features, verbose=False)\n",
    "        fold_full_preds += m.predict(X_val)\n",
    "\n",
    "    fold_full_preds /= len(SEED_LIST)\n",
    "    full_style_oof[val_idx] = fold_full_preds\n",
    "\n",
    "full_style_rmse = rmse(y_cb, full_style_oof)\n",
    "print(f\"Full-style ensemble OOF RMSE: {full_style_rmse:.6f}\")\n",
    "\n",
    "# Full-seed test preds: multi-seed on full data\n",
    "for seed in SEED_LIST:\n",
    "    m = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=avg_best_iter,\n",
    "        random_seed=seed,\n",
    "        thread_count=THREADS,\n",
    "        verbose=False,\n",
    "        **catboost_best\n",
    "    )\n",
    "    m.fit(X_cb, y_cb, cat_features=cat_features, verbose=False)\n",
    "    full_seed_test_preds += m.predict(test_cb)\n",
    "\n",
    "full_seed_test_preds /= len(SEED_LIST)\n",
    "\n",
    "\n",
    "# Best alpha blend search (CV-only vs Full-style)\n",
    "print(\"\\nAlpha search between CV-only and Full-style...\")\n",
    "alphas = np.linspace(0, 1, 201)\n",
    "best_alpha, best_alpha_rmse = None, float(\"inf\")\n",
    "\n",
    "for a in alphas:\n",
    "    blend_oof = a * cv_only_oof + (1 - a) * full_style_oof\n",
    "    r = rmse(y_cb, blend_oof)\n",
    "    if r < best_alpha_rmse:\n",
    "        best_alpha_rmse = r\n",
    "        best_alpha = a\n",
    "\n",
    "print(\"\\n=== Best alpha blend (CV vs Full) ===\")\n",
    "print(f\"Best alpha (CV weight): {best_alpha:.4f}\")\n",
    "print(f\"Best blended OOF RMSE : {best_alpha_rmse:.6f}\")\n",
    "\n",
    "alpha_test_preds = best_alpha * cv_test_preds + (1 - best_alpha) * full_seed_test_preds\n",
    "\n",
    "# Ridge meta-stacking (CV-only vs Full-style)\n",
    "print(\"\\nRidge meta-stacking on (CV-only, Full-style)...\")\n",
    "meta_X = np.vstack([cv_only_oof, full_style_oof]).T\n",
    "meta_test = np.vstack([cv_test_preds, full_seed_test_preds]).T\n",
    "\n",
    "ridge_alphas = np.logspace(-4, 2, 60)\n",
    "best_ra, best_rr, best_ridge = None, float(\"inf\"), None\n",
    "\n",
    "for ra in ridge_alphas:\n",
    "    ridge = Ridge(alpha=ra, fit_intercept=True, random_state=42)\n",
    "    ridge.fit(meta_X, y_cb)\n",
    "    preds_oof = ridge.predict(meta_X)\n",
    "    r = rmse(y_cb, preds_oof)\n",
    "    if r < best_rr:\n",
    "        best_rr, best_ra, best_ridge = r, ra, ridge\n",
    "\n",
    "print(\"\\n=== Best Ridge stack (CV vs Full) ===\")\n",
    "print(f\"Best Ridge alpha: {best_ra:.6f}\")\n",
    "print(f\"Best Ridge-stacked OOF RMSE: {best_rr:.6f}\")\n",
    "print(\"Ridge weights (CV, Full):\", best_ridge.coef_, \"intercept =\", best_ridge.intercept_)\n",
    "\n",
    "ridge_test_preds = best_ridge.predict(meta_test)\n",
    "\n",
    "# SNAPSHOT OOF (per fold)\n",
    "print(\"\\nBuilding Snapshot OOF ensemble (seeds + param variants)...\")\n",
    "snapshot_oof = np.zeros(len(X_cb), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr = y_cb.iloc[tr_idx]\n",
    "\n",
    "    fold_preds = np.zeros(len(val_idx), dtype=float)\n",
    "\n",
    "    for params in PARAM_VARIANTS:\n",
    "        for seed in SEED_LIST:\n",
    "            m = CatBoostRegressor(\n",
    "                loss_function=\"RMSE\",\n",
    "                n_estimators=avg_best_iter,\n",
    "                random_seed=seed,\n",
    "                thread_count=THREADS,\n",
    "                verbose=False,\n",
    "                **params\n",
    "            )\n",
    "            m.fit(X_tr, y_tr, cat_features=cat_features, verbose=False)\n",
    "            fold_preds += m.predict(X_val)\n",
    "\n",
    "    fold_preds /= (len(PARAM_VARIANTS) * len(SEED_LIST))\n",
    "    snapshot_oof[val_idx] = fold_preds\n",
    "\n",
    "snapshot_oof_rmse = rmse(y_cb, snapshot_oof)\n",
    "print(f\"Snapshot OOF RMSE: {snapshot_oof_rmse:.6f}\")\n",
    "\n",
    "# SNAPSHOT full-seed test preds  <<< ADDED\n",
    "print(\"\\nTraining Snapshot Full-Seed Ensemble on full data...\")\n",
    "snapshot_full_test_preds = np.zeros(len(test_cb), dtype=float)\n",
    "\n",
    "for params in PARAM_VARIANTS:\n",
    "    for seed in SEED_LIST:\n",
    "        m = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=avg_best_iter,\n",
    "            random_seed=seed,\n",
    "            thread_count=THREADS,\n",
    "            verbose=False,\n",
    "            **params\n",
    "        )\n",
    "        m.fit(X_cb, y_cb, cat_features=cat_features, verbose=False)\n",
    "        snapshot_full_test_preds += m.predict(test_cb)\n",
    "\n",
    "snapshot_full_test_preds /= (len(PARAM_VARIANTS) * len(SEED_LIST))\n",
    "print(\"Snapshot full-seed preds ready.\")\n",
    "\n",
    "# Alpha search (CV-only vs Snapshot)\n",
    "print(\"\\nAlpha search between CV-only and Snapshot...\")\n",
    "alphas2 = np.linspace(0, 1, 401)  # finer search\n",
    "best_alpha_snap, best_alpha_snap_rmse = None, float(\"inf\")\n",
    "\n",
    "for a in alphas2:\n",
    "    blend_oof = a * cv_only_oof + (1 - a) * snapshot_oof\n",
    "    r = rmse(y_cb, blend_oof)\n",
    "    if r < best_alpha_snap_rmse:\n",
    "        best_alpha_snap_rmse = r\n",
    "        best_alpha_snap = a\n",
    "\n",
    "print(\"\\n=== Best alpha blend (CV vs Snapshot) ===\")\n",
    "print(f\"Best alpha (CV weight): {best_alpha_snap:.4f}\")\n",
    "print(f\"Best blended OOF RMSE : {best_alpha_snap_rmse:.6f}\")\n",
    "\n",
    "alpha_snapshot_test_preds = best_alpha_snap * cv_test_preds + (1 - best_alpha_snap) * snapshot_full_test_preds\n",
    "\n",
    "# Save submissions (original + snapshot adds)\n",
    "sub_cv = pd.DataFrame({id_col: test[id_col], target: cv_test_preds})\n",
    "save_submission(sub_cv, run_name=\"felipe_catboost_cv_only_bestparams\")\n",
    "print(\"Saved CV-only submission.\")\n",
    "\n",
    "sub_full = pd.DataFrame({id_col: test[id_col], target: full_seed_test_preds})\n",
    "save_submission(sub_full, run_name=\"felipe_catboost_full_seed_only_bestparams\")\n",
    "print(\"Saved full-seed-only submission.\")\n",
    "\n",
    "sub_alpha = pd.DataFrame({id_col: test[id_col], target: alpha_test_preds})\n",
    "save_submission(sub_alpha, run_name=f\"felipe_catboost_best_alpha_blend_{best_alpha:.4f}\")\n",
    "print(\"Saved best-alpha CV-vs-Full submission.\")\n",
    "\n",
    "sub_ridge = pd.DataFrame({id_col: test[id_col], target: ridge_test_preds})\n",
    "save_submission(sub_ridge, run_name=f\"felipe_catboost_ridge_stack_alpha_{best_ra:.6f}\")\n",
    "print(\"Saved ridge-stacked CV-vs-Full submission.\")\n",
    "\n",
    "# Snapshot-only submission\n",
    "sub_snapshot = pd.DataFrame({id_col: test[id_col], target: snapshot_full_test_preds})\n",
    "save_submission(sub_snapshot, run_name=\"felipe_catboost_snapshot_full_seed_bestparams\")\n",
    "print(\"Saved Snapshot full-seed-only submission.\")\n",
    "\n",
    "# Alpha CV vs Snapshot submission\n",
    "sub_alpha_snap = pd.DataFrame({id_col: test[id_col], target: alpha_snapshot_test_preds})\n",
    "save_submission(sub_alpha_snap, run_name=f\"felipe_catboost_alpha_blend_cv_snapshot_{best_alpha_snap:.4f}\")\n",
    "print(\"Saved best-alpha CV-vs-Snapshot submission.\")\n",
    "\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61693a8",
   "metadata": {},
   "source": [
    "#### As stated earlier, the model ended up taking up 4+ hours to train after we started it before the deadline, so we ended up stopping it after we saw it likely wasn't finishing anytime soon and deadline for Kaggle submission had passed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96473ecf",
   "metadata": {},
   "source": [
    "### 6.3 Cross-Validation Experiments and Robustness\n",
    "\n",
    "A major focus of our modeling approach was making sure our CV estimates were **reliable** and not overly optimistic.\n",
    "\n",
    "We experimented with multiple validation strategies:\n",
    "\n",
    "- **Standard KFold with shuffling**  \n",
    "  - Our default setup: 5-fold KFold with `shuffle=True` and a fixed `random_state`.  \n",
    "  - Simple and effective for quickly comparing model variants.\n",
    "\n",
    "- **GroupKFold by `Farm_ID` (sanity check)**  \n",
    "  - To test whether the model was accidentally memorizing per-farm patterns in a way that would not transfer to unseen farms, we ran experiments where entire farms were held out in validation.  \n",
    "  - Result: RMSE under GroupKFold was very similar to our standard KFold RMSE, suggesting that the model generalizes reasonably well across farms.\n",
    "\n",
    "- **Farm feature experiment (removing farm-derived features)**  \n",
    "  - When we noticed our Kaggle score was getting worse while our local CV score was getting better, we worried that some of our farm-level features might be leaking or behaving in a way that would not generalize. In particular, an earlier version of `Farm_Cluster` was clearly wrong: we fit KMeans on the entire train set and then fit a *separate* KMeans on the test set, so cluster labels did not match between train and test and CV was over-optimistic.  \n",
    "  - To be safe, we ran an experiment where we **removed all farm-derived features** from the model: target-encoded `Farm_Performance`, farm clusters, and farm-delta features like `Prev_vs_Farm` and `Prev_over_Farm`.  \n",
    "  - The resulting CatBoost model had **slightly worse RMSE** than our full farm-aware configuration, which suggests that once implemented correctly (fold-safe target encoding, consistent clustering, no test-time refits), these farm-level features provide genuine predictive signal rather than hidden leakage.  \n",
    "  - Together with the GroupKFold-by-farm results, this gave us confidence that using farm-aware features was appropriate and that the final model was not simply memorizing specific farms in a way that would fail on new data and stuck with it for our final model.\n",
    "\n",
    "- **Time-related considerations**  \n",
    "  - We created date-based features (e.g., month, week of year, day of week) and considered that the hidden test set might correspond to later calendar periods.  \n",
    "  - We monitored whether features like `year` or `date_ordinal` appeared to cause over-optimistic CV. In parallel, we compared CV scores with and without these trend-like features as a robustness check.\n",
    "\n",
    "Overall, these CV experiments increased our confidence that the improvements we saw during development were not purely due to leakage or overfitting to a specific fold configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98d0ad",
   "metadata": {},
   "source": [
    "### Attempt to improve RMSE by using our base CatBoost model + removing year + date_ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0134aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CatBoost-only\n",
    "#   - KEEP Farm_Cluster\n",
    "#   - REMOVE year + date_ordinal\n",
    "#   - Snapshot ensemble (seeds + small param variants)\n",
    "#   - Alpha blend between CV-only and Snapshot\n",
    "#   - No Ridge\n",
    "#   - USE ALL CPU CORES\n",
    "# ======================================================\n",
    "import os, sys, importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ALL_CORES = os.cpu_count() or 4\n",
    "print(f\"[INFO] Using ALL_CORES = {ALL_CORES}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(ALL_CORES)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(ALL_CORES)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(ALL_CORES)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(ALL_CORES)\n",
    "\n",
    "# submission utils\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import submission_utils\n",
    "importlib.reload(submission_utils)\n",
    "from submission_utils import save_submission\n",
    "\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "test  = pd.read_csv(\"../../data/cattle_data_test.csv\")\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "\n",
    "# CATBOOST preprocessing\n",
    "#      - kept Farm_Cluster block\n",
    "#      - removed year + date_ordinal\n",
    "def preprocess_pipeline_cb(df, encode_flag=True, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "\n",
    "    # target extraction + filtering\n",
    "    if target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0]\n",
    "        y = milk_features[target_col]\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    # Breed cleaning\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    # Housing score fill\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    # Feed quantity fill by type\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "\n",
    "    # numeric fills\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    milk_features[numeric_cols] = milk_features[numeric_cols].fillna(\n",
    "        milk_features[numeric_cols].median()\n",
    "    )\n",
    "\n",
    "    # Date features (SAFE seasonal only)\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"is_weekend\"] = milk_features[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "\n",
    "    # Farm_Cluster\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "\n",
    "        farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "        scaler = StandardScaler()\n",
    "        farm_scaled = scaler.fit_transform(farm_features)\n",
    "\n",
    "        # KMeans also uses OpenMP under the hood; env vars above help it use all cores.\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "\n",
    "        milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "\n",
    "    # drop IDs\n",
    "    milk_features = milk_features.drop(columns=[\"Cattle_ID\", \"Farm_ID\"], errors=\"ignore\")\n",
    "\n",
    "    if encode_flag:\n",
    "        cat_cols = milk_features.select_dtypes(include=\"object\").columns.tolist()\n",
    "        milk_features = pd.get_dummies(milk_features, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    # fallback if missing\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    return milk_features.copy(), y\n",
    "\n",
    "\n",
    "# Build base datasets\n",
    "X_cb, y_cb = preprocess_pipeline_cb(train, encode_flag=False, target_col=target)\n",
    "test_cb, _ = preprocess_pipeline_cb(test, encode_flag=False, target_col=None)\n",
    "cat_features = X_cb.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "\n",
    "# Best CatBoost params\n",
    "catboost_best = {\n",
    "    \"depth\": 6,\n",
    "    \"learning_rate\": 0.022872270426979868,\n",
    "    \"l2_leaf_reg\": 4.014881862534349,\n",
    "    \"subsample\": 0.847236734294946,\n",
    "    \"random_strength\": 0.731107887920747,\n",
    "    \"bagging_temperature\": 0.4635035306948895\n",
    "}\n",
    "\n",
    "SEED_LIST = [101, 202, 303, 404, 505]\n",
    "\n",
    "# snapshot variants\n",
    "PARAM_VARIANTS = [\n",
    "    dict(catboost_best),\n",
    "    dict(catboost_best, depth=5),\n",
    "    dict(catboost_best, depth=7),\n",
    "    dict(catboost_best, learning_rate=catboost_best[\"learning_rate\"] * 0.9),\n",
    "    dict(catboost_best, learning_rate=catboost_best[\"learning_rate\"] * 1.1),\n",
    "]\n",
    "\n",
    "\n",
    "# CV-only OOF + CV-fold test preds\n",
    "print(\"\\nRebuilding CatBoost CV-only OOF + test preds...\")\n",
    "cv_only_oof = np.zeros(len(X_cb))\n",
    "cv_models = []\n",
    "fold_best_iters = []\n",
    "fold_rmses = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr, y_val = y_cb.iloc[tr_idx], y_cb.iloc[val_idx]\n",
    "\n",
    "    cb = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=ALL_CORES,  # NEW\n",
    "        verbose=False,\n",
    "        use_best_model=True,\n",
    "        **catboost_best\n",
    "    )\n",
    "    cb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_iter = cb.get_best_iteration()\n",
    "    preds_val = cb.predict(X_val)\n",
    "    fold_rmse = rmse(y_val, preds_val)\n",
    "\n",
    "    print(f\"CB Fold {fold} RMSE: {fold_rmse:.6f}, best_iter={best_iter}\")\n",
    "\n",
    "    cv_only_oof[val_idx] = preds_val\n",
    "    cv_models.append(cb)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    fold_rmses.append(fold_rmse)\n",
    "\n",
    "cv_only_rmse = rmse(y_cb, cv_only_oof)\n",
    "avg_best_iter = int(np.mean(fold_best_iters))\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(f\"CV-only ensemble OOF RMSE: {cv_only_rmse:.6f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "print(\"Average best_iteration across folds:\", avg_best_iter)\n",
    "print(\"===============================\\n\")\n",
    "\n",
    "cv_test_preds = np.mean([m.predict(test_cb) for m in cv_models], axis=0)\n",
    "\n",
    "\n",
    "# Snapshot OOF ensemble (per fold)\n",
    "print(\"\\nBuilding leak-safe Snapshot OOF ensemble...\")\n",
    "snapshot_oof = np.zeros(len(X_cb), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr = y_cb.iloc[tr_idx]\n",
    "\n",
    "    fold_preds = np.zeros(len(val_idx), dtype=float)\n",
    "\n",
    "    for params in PARAM_VARIANTS:\n",
    "        for seed in SEED_LIST:\n",
    "            m = CatBoostRegressor(\n",
    "                loss_function=\"RMSE\",\n",
    "                n_estimators=avg_best_iter,\n",
    "                random_seed=seed,\n",
    "                thread_count=ALL_CORES,  # NEW\n",
    "                verbose=False,\n",
    "                **params\n",
    "            )\n",
    "            m.fit(X_tr, y_tr, cat_features=cat_features, verbose=False)\n",
    "            fold_preds += m.predict(X_val)\n",
    "\n",
    "    fold_preds /= (len(PARAM_VARIANTS) * len(SEED_LIST))\n",
    "    snapshot_oof[val_idx] = fold_preds\n",
    "\n",
    "snapshot_oof_rmse = rmse(y_cb, snapshot_oof)\n",
    "print(f\"Snapshot OOF RMSE: {snapshot_oof_rmse:.6f}\")\n",
    "\n",
    "\n",
    "# Full Snapshot ensemble on FULL data (main final preds)\n",
    "print(\"\\nTraining Snapshot Full-Seed Ensemble on full data...\")\n",
    "full_snapshot_test = np.zeros(len(test_cb), dtype=float)\n",
    "\n",
    "for params in PARAM_VARIANTS:\n",
    "    for seed in SEED_LIST:\n",
    "        m = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=avg_best_iter,\n",
    "            random_seed=seed,\n",
    "            thread_count=ALL_CORES,\n",
    "            verbose=False,\n",
    "            **params\n",
    "        )\n",
    "        m.fit(X_cb, y_cb, cat_features=cat_features, verbose=False)\n",
    "        full_snapshot_test += m.predict(test_cb)\n",
    "\n",
    "full_snapshot_test /= (len(PARAM_VARIANTS) * len(SEED_LIST))\n",
    "print(\"Full snapshot ensemble ready.\")\n",
    "\n",
    "\n",
    "# Alpha search (CV-only OOF vs Snapshot OOF)\n",
    "print(\"\\nAlpha search between CV-only and Snapshot OOF...\")\n",
    "alphas = np.linspace(0, 1, 401)  # 0.0025 steps\n",
    "best_alpha, best_alpha_rmse = None, float(\"inf\")\n",
    "\n",
    "for a in alphas:\n",
    "    blend_oof = a * cv_only_oof + (1 - a) * snapshot_oof\n",
    "    r = rmse(y_cb, blend_oof)\n",
    "    if r < best_alpha_rmse:\n",
    "        best_alpha_rmse = r\n",
    "        best_alpha = a\n",
    "\n",
    "print(f\"Best alpha (CV weight) = {best_alpha:.4f}\")\n",
    "print(f\"Best alpha-blended OOF RMSE = {best_alpha_rmse:.6f}\")\n",
    "\n",
    "alpha_blend_test = best_alpha * cv_test_preds + (1 - best_alpha) * full_snapshot_test\n",
    "\n",
    "# Save submissions\n",
    "sub_cv = pd.DataFrame({id_col: test[id_col], target: cv_test_preds})\n",
    "save_submission(sub_cv, run_name=\"felipe_cb_cv_only_keep_cluster_no_year_ordinal\")\n",
    "print(\"Saved CV-only submission.\")\n",
    "\n",
    "sub_full = pd.DataFrame({id_col: test[id_col], target: full_snapshot_test})\n",
    "save_submission(sub_full, run_name=\"felipe_cb_full_snapshot_keep_cluster_no_year_ordinal\")\n",
    "print(\"Saved FULL snapshot submission (main final).\")\n",
    "\n",
    "sub_alpha = pd.DataFrame({id_col: test[id_col], target: alpha_blend_test})\n",
    "save_submission(sub_alpha, run_name=f\"felipe_cb_alpha_blend_keep_cluster_{best_alpha:.4f}\")\n",
    "print(\"Saved alpha blend submission.\")\n",
    "\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740bd60",
   "metadata": {},
   "source": [
    "[INFO] Using ALL_CORES = 10\n",
    "\n",
    "Rebuilding CatBoost CV-only OOF + test preds...\n",
    "\n",
    "CB Fold 1 RMSE: 4.107923, best_iter=843\n",
    "\n",
    "CB Fold 2 RMSE: 4.100420, best_iter=1229\n",
    "\n",
    "CB Fold 3 RMSE: 4.120831, best_iter=1173\n",
    "\n",
    "CB Fold 4 RMSE: 4.106234, best_iter=1040\n",
    "\n",
    "CB Fold 5 RMSE: 4.098906, best_iter=1184\n",
    "\n",
    "\n",
    "CV-only ensemble OOF RMSE: 4.106870\n",
    "\n",
    "Fold RMSEs: [4.10792264999412, 4.10042018812222, 4.120830507526178, 4.106233582481572, 4.098906278206792]\n",
    "\n",
    "Best iters: [843, 1229, 1173, 1040, 1184]\n",
    "\n",
    "Average best_iteration across folds: 1093\n",
    "\n",
    "\n",
    "\n",
    "Building leak-safe Snapshot OOF ensemble...\n",
    "\n",
    "Snapshot OOF RMSE: 4.106453\n",
    "\n",
    "Training Snapshot Full-Seed Ensemble on full data...\n",
    "\n",
    "Full snapshot ensemble ready.\n",
    "\n",
    "Alpha search between CV-only and Snapshot OOF...\n",
    "\n",
    "Best alpha (CV weight) = 0.2100\n",
    "\n",
    "Best alpha-blended OOF RMSE = 4.106420\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_cb_cv_only_keep_cluster_no_year_ordinal__2025-11-23__22-50-12.csv\n",
    "\n",
    "Saved CV-only submission.\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_cb_full_snapshot_keep_cluster_no_year_ordinal__2025-11-23__22-50-12.csv\n",
    "Saved FULL snapshot submission (main final).\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_cb_alpha_blend_keep_cluster_0.2100__2025-11-23__22-50-12.csv\n",
    "Saved alpha blend submission.\n",
    "\n",
    "DONE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243ba99",
   "metadata": {},
   "source": [
    "### 6.4 Final Model and Reflection\n",
    "\n",
    "Our final submission is based on a **CatBoost-only ensemble**, with:\n",
    "\n",
    "- Carefully tuned hyperparameters obtained via Optuna.\n",
    "- Multiple random seeds to create a diverse ensemble.\n",
    "- An alpha-blending step to optimally combine:\n",
    "  - The CV-trained CatBoost model, and  \n",
    "  - The multi-seed ensemble.\n",
    "\n",
    "On our internal 5-fold CV, this final configuration achieved a best RMSE of approximately **4.106145**, and translated into 4th place out of 52 total teams on the course Kaggle leaderboard.\n",
    "\n",
    "**What helped the most:**\n",
    "\n",
    "- Moving from simple baselines to tuned CatBoost gave the largest improvement.\n",
    "- Thoughtful feature engineering (especially around date, feed, and farm-level behavior) provided strong, interpretable signals.\n",
    "- Multi-seed and snapshot ensembling added a smaller but reliable boost and made predictions more stable.\n",
    "\n",
    "**What helped less or was risky:**\n",
    "\n",
    "- Very aggressive stacking and meta-models sometimes improved CV by a tiny margin but did not always translate into better leaderboard performance.\n",
    "- Certain features and encodings (e.g., farm clustering variations, very trend-heavy date features) required careful validation and could lead to overly optimistic CV if not tested properly.\n",
    "\n",
    "Overall, our modeling process was iterative and experimental: we started from simple models, gradually introduced more sophisticated techniques, and continuously used cross-validation and leaderboard feedback to decide which ideas were genuinely helpful and which were mainly adding complexity. This approach allowed us to build a final model that is both **accurate** and **robust**, while also learning a lot about practical model development in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d1ed1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad317fbe",
   "metadata": {},
   "source": [
    "# Final model we stuck with for Kaggle submission (alpha blend 0.4400 Version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# submission utils\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import submission_utils\n",
    "importlib.reload(submission_utils)\n",
    "from submission_utils import save_submission\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"../../data/cattle_data_train.csv\")\n",
    "test  = pd.read_csv(\"../../data/cattle_data_test.csv\")\n",
    "\n",
    "target = \"Milk_Yield_L\"\n",
    "id_col = \"Cattle_ID\"\n",
    "\n",
    "# CATBOOST preprocessing\n",
    "def preprocess_pipeline_cb(df, encode_flag=True, target_col=\"Milk_Yield_L\", n_clusters=10):\n",
    "    milk_features = df.copy()\n",
    "\n",
    "    if target_col in milk_features.columns:\n",
    "        milk_features = milk_features[milk_features[target_col] >= 0]\n",
    "        y = milk_features[target_col]\n",
    "        milk_features = milk_features.drop(columns=[target_col])\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    if \"Breed\" in milk_features.columns:\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].str.strip()\n",
    "        milk_features[\"Breed\"] = milk_features[\"Breed\"].replace({\"Holstien\": \"Holstein\"})\n",
    "\n",
    "    if \"Housing_Score\" in milk_features.columns:\n",
    "        milk_features[\"Housing_Score\"] = milk_features[\"Housing_Score\"].fillna(\n",
    "            milk_features[\"Housing_Score\"].median()\n",
    "        )\n",
    "\n",
    "    if \"Feed_Quantity_kg\" in milk_features.columns and \"Feed_Type\" in milk_features.columns:\n",
    "        milk_features[\"Feed_Quantity_kg\"] = milk_features.groupby(\"Feed_Type\")[\"Feed_Quantity_kg\"].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "\n",
    "    numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "    milk_features[numeric_cols] = milk_features[numeric_cols].fillna(milk_features[numeric_cols].median())\n",
    "\n",
    "    if \"Date\" in milk_features.columns:\n",
    "        milk_features[\"Date\"] = pd.to_datetime(milk_features[\"Date\"])\n",
    "        milk_features[\"year\"] = milk_features[\"Date\"].dt.year\n",
    "        milk_features[\"month\"] = milk_features[\"Date\"].dt.month\n",
    "        milk_features[\"day\"] = milk_features[\"Date\"].dt.day\n",
    "        milk_features[\"dayofweek\"] = milk_features[\"Date\"].dt.dayofweek\n",
    "        milk_features[\"weekofyear\"] = milk_features[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "        milk_features[\"quarter\"] = milk_features[\"Date\"].dt.quarter\n",
    "        milk_features[\"is_weekend\"] = milk_features[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "        milk_features[\"date_ordinal\"] = milk_features[\"Date\"].map(pd.Timestamp.toordinal)\n",
    "        milk_features = milk_features.drop(columns=[\"Date\"])\n",
    "\n",
    "    if \"Farm_ID\" in milk_features.columns:\n",
    "        farm_numeric_cols = milk_features.select_dtypes(include=\"number\").columns.tolist()\n",
    "        farm_numeric_cols = [c for c in farm_numeric_cols if c not in [\"Cattle_ID\"]]\n",
    "\n",
    "        farm_features = milk_features.groupby(\"Farm_ID\")[farm_numeric_cols].mean()\n",
    "        scaler = StandardScaler()\n",
    "        farm_scaled = scaler.fit_transform(farm_features)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        farm_features[\"Cluster\"] = kmeans.fit_predict(farm_scaled)\n",
    "        milk_features[\"Farm_Cluster\"] = milk_features[\"Farm_ID\"].map(farm_features[\"Cluster\"])\n",
    "\n",
    "    milk_features = milk_features.drop(columns=[\"Cattle_ID\", \"Farm_ID\"], errors=\"ignore\")\n",
    "\n",
    "    if encode_flag:\n",
    "        cat_cols = milk_features.select_dtypes(include=\"object\").columns.tolist()\n",
    "        milk_features = pd.get_dummies(milk_features, columns=cat_cols, drop_first=False)\n",
    "\n",
    "    if \"Farm_Cluster\" not in milk_features.columns:\n",
    "        milk_features[\"Farm_Cluster\"] = 0\n",
    "\n",
    "    return milk_features.copy(), y\n",
    "\n",
    "\n",
    "# Build base datasets (CatBoost only)\n",
    "X_cb, y_cb = preprocess_pipeline_cb(train, encode_flag=False, target_col=target)\n",
    "test_cb, _ = preprocess_pipeline_cb(test, encode_flag=False, target_col=None)\n",
    "cat_features = X_cb.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "\n",
    "# Best params\n",
    "catboost_best = {\n",
    "    \"depth\": 6,\n",
    "    \"learning_rate\": 0.022872270426979868,\n",
    "    \"l2_leaf_reg\": 4.014881862534349,\n",
    "    \"subsample\": 0.847236734294946,\n",
    "    \"random_strength\": 0.731107887920747,\n",
    "    \"bagging_temperature\": 0.4635035306948895\n",
    "}\n",
    "\n",
    "SEED_LIST = [101, 202, 303, 404, 505]\n",
    "\n",
    "\n",
    "# CV-only OOF + test preds\n",
    "print(\"\\nRebuilding CatBoost CV-only OOF + test preds...\")\n",
    "cv_only_oof = np.zeros(len(X_cb))\n",
    "cv_models = []\n",
    "fold_best_iters = []\n",
    "fold_rmses = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr, y_val = y_cb.iloc[tr_idx], y_cb.iloc[val_idx]\n",
    "\n",
    "    cb = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=42,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        use_best_model=True,\n",
    "        **catboost_best\n",
    "    )\n",
    "    cb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_iter = cb.get_best_iteration()\n",
    "    preds_val = cb.predict(X_val)\n",
    "    fold_rmse = rmse(y_val, preds_val)\n",
    "\n",
    "    print(f\"CB Fold {fold} RMSE: {fold_rmse:.6f}, best_iter={best_iter}\")\n",
    "\n",
    "    cv_only_oof[val_idx] = preds_val\n",
    "    cv_models.append(cb)\n",
    "    fold_best_iters.append(best_iter)\n",
    "    fold_rmses.append(fold_rmse)\n",
    "\n",
    "cv_only_rmse = rmse(y_cb, cv_only_oof)\n",
    "avg_best_iter = int(np.mean(fold_best_iters))\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(f\"CV-only ensemble OOF RMSE: {cv_only_rmse:.6f}\")\n",
    "print(\"Fold RMSEs:\", fold_rmses)\n",
    "print(\"Best iters:\", fold_best_iters)\n",
    "print(\"Average best_iteration across folds:\", avg_best_iter)\n",
    "print(\"===============================\\n\")\n",
    "\n",
    "# CV-only test preds\n",
    "cv_test_preds = np.mean([m.predict(test_cb) for m in cv_models], axis=0)\n",
    "\n",
    "\n",
    "# Full-style OOF + full-seed test preds\n",
    "print(\"\\nRebuilding Full-style OOF + full-seed test preds...\")\n",
    "full_style_oof = np.zeros(len(X_cb))\n",
    "full_seed_test_preds = np.zeros(len(test_cb))\n",
    "\n",
    "# Full-style OOF: per fold, train multi-seed on fold-train only\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_cb), 1):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx], X_cb.iloc[val_idx]\n",
    "    y_tr = y_cb.iloc[tr_idx]\n",
    "\n",
    "    fold_full_preds = np.zeros(len(val_idx), dtype=float)\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        m = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            n_estimators=avg_best_iter,\n",
    "            random_seed=seed,\n",
    "            thread_count=4,\n",
    "            verbose=False,\n",
    "            **catboost_best\n",
    "        )\n",
    "        m.fit(X_tr, y_tr, cat_features=cat_features, verbose=False)\n",
    "        fold_full_preds += m.predict(X_val)\n",
    "\n",
    "    fold_full_preds /= len(SEED_LIST)\n",
    "    full_style_oof[val_idx] = fold_full_preds\n",
    "\n",
    "full_style_rmse = rmse(y_cb, full_style_oof)\n",
    "print(f\"Full-style ensemble OOF RMSE: {full_style_rmse:.6f}\")\n",
    "\n",
    "# Full-seed test preds: multi-seed on full data\n",
    "for seed in SEED_LIST:\n",
    "    m = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        n_estimators=avg_best_iter,\n",
    "        random_seed=seed,\n",
    "        thread_count=4,\n",
    "        verbose=False,\n",
    "        **catboost_best\n",
    "    )\n",
    "    m.fit(X_cb, y_cb, cat_features=cat_features, verbose=False)\n",
    "    full_seed_test_preds += m.predict(test_cb)\n",
    "\n",
    "full_seed_test_preds /= len(SEED_LIST)\n",
    "\n",
    "\n",
    "# Best alpha blend search (CV-only vs Full-style)\n",
    "print(\"\\nAlpha search between CV-only and Full-style...\")\n",
    "alphas = np.linspace(0, 1, 201)\n",
    "best_alpha, best_alpha_rmse = None, float(\"inf\")\n",
    "\n",
    "for a in alphas:\n",
    "    blend_oof = a * cv_only_oof + (1 - a) * full_style_oof\n",
    "    r = rmse(y_cb, blend_oof)\n",
    "    if r < best_alpha_rmse:\n",
    "        best_alpha_rmse = r\n",
    "        best_alpha = a\n",
    "\n",
    "print(\"\\n=== Best alpha blend (CV vs Full) ===\")\n",
    "print(f\"Best alpha (CV weight): {best_alpha:.4f}\")\n",
    "print(f\"Best blended OOF RMSE : {best_alpha_rmse:.6f}\")\n",
    "\n",
    "alpha_test_preds = best_alpha * cv_test_preds + (1 - best_alpha) * full_seed_test_preds\n",
    "\n",
    "\n",
    "# Ridge meta-stacking (CV-only vs Full-style)\n",
    "print(\"\\nRidge meta-stacking on (CV-only, Full-style)...\")\n",
    "meta_X = np.vstack([cv_only_oof, full_style_oof]).T\n",
    "meta_test = np.vstack([cv_test_preds, full_seed_test_preds]).T\n",
    "\n",
    "ridge_alphas = np.logspace(-4, 2, 60)\n",
    "best_ra, best_rr, best_ridge = None, float(\"inf\"), None\n",
    "\n",
    "for ra in ridge_alphas:\n",
    "    ridge = Ridge(alpha=ra, fit_intercept=True, random_state=42)\n",
    "    ridge.fit(meta_X, y_cb)\n",
    "    preds_oof = ridge.predict(meta_X)\n",
    "    r = rmse(y_cb, preds_oof)\n",
    "    if r < best_rr:\n",
    "        best_rr, best_ra, best_ridge = r, ra, ridge\n",
    "\n",
    "print(\"\\n=== Best Ridge stack (CV vs Full) ===\")\n",
    "print(f\"Best Ridge alpha: {best_ra:.6f}\")\n",
    "print(f\"Best Ridge-stacked OOF RMSE: {best_rr:.6f}\")\n",
    "print(\"Ridge weights (CV, Full):\", best_ridge.coef_, \"intercept:\", best_ridge.intercept_)\n",
    "\n",
    "ridge_test_preds = best_ridge.predict(meta_test)\n",
    "\n",
    "\n",
    "# Save submissions\n",
    "sub_cv = pd.DataFrame({id_col: test[id_col], target: cv_test_preds})\n",
    "save_submission(sub_cv, run_name=\"felipe_catboost_cv_only_bestparams\")\n",
    "print(\"Saved CV-only submission.\")\n",
    "\n",
    "sub_full = pd.DataFrame({id_col: test[id_col], target: full_seed_test_preds})\n",
    "save_submission(sub_full, run_name=\"felipe_catboost_full_seed_only_bestparams\")\n",
    "print(\"Saved full-seed-only submission.\")\n",
    "\n",
    "sub_alpha = pd.DataFrame({id_col: test[id_col], target: alpha_test_preds})\n",
    "save_submission(sub_alpha, run_name=f\"felipe_catboost_best_alpha_blend_{best_alpha:.4f}\")\n",
    "print(\"Saved best-alpha blend submission.\")\n",
    "\n",
    "sub_ridge = pd.DataFrame({id_col: test[id_col], target: ridge_test_preds})\n",
    "save_submission(sub_ridge, run_name=f\"felipe_catboost_ridge_stack_alpha_{best_ra:.6f}\")\n",
    "print(\"Saved ridge-stacked submission.\")\n",
    "\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c198850",
   "metadata": {},
   "source": [
    "\n",
    "Rebuilding CatBoost CV-only OOF + test preds...\n",
    "\n",
    "CB Fold 1 RMSE: 4.106957, best_iter=1157\n",
    "\n",
    "CB Fold 2 RMSE: 4.100151, best_iter=1127\n",
    "\n",
    "CB Fold 3 RMSE: 4.120099, best_iter=1093\n",
    "\n",
    "CB Fold 4 RMSE: 4.106328, best_iter=1094\n",
    "\n",
    "CB Fold 5 RMSE: 4.098421, best_iter=999\n",
    "\n",
    "\n",
    "CV-only ensemble OOF RMSE: 4.106398\n",
    "\n",
    "Fold RMSEs: [4.106956550228758, 4.100151472353626, 4.120099489769896, 4.106327628323443, 4.0984207308223395]\n",
    "\n",
    "Best iters: [1157, 1127, 1093, 1094, 999]\n",
    "\n",
    "Average best_iteration across folds: 1094\n",
    "\n",
    "\n",
    "\n",
    "Rebuilding Full-style OOF + full-seed test preds...\n",
    "\n",
    "Full-style ensemble OOF RMSE: 4.106304\n",
    "\n",
    "Alpha search between CV-only and Full-style...\n",
    "\n",
    "=== Best alpha blend (CV vs Full) ===\n",
    "\n",
    "Best alpha (CV weight): 0.4400\n",
    "\n",
    "Best blended OOF RMSE : 4.106145\n",
    "\n",
    "Ridge meta-stacking on (CV-only, Full-style)...\n",
    "\n",
    "=== Best Ridge stack (CV vs Full) ===\n",
    "\n",
    "Best Ridge alpha: 0.000100\n",
    "\n",
    "Best Ridge-stacked OOF RMSE: 4.105931\n",
    "\n",
    "Ridge weights (CV, Full): [0.44389453 0.56851011] intercept: -0.1922872770191031\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_cv_only_bestparams__2025-11-23__07-28-02.csv\n",
    "\n",
    "Saved CV-only submission.\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_full_seed_only_bestparams__2025-11-23__07-28-02.csv\n",
    "\n",
    "Saved full-seed-only submission.\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_best_alpha_blend_0.4400__2025-11-23__07-28-02.csv\n",
    "\n",
    "Saved best-alpha blend submission.\n",
    "\n",
    "Saved submission -> /Users/felipebenitez/ML-Project/submissions/felipe_catboost_ridge_stack_alpha_0.000100__2025-11-23__07-28-02.csv\n",
    "\n",
    "Saved ridge-stacked submission.\n",
    "\n",
    "DONE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
