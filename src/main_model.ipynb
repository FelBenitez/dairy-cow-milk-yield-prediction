{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce5db82",
   "metadata": {},
   "source": [
    "# Predicting Daily Milk Yield - Fall 2025 ML Course Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Felipe Benitez, feb478\n",
    "# Edwin Torres, ert863\n",
    "# Gora Bepary, gcb883\n",
    "# Sankarsh Narayanan, EID\n",
    "\n",
    "'''\n",
    "Make cells to explain what dataset is\n",
    "Objective\n",
    "Tools used\n",
    "Summarizing final results, such like \"Best local CV RMSE... Best Kaggle RMSE... Kaggle LB Score..., etc.\" Things to orient reader.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430ff86",
   "metadata": {},
   "source": [
    "# Data Loading & Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Goal: show we know what we're working with before touching models.\n",
    "So show train/test. Other things we could show as example is train and test shape, train head, info, describe, # of rows/columns, target column, presence of categorical columns, missing values, just to name a few. Show whatever is important and useful. This will contribute to Data Exploration + Quality & Clarity.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac009a5e",
   "metadata": {},
   "source": [
    "# Data Cleaning Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3c347",
   "metadata": {},
   "source": [
    "## 3.1 Summary of what worked\n",
    "\n",
    "- **Removed invalid target values**  \n",
    "  - Filtered out rows where `Milk_Yield_L < 0` for both CatBoost and XGBoost pipelines.\n",
    "\n",
    "- **Standardized categorical text**  \n",
    "  - Trimmed whitespace from `Breed` and corrected inconsistent spelling (`Holstien → Holstein`).\n",
    "\n",
    "- **Imputed missing values**\n",
    "  - `Housing_Score`: replaced missing values using the column median.\n",
    "  - `Feed_Quantity_kg`: imputed per‐`Feed_Type` median to preserve context.\n",
    "  - All remaining numeric fields: filled missing entries using column medians.\n",
    "  - Median is safe to extremes in general.\n",
    "\n",
    "- **Temporal feature extraction from `Date`**  \n",
    "  - Parsed as datetime and derived features: `month`, `day`, `dayofweek`, `weekofyear`, `quarter`, and `is_weekend`; then removed `Date`.\n",
    "\n",
    "- **Categorical feature handling**\n",
    "  - CatBoost: preserved original categorical columns and passed them as native categorical features.\n",
    "  - XGBoost: applied one-hot encoding after other feature engineering steps.\n",
    "\n",
    "- **Farm identifier treatment**\n",
    "  - Removed `Farm_ID` in CatBoost pipeline.\n",
    "  - In XGBoost pipeline, applied fold-safe target encoding to convert `Farm_ID` into a numeric performance metric.\n",
    "\n",
    "- **Outlier columns and IDs**\n",
    "  - Dropped columns that uniquely identify samples (`Cattle_ID`) in both pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3af10b",
   "metadata": {},
   "source": [
    "## 3.2 Summary of what did not work. \n",
    "- **Handeling Negative Values**\n",
    "    - Taking an absolute value of those missing values made performance worse.\n",
    "    - Since 55% of entries were negative, we treated this as a systemic issue rather than random noise. Positive values (mean 5.74, max 31.2) and negative values (mean −4.22, min −8.8) appeared to represent different behaviors, so taking the absolute value mixed distinct patterns. We attempted to separate them into different features, but this split did not improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32770a7c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913258dc",
   "metadata": {},
   "source": [
    "# Target Distribution 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6959d",
   "metadata": {},
   "source": [
    "# Relationships with Key Features (or things we tried idk) 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd260ac",
   "metadata": {},
   "source": [
    "# Farm-Level Differences 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since we did farm clustering, show why and how, and also how it ended up being wrong in some way. Just talk about why we did this for example\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4840f",
   "metadata": {},
   "source": [
    "# Feature Engineering 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da875272",
   "metadata": {},
   "source": [
    "### 5.1 Overview\n",
    "\n",
    "We didn’t just feed the raw CSVs directly into our models. We iteratively engineered features, tested them with cross validation, and only kept transformations that were neutral or helpful for RMSE. Most experiments were guided by dairy domain reasoning, short and focused code changes, and immediate validation through average CV RMSE. If a feature block added noise or made the models worse, we removed it from the final pipeline.\n",
    "\n",
    "A major early issue involved the **date column**. The raw file stored the date as an **object** rather than a true `datetime`. Because our preprocessing step converted all object columns into categoricals, the date was being treated as thousands of unrelated categories instead of a temporal variable. This caused instability and strange model behavior. Converting the date into a proper `datetime` type and extracting structured components — such as **year**, **month**, **day**, and **day of week** — immediately fixed this instability and made the models behave far more consistently. This correction became an important foundation for every later feature block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee4b58",
   "metadata": {},
   "source": [
    "### 5.2 Core CatBoost Feature Engineering\n",
    "\n",
    "For CatBoost, we built features incrementally, keeping only the pieces that consistently helped or at least did not hurt CV RMSE.\n",
    "\n",
    "**Kept features and steps included:**\n",
    "- Biologically meaningful ratios:\n",
    "  - `Feed_per_kg_bw`\n",
    "  - Temperature-Humidity Index (THI) for heat stress  \n",
    "  - `Walk_per_graze` (distance walked per grazing hour)\n",
    "- Explicitly labeling categorical columns so CatBoost uses its native encoding properly.\n",
    "- Dropping 74 rows with **negative** `Milk_Yield_L` labels, which are physically impossible and were hurting training.\n",
    "- Lactation curve features such as:\n",
    "  - `is_peak_lactation`, `is_early_lactation`, `is_late_lactation`\n",
    "  - `dim_squared`, `dim_cubed`, `dim_log`\n",
    "  - `dim_parity`\n",
    "\n",
    "These changes moved CatBoost from about **4.114 RMSE → ~4.108**, with dropping negative labels being the biggest single improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e858824",
   "metadata": {},
   "source": [
    "### 5.3 CatBoost Ideas We Tested But Did Not Keep\n",
    "\n",
    "We experimented with several larger interaction blocks that ended up hurting RMSE:\n",
    "\n",
    "- Additional efficiency ratios (`water_per_weight`, `age_parity_ratio`, `age_parity_product`, etc.)\n",
    "- Activity combinations (`total_activity`, `rest_activity_ratio`)\n",
    "- More complicated temperature-humidity interactions\n",
    "- Bundled vaccine indicators or sums\n",
    "\n",
    "Most of these made RMSE worse or added noise, so we removed them and kept only the simpler, more stable features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0326d",
   "metadata": {},
   "source": [
    "### 5.4 Rumination and Farm-Level Statistics\n",
    "\n",
    "We explored multiple strategies to handle the unusual rumination values and farm-level context:\n",
    "\n",
    "- Making all rumination values positive made the model worse.\n",
    "- Splitting rumination into “positive mode” and “negative mode” plus flags was more logical but still not better in practice.\n",
    "- Treating negative rumination values as **missing (NaN)** was neutral and aligned with the idea of faulty sensors.\n",
    "- Farm-level stats — farm mean and standard deviation for predictors like:\n",
    "  - `Weight_kg`, `Feed_Quantity_kg`, `Water_Intake_L`, `Age_Months`\n",
    "  - `Days_in_Milk`, `Ambient_Temperature_C`\n",
    "\n",
    "These features describe farm environment without leaking targets.\n",
    "\n",
    "In the final pipeline, we used a simpler rumination approach and a **lighter** set of farm statistics to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc2d5c",
   "metadata": {},
   "source": [
    "### 5.5 Peer-Relative Features and Clustering\n",
    "\n",
    "We explored two types of contextual features:\n",
    "\n",
    "#### 1. Peer-relative features\n",
    "For each farm, we compared each cow to its farm peers via:\n",
    "- **Difference** features (e.g., `Weight_kg_vs_farm_diff`)\n",
    "- **Ratio** features (e.g., `Weight_kg_vs_farm_ratio`)\n",
    "\n",
    "These were meant to capture whether a cow is above or below typical farm-level baselines.\n",
    "\n",
    "#### 2. Farm clustering\n",
    "We tried clustering farms with KMeans and assigning each farm a cluster label.\n",
    "However, since train and test were clustered separately, the IDs did not align, which created anti-signal.  \n",
    "Once we realized this, we removed `Farm_Cluster`.\n",
    "\n",
    "Peer-relative features were conceptually strong but did not outperform simpler normalized farm stats, so we relied mainly on the latter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630eb21d",
   "metadata": {},
   "source": [
    "### 5.6 XGBoost Feature Engineering\n",
    "\n",
    "We also maintained an XGBoost pipeline to complement CatBoost and use as a second model family.  \n",
    "XGBoost was very sensitive to early feature blocks, especially before we correctly processed the **date column**, which had been treated as a high-cardinality categorical. Once we converted the date to `datetime` and stabilized the feature set, **we returned to XGBoost**, and it became much more consistent.\n",
    "\n",
    "We engineered and tested:\n",
    "\n",
    "- Age and parity transforms (`Age_Years`, `Age_Years2`, `Parity2`, `Age_x_Parity`)\n",
    "- Nonlinear `Days_in_Milk` transforms (`DIM_log`)\n",
    "- Efficiency ratios (`Feed_per_kgBW`, `Water_per_kgBW`, `PrevYield_per_Feed`)\n",
    "- THI and simple vaccine summaries (`Vax_Sum`)\n",
    "- Farm-delta and cohort-relative features\n",
    "- A large biological block of lactation and health indicators\n",
    "\n",
    "The most consistently helpful additions were:\n",
    "- Simple **parity categories**\n",
    "- **Farm-normalized predictors**\n",
    "- A **Vax_Sum + farm-delta** combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78f4d1",
   "metadata": {},
   "source": [
    "### 5.7 Encodings and Dimensionality Reduction\n",
    "\n",
    "Instead of PCA, we used targeted supervised encodings and structured normalization:\n",
    "\n",
    "**Tried:**\n",
    "- Farm-normalized predictors (subtracting or standardizing by farm averages)\n",
    "- K-Fold target encoding with out-of-fold means\n",
    "- Frequency encoding for large categoricals such as `Breed`\n",
    "\n",
    "Target encoding usually hurt XGBoost and didn’t help CatBoost, so we removed it.  \n",
    "Farm-normalized predictors and simple parity categories were the most reliable encodings.\n",
    "\n",
    "We considered PCA, but tree models handle moderate dimensionality well and PCA reduces interpretability.  \n",
    "Targeted feature selection and selective dropping worked better for our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e09f85",
   "metadata": {},
   "source": [
    "### 5.8 Summary\n",
    "\n",
    "Across all iterations, the most important feature engineering wins were:\n",
    "\n",
    "- **Fixing the date column** by converting it from object → datetime  \n",
    "- **Dropping physically impossible negative labels**\n",
    "- **Meaningful biological ratios and farm-normalized predictors**\n",
    "- **Lightweight farm statistics**\n",
    "- **Avoiding oversized interaction blocks**\n",
    "- **Revisiting XGBoost only after stabilizing the feature space**\n",
    "\n",
    "These steps led to a stable, interpretable, and high-performing feature set that consistently improved CatBoost and allowed XGBoost to be evaluated fairly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f519a",
   "metadata": {},
   "source": [
    "### 5.9 Model Feedback During Feature Engineering\n",
    "\n",
    "Our modeling work during feature engineering focused on using CatBoost, LightGBM, and XGBoost as feedback tools rather than finalized models. We applied the same preprocessing pipeline and cross validation splits to each model to understand how different engineered features affected stability, signal strength, and overall RMSE.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- CatBoost consistently responded the best to our engineered features, making it a reliable indicator of whether a new feature block was helpful or harmful.\n",
    "- LightGBM performed reasonably but tended to lose performance once the feature space became more complex.\n",
    "- XGBoost struggled early on—especially before fixing the date column—but improved significantly after the feature pipeline was stabilized and we revisited it near the end.\n",
    "\n",
    "We also monitored feature interactions through multiple CatBoost runs, tracking how RMSE changed as new biological ratios, farm-normalized predictors, and date-derived components were added. This allowed us to keep only the transformations that consistently improved model behavior.\n",
    "\n",
    "Overall, this model feedback loop was essential during feature engineering. It helped confirm which features were robust across different frameworks and guided the final feature set before moving on to full modeling and tuned ensembles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639ba8f",
   "metadata": {},
   "source": [
    "# Baseline Model (Modeling Approach) 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143db51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we show the baseline model we used w/ default ish paramaters. Show it, train it, run it, no optuna, show fold RMSEs, Average CV RMSEs, and explain what it's doing for us and anything else we can add to make this section full\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab589c7",
   "metadata": {},
   "source": [
    "## 6. Modeling Approach & Experiments\n",
    "In this section we describe our full modeling process starting from our base model, from how we tuned and compared, the different experiments, and how we eventually arrived and stuck with our final CatBoost ensemble with blending. The goal was not just to get a good leaderboard score but also explore model decisions and what needed to take place for things to improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be3d5f",
   "metadata": {},
   "source": [
    "### 6.1 Hyperparameter Tuning with Optuna\n",
    "\n",
    "After we selected CatBoost as our main model and saw that additional feature engineering was giving only small gains compared to the numerous experiments we tried with features, we focused on **hyperparameter tuning** to squeeze out as much performance as possible from our single best strongest learner.\n",
    "\n",
    "We actually ran **two separate Optuna studies** for CatBoost:\n",
    "\n",
    "- **Run 1 – 40 trials (our best hyperparameters)**\n",
    "- **Run 2 – 80 trials with an expanded search space**\n",
    "\n",
    "Both runs used the same 5-fold CV split and the same preprocessing pipeline, so their RMSEs are directly comparable.\n",
    "\n",
    "\n",
    "#### 6.1.1 First Optuna Run (40 trials)\n",
    "\n",
    "In the first study, we tuned the “core” CatBoost hyperparameters:\n",
    "\n",
    "- `depth` (5–7)\n",
    "- `learning_rate` (0.02–0.04)\n",
    "- `l2_leaf_reg` (L2 regularization)\n",
    "- `subsample` (row subsampling)\n",
    "- `random_strength` (randomness in split selection)\n",
    "- `bagging_temperature` (controls how aggressive the sampling is)\n",
    "- `n_estimators` was capped at 3000, with early stopping on each fold\n",
    "\n",
    "For each trial, the objective function trained on 4 folds and validated on the 5th, and we minimized the **mean 5-fold RMSE**.\n",
    "\n",
    "- **Best CV RMSE (Run 1):** ≈ **4.10639**\n",
    "- **Best hyperparameters (Run 1):**  \n",
    "  - `depth = 6`  \n",
    "  - `learning_rate ≈ 0.0229`  \n",
    "  - `l2_leaf_reg ≈ 4.01`  \n",
    "  - `subsample ≈ 0.847`  \n",
    "  - `random_strength ≈ 0.73`  \n",
    "  - `bagging_temperature ≈ 0.46`\n",
    "\n",
    "When we retrained a 5-fold CV ensemble with these parameters, we got:\n",
    "\n",
    "- **Final 5-fold CV RMSE:** ≈ **4.1064**  \n",
    "- **Best iterations per fold:** around **1,000–1,150 trees**, with an average of ~**1,094** boosting rounds\n",
    "\n",
    "This first Optuna run gave us the **strongest configuration** we found and became the base for our ensembling and blending experiments.\n",
    "\n",
    "\n",
    "#### 6.1.2 Second Optuna Run (80 trials, expanded space)\n",
    "\n",
    "Later, we ran a **second Optuna study with 80 trials**, this time expanding the search space to include more tree-shape and regularization parameters:\n",
    "\n",
    "- New hyperparameters included:\n",
    "  - `border_count` (number of candidate split points)\n",
    "  - `min_data_in_leaf` (minimum samples per leaf)\n",
    "  - `bootstrap_type = \"Bayesian\"` with tuned `bagging_temperature`\n",
    "- We continued to tune:\n",
    "  - `learning_rate`\n",
    "  - `l2_leaf_reg`\n",
    "  - `random_strength`\n",
    "\n",
    "Again we minimized the mean 5-fold CV RMSE.\n",
    "\n",
    "- **Best CV RMSE (Run 2):** ≈ **4.10642**\n",
    "\n",
    "So the second run got basically the **same performance**, but **very slightly worse** than Run 1 (difference on the order of 0.00003 in RMSE, which is completely negligible and likely within CV noise).\n",
    "\n",
    "The best hyperparameters from Run 2 looked like:\n",
    "\n",
    "- `depth = 6` (fixed)\n",
    "- `border_count = 128`\n",
    "- `learning_rate ≈ 0.0150`\n",
    "- `l2_leaf_reg ≈ 1.94`\n",
    "- `random_strength ≈ 0.30`\n",
    "- `bagging_temperature ≈ 2.16`\n",
    "- `min_data_in_leaf = 44`\n",
    "- `bootstrap_type = \"Bayesian\"`\n",
    "- `grow_policy = \"SymmetricTree\"`\n",
    "\n",
    "When we retrained with these parameters:\n",
    "\n",
    "- **Final 5-fold CV RMSE:** again ≈ **4.1064**\n",
    "- **Best iterations per fold:** much **larger**, around **1,700–2,100 trees**, with an average of ~**1,883** boosting rounds\n",
    "\n",
    "\n",
    "#### 6.1.3 Interpreting the differences between the two runs\n",
    "\n",
    "Even though Run 2 searched a bigger space and ran for more trials, it did **not** improve RMSE beyond Run 1. The difference in parameters tells us why:\n",
    "\n",
    "- **Learning rate**\n",
    "  - Run 1: `learning_rate ≈ 0.0229`  \n",
    "  - Run 2: `learning_rate ≈ 0.0150` (smaller)  \n",
    "  → A smaller learning rate usually needs **more trees** (which we see from the best iterations) and can make training slower without guaranteeing a better optimum. In our case, the lower learning rate just led to **more boosting rounds** with essentially the same RMSE.\n",
    "\n",
    "- **L2 regularization (`l2_leaf_reg`)**\n",
    "  - Run 1: `≈ 4.01` (stronger regularization)  \n",
    "  - Run 2: `≈ 1.94` (weaker regularization)  \n",
    "  → Run 2 allowed individual leaves to fit slightly more aggressively, but we also increased bagging and min leaf size. These trade-offs roughly canceled out, leading again to almost identical performance.\n",
    "\n",
    "- **Bagging behavior**\n",
    "  - Run 1: `bagging_temperature ≈ 0.46` (milder stochasticity)  \n",
    "  - Run 2: `bagging_temperature ≈ 2.16` + `bootstrap_type = \"Bayesian\"`  \n",
    "  → Run 2 used **much more aggressive Bayesian-style bagging**, injecting more randomness into which data points each tree sees. This can help reduce overfitting, but because our dataset is large and our Run 1 model was already well-regularized, the extra randomness did not produce a clear RMSE gain.\n",
    "\n",
    "- **Tree shape and leaf constraints**\n",
    "  - Run 1: used CatBoost’s default `border_count` and leaf constraints  \n",
    "  - Run 2: explicitly tuned  \n",
    "    - `border_count = 128` (fewer split candidates than 254, slightly simpler trees)  \n",
    "    - `min_data_in_leaf = 44` (prevents tiny leaves, smooths predictions)  \n",
    "  → These changes **regularize** the tree structure: they avoid overly fine splits and tiny leaves. That can improve generalization if the model is overfitting, but in our case the Run 1 configuration was already near the bias-variance sweet spot, so the extra constraints did not translate into a meaningful RMSE improvement.\n",
    "\n",
    "Overall, the second Optuna run **validated** that we were already sitting in a very flat optimum: many slightly different hyperparameter combinations (with different regularization/bagging trade-offs) all land around RMSE ≈ 4.1064.\n",
    "\n",
    "Because **Run 1** achieved the **lowest CV RMSE** and used a slightly simpler set of hyperparameters, we treated it as our **primary “best” configuration** and used it as the base for our CatBoost ensembles and blending experiments. The second run mainly served as a robustness check and showed that even after 80 more trials and a richer search space, we could not significantly beat our original tuned model.\n",
    "\n",
    "#### 6.1.4 XGBoost Hyperparameter Tuning with Optuna (500 trials)\n",
    "\n",
    "To build a strong **second model** for ensembling, we also ran a large Optuna study for **XGBoost** with **500 trials**, using the same 5-fold CV and essentially the same preprocessing pipeline as CatBoost:\n",
    "\n",
    "- cleaned and engineered features (date features, farm clustering, vaccine sum, parity indicators),\n",
    "- applied **fold-safe target encoding** on `Farm_ID` and created farm-delta features (`Prev_vs_Farm`, `Prev_over_Farm`),\n",
    "- one-hot encoded categorical variables **within each fold** and aligned train/validation columns to avoid leakage.\n",
    "\n",
    "Each Optuna trial trained a 5-fold CV XGBoost regressor (with early stopping) and minimized the **mean 5-fold RMSE**.\n",
    "\n",
    "The search space covered both **tree structure** and **regularization**:\n",
    "\n",
    "- Tree growth and structure:\n",
    "  - `grow_policy ∈ {depthwise, lossguide}`\n",
    "  - `max_depth` (0–12, depending on `grow_policy`)\n",
    "  - `max_leaves` (16–256)\n",
    "  - `max_bin ∈ {128, 256, 512}`\n",
    "- Learning and sampling:\n",
    "  - `learning_rate ∈ [0.005, 0.1]` (log-scaled)\n",
    "  - `subsample ∈ [0.5, 0.95]`\n",
    "  - `colsample_bytree`, `colsample_bylevel`, `colsample_bynode ∈ [0.5, 0.95]`\n",
    "- Regularization:\n",
    "  - `gamma ∈ [1e-4, 10]` (log-scaled)\n",
    "  - `reg_alpha ∈ [1e-4, 50]` (log-scaled)\n",
    "  - `reg_lambda ∈ [1e-3, 50]` (log-scaled)\n",
    "\n",
    "The **best trial** out of 500 achieved a **mean CV RMSE ≈ 4.1151** with a relatively shallow but strongly regularized configuration:\n",
    "\n",
    "- `grow_policy = \"depthwise\"`\n",
    "- `max_depth = 4`\n",
    "- `max_bin = 128`\n",
    "- `learning_rate ≈ 0.00681`\n",
    "- `max_leaves ≈ 171`\n",
    "- `min_child_weight ≈ 2.88`\n",
    "- `subsample ≈ 0.529`\n",
    "- `colsample_bytree ≈ 0.528`\n",
    "- `colsample_bylevel ≈ 0.920`\n",
    "- `colsample_bynode ≈ 0.941`\n",
    "- `gamma ≈ 0.632`\n",
    "- `reg_alpha ≈ 49.96`\n",
    "- `reg_lambda ≈ 35.12`\n",
    "\n",
    "Using these tuned hyperparameters, we then retrained:\n",
    "\n",
    "- A **5-fold CV ensemble**, which achieved  \n",
    "  - **Final 5-fold CV RMSE:** ≈ **4.1151**  \n",
    "  - Fold RMSEs in the range **4.106–4.127**, with best iteration counts around **2,800–3,300** trees per fold.\n",
    "- A **full-data multi-seed ensemble**, where we:\n",
    "  - re-fit on all training data for ~the average best-iteration from CV,\n",
    "  - used multiple random seeds (e.g., 42, 100, 200, 300, 400),\n",
    "  - averaged their predictions to produce our final XGBoost test predictions.\n",
    "\n",
    "Even after this large 500-trial search, tuned XGBoost remained slightly weaker than our best CatBoost configuration (≈ **4.115** vs. ≈ **4.106** RMSE). However, because its error pattern was different, this XGBoost model was still **valuable as a complementary learner**, and we used its OOF and test predictions in our **stacking / blending experiments** described in Section 6.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8761f",
   "metadata": {},
   "source": [
    "### 6.2 Ensembling, Stacking, and Snapshot Strategy\n",
    "\n",
    "Once we had a strong single CatBoost model, we explored techniques to reduce variance and squeeze out additional performance:\n",
    "\n",
    "#### 6.2.1 Multi-Seed Ensembling\n",
    "\n",
    "Even with fixed hyperparameters, CatBoost’s training process is stochastic (e.g., random permutations of categorical features, bootstrap sampling). We trained the same CatBoost configuration with **different random seeds**, and averaged their predictions:\n",
    "\n",
    "- Trained the tuned CatBoost model on the full training data with several seeds (e.g., 5 seeds).  \n",
    "- Averaged the predictions from all seed models on the test set.\n",
    "\n",
    "This simple **multi-seed ensemble** slightly improved CV RMSE and also stabilized leaderboard performance, consistent with variance-reduction theory.\n",
    "\n",
    "\n",
    "#### 6.2.2 Stacking and Blending\n",
    "\n",
    "We also experimented with **stacking / blending** strategies:\n",
    "\n",
    "- **CatBoost + XGBoost blend**  \n",
    "  - Trained both a tuned CatBoost model and a tuned XGBoost model (500-trial Optuna study described in 6.1.4).  \n",
    "  - Created out-of-fold (OOF) predictions for each model using 5-fold CV.  \n",
    "  - Searched over a blending weight `alpha` in  \n",
    "    `y_blend = alpha * y_catboost + (1 - alpha) * y_xgboost`.  \n",
    "  - Selected the `alpha` that minimized OOF RMSE, then used that same weight to blend test predictions.\n",
    "\n",
    "- **Ridge stacking on top of OOF predictions**  \n",
    "  - Built a 2-feature meta-dataset where each row contained `(CatBoost_OOF, XGBoost_OOF)`.  \n",
    "  - Trained a Ridge regression model to learn the optimal linear combination.  \n",
    "  - Used the fitted Ridge model to combine test predictions.  \n",
    "\n",
    "These stacking experiments generally gave small gains on CV, but in some cases were less stable on the leaderboard than a pure CatBoost ensemble. This is likely because the meta-learner can overfit to the noise in the OOF predictions, especially when the base models are already highly correlated.\n",
    "\n",
    "#### 6.2.3 Snapshot-Style Ensembling (Parameter Variants)\n",
    "\n",
    "We then tried to extend this idea to a **snapshot ensemble** during our last moments before the submission deadline:\n",
    "\n",
    "- In addition to changing the random seed, we trained several “nearby” versions of the model by slightly varying:\n",
    "  - `depth` (e.g., 5, 6, 7)\n",
    "  - `learning_rate` (e.g., ±10% around the tuned value)\n",
    "- For each parameter variant, we trained models with multiple seeds and averaged all of them.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- **Shallower trees** (e.g., depth 5) capture smoother, more global patterns.\n",
    "- **Deeper trees** (e.g., depth 7) can model more complex feature interactions but risk overfitting.\n",
    "- Slight learning-rate shifts change the effective regularization.\n",
    "\n",
    "By averaging these diverse models, we hoped to obtain a more robust predictor that was less sensitive to any specific hyperparameter setting or random seed. The model ended up taking up 4+ hours to train after we started it at 10pm before the 11:55pm deadline, so we ended up stopping it after we saw it likely wasn't finishing anytime soon, but believe it would have done best since it was squeezing out our already best model even more.\n",
    "\n",
    "In the final model, we relied primarily on **CatBoost-only CV + Multi-Seed ensembling**, with blending used carefully and only when it showed consistent improvement across folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96473ecf",
   "metadata": {},
   "source": [
    "### 6.3 Cross-Validation Experiments and Robustness\n",
    "\n",
    "A major focus of our modeling approach was making sure our CV estimates were **reliable** and not overly optimistic.\n",
    "\n",
    "We experimented with multiple validation strategies:\n",
    "\n",
    "- **Standard KFold with shuffling**  \n",
    "  - Our default setup: 5-fold KFold with `shuffle=True` and a fixed `random_state`.  \n",
    "  - Simple and effective for quickly comparing model variants.\n",
    "\n",
    "- **GroupKFold by `Farm_ID` (sanity check)**  \n",
    "  - To test whether the model was accidentally memorizing per-farm patterns in a way that would not transfer to unseen farms, we ran experiments where entire farms were held out in validation.  \n",
    "  - Result: RMSE under GroupKFold was very similar to our standard KFold RMSE, suggesting that the model generalizes reasonably well across farms.\n",
    "\n",
    "- **Time-related considerations**  \n",
    "  - We created date-based features (e.g., month, week of year, day of week) and considered that the hidden test set might correspond to later calendar periods.  \n",
    "  - We monitored whether features like `year` or `date_ordinal` appeared to cause over-optimistic CV. In parallel, we compared CV scores with and without these trend-like features as a robustness check.\n",
    "\n",
    "Overall, these CV experiments increased our confidence that the improvements we saw during development were not purely due to leakage or overfitting to a specific fold configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243ba99",
   "metadata": {},
   "source": [
    "### 6.4 Final Model and Reflection\n",
    "\n",
    "Our final submission is based on a **CatBoost-only ensemble**, with:\n",
    "\n",
    "- Carefully tuned hyperparameters obtained via Optuna.\n",
    "- Multiple random seeds to create a diverse ensemble.\n",
    "- An alpha-blending step to optimally combine:\n",
    "  - The CV-trained CatBoost model, and  \n",
    "  - The multi-seed ensemble.\n",
    "\n",
    "On our internal 5-fold CV, this final configuration achieved a best RMSE of approximately **4.106145**, and translated into 4th place out of 52 total teams on the course Kaggle leaderboard.\n",
    "\n",
    "**What helped the most:**\n",
    "\n",
    "- Moving from simple baselines to tuned CatBoost gave the largest improvement.\n",
    "- Thoughtful feature engineering (especially around date, feed, and farm-level behavior) provided strong, interpretable signals.\n",
    "- Multi-seed and snapshot ensembling added a smaller but reliable boost and made predictions more stable.\n",
    "\n",
    "**What helped less or was risky:**\n",
    "\n",
    "- Very aggressive stacking and meta-models sometimes improved CV by a tiny margin but did not always translate into better leaderboard performance.\n",
    "- Certain features and encodings (e.g., farm clustering variations, very trend-heavy date features) required careful validation and could lead to overly optimistic CV if not tested properly.\n",
    "\n",
    "Overall, our modeling process was iterative and experimental: we started from simple models, gradually introduced more sophisticated techniques, and continuously used cross-validation and leaderboard feedback to decide which ideas were genuinely helpful and which were mainly adding complexity. This approach allowed us to build a final model that is both **accurate** and **robust**, while also learning a lot about practical model development in the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
